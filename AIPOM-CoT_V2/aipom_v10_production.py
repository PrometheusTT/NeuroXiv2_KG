"""
AIPOM-CoT V10 PRODUCTION
========================
å®Œæ•´é›†æˆæ‰€æœ‰P0å’ŒP1ç»„ä»¶:
âœ… P0-1: æ™ºèƒ½å®ä½“è¯†åˆ« (IntelligentEntityRecognizer)
âœ… P0-2: Benchmarkè¯„ä¼°ç³»ç»Ÿ (BenchmarkRunner)
âœ… P1-1: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’ (DynamicSchemaPathPlanner)
âœ… P1-2: ç»“æ„åŒ–åæ€ (StructuredReflector)

è¿™æ˜¯ç”Ÿäº§å°±ç»ªç‰ˆæœ¬,å¯ä»¥ç›´æ¥ç”¨äº:
- å®Œæ•´Benchmarkè¯„ä¼°
- è®ºæ–‡Figure 3/4/5å¤ç°
- ä¸baselineå¯¹æ¯”

Author: Claude & PrometheusTT
Date: 2025-01-12
"""

import json
import logging
import os
import time
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field
import re
from pathlib import Path

import numpy as np

from neo4j_exec import Neo4jExec
from adaptive_planner import AdaptivePlanner, AnalysisDepth, AnalysisState
from aipom_cot_true_agent_v2 import (
    RealSchemaCache,
    StatisticalTools,
    RealFingerprintAnalyzer,
    AgentPhase,
    AgentState,
    ReasoningStep
)

# å¯¼å…¥æ–°ç»„ä»¶
from intelligent_entity_recognition import (
    IntelligentEntityRecognizer,
    EntityClusteringEngine
)
from schema_path_planner import DynamicSchemaPathPlanner
from structured_reflection import StructuredReflector

try:
    from openai import OpenAI
except ImportError:
    raise ImportError("Please install openai: pip install openai")

logger = logging.getLogger(__name__)


# ==================== Enhanced Agent State ====================

@dataclass
class EnhancedAgentState(AgentState):
    """æ‰©å±•çš„AgentçŠ¶æ€"""

    # æ–°å¢å­—æ®µ
    entity_matches: List = field(default_factory=list)  # EntityMatchåˆ—è¡¨
    entity_clusters: List = field(default_factory=list)  # EntityClusteråˆ—è¡¨
    structured_reflections: List = field(default_factory=list)  # StructuredReflectionåˆ—è¡¨
    schema_paths_used: List = field(default_factory=list)  # ä½¿ç”¨çš„schemaè·¯å¾„


# ==================== Production Agent V10 ====================

class AIPOMCoTV10:
    """
    AIPOM-CoT V10 ç”Ÿäº§ç‰ˆæœ¬

    å®Œæ•´åŠŸèƒ½:
    1. æ™ºèƒ½å®ä½“è¯†åˆ« (æ— éœ€hardcodedåˆ—è¡¨)
    2. åŠ¨æ€Schemaè·¯å¾„è§„åˆ’ (å›¾ç®—æ³•)
    3. ç»“æ„åŒ–åæ€ (é‡åŒ–è¯„ä¼°)
    4. å®Œæ•´ç»Ÿè®¡å·¥å…·
    5. å¤šæ¨¡æ€åˆ†æ
    6. è‡ªé€‚åº”é‡è§„åˆ’
    """

    def __init__(self,
                 neo4j_uri: str,
                 neo4j_user: str,
                 neo4j_pwd: str,
                 database: str,
                 schema_json_path: str,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4o"):

        # æ•°æ®åº“è¿æ¥
        self.db = Neo4jExec(neo4j_uri, neo4j_user, neo4j_pwd, database=database)

        # Schema
        self.schema = RealSchemaCache(schema_json_path)

        # ===== æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– =====

        # P0-1: æ™ºèƒ½å®ä½“è¯†åˆ«
        logger.info("ğŸ” Initializing intelligent entity recognition...")
        self.entity_recognizer = IntelligentEntityRecognizer(self.db, self.schema)
        self.entity_clusterer = EntityClusteringEngine(self.db, self.schema)

        # P1-1: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
        logger.info("ğŸ—ºï¸  Initializing dynamic schema path planning...")
        self.path_planner = DynamicSchemaPathPlanner(self.schema)

        # P1-2: ç»“æ„åŒ–åæ€
        logger.info("ğŸ¤” Initializing structured reflection...")
        self.reflector = StructuredReflector()

        # åŸæœ‰ç»„ä»¶
        self.stats = StatisticalTools()
        self.fingerprint = RealFingerprintAnalyzer(self.db, self.schema)

        # OpenAI
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model


        self.adaptive_planner = AdaptivePlanner(self.schema, self.path_planner,self.client)
        # ğŸ†• æ·»åŠ Focus-Driven Planner
        logger.info("ğŸ¯ Initializing focus-driven planning...")
        from focus_driven_planner import FocusDrivenPlanner
        self.focus_planner = FocusDrivenPlanner(self.schema, self.db)

        # ğŸ†• æ·»åŠ Comparative Analysis Planner
        logger.info("ğŸ“Š Initializing comparative analysis planning...")
        from comparative_analysis_planner import ComparativeAnalysisPlanner
        self.comparative_planner = ComparativeAnalysisPlanner(
            self.db,
            self.fingerprint,
            self.stats
        )

        logger.info("âœ… AIPOM-CoT V10 initialized successfully!")
        logger.info(f"   â€¢ Entity recognition: Ready")
        logger.info(f"   â€¢ Schema path planning: Ready")
        logger.info(f"   â€¢ Structured reflection: Ready")

    # ==================== Main Entry Point ====================

    """
    å®Œæ•´çš„answeræ–¹æ³•å®ç° - é›†æˆè‡ªé€‚åº”è§„åˆ’
    """

    def answer(self, question: str, max_iterations: int = 15) -> Dict[str, Any]:
        """
        ä¸»å…¥å£: å›ç­”é—®é¢˜ (å®Œæ•´ç‰ˆ)

        å®Œæ•´æµç¨‹:
        1. æ™ºèƒ½å®ä½“è¯†åˆ«
        2. å®ä½“èšç±»
        3. ç¡®å®šåˆ†ææ·±åº¦
        4. æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨ (Adaptive/Focus-Driven/Comparative)
        5. è‡ªé€‚åº”æ‰§è¡Œå¾ªç¯ (åŒ…å«ç»Ÿè®¡åˆ†æ)
        6. ç­”æ¡ˆåˆæˆ (ç§‘å­¦å™äº‹)
        """
        logger.info(f"ğŸ¯ Question: {question}")
        start_time = time.time()

        state = EnhancedAgentState(question=question)

        # ===== PHASE 1: INTELLIGENT PLANNING =====
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“‹ PHASE 1: INTELLIGENT PLANNING (Enhanced)")
        logger.info("=" * 70)

        state.phase = AgentPhase.PLANNING

        # Step 1-2: å®ä½“è¯†åˆ« + èšç±»
        logger.info("  [1/4] Intelligent entity recognition...")
        entity_matches = self.entity_recognizer.recognize_entities(question)
        state.entity_matches = entity_matches

        logger.info(f"     Found {len(entity_matches)} entity matches")
        for match in entity_matches[:5]:
            logger.info(f"       â€¢ {match.text} ({match.entity_type}) [{match.confidence:.2f}]")

        logger.info("  [2/4] Entity clustering...")
        entity_clusters = self.entity_clusterer.cluster_entities(entity_matches, question)
        state.entity_clusters = entity_clusters

        logger.info(f"     Created {len(entity_clusters)} entity clusters")
        for cluster in entity_clusters:
            logger.info(f"       â€¢ {cluster.cluster_type}: {cluster.primary_entity.text}")

        # ğŸ†• Step 3: ç¡®å®šåˆ†ææ·±åº¦
        from adaptive_planner import determine_analysis_depth, AnalysisState

        logger.info("  [3/4] Determining analysis depth...")
        target_depth = determine_analysis_depth(question)
        logger.info(f"     Target depth: {target_depth.value}")

        # ğŸ†• Step 4: åˆå§‹åŒ–åˆ†æçŠ¶æ€
        logger.info("  [4/4] Initializing analysis state...")

        analysis_state = AnalysisState(
            discovered_entities={},
            executed_steps=[],
            modalities_covered=[],
            current_focus='gene' if entity_clusters and entity_clusters[0].cluster_type == 'gene_marker' else 'region',
            target_depth=target_depth,
            question_intent=self._classify_question_intent(question)
        )

        # å¡«å……åˆå§‹å®ä½“
        for cluster in entity_clusters:
            entity_type = cluster.primary_entity.entity_type
            entity_id = cluster.primary_entity.entity_id

            analysis_state.discovered_entities.setdefault(entity_type, []).append(entity_id)

            for related in cluster.related_entities:
                analysis_state.discovered_entities.setdefault(
                    related.entity_type, []
                ).append(related.entity_id)

        # å…¼å®¹æ€§
        state.entities = [
            {'text': m.text, 'type': m.entity_type, 'confidence': m.confidence}
            for m in entity_matches[:10]
        ]

        # ğŸ†• å­˜å‚¨analysis_stateåˆ°state
        state.analysis_state = analysis_state

        logger.info(f"âœ… Planning complete")
        logger.info(f"   â€¢ Target depth: {target_depth.value}")
        logger.info(f"   â€¢ Initial entities: {list(analysis_state.discovered_entities.keys())}")

        # ===== PHASE 2: ADAPTIVE EXECUTION =====
        logger.info("\n" + "=" * 70)
        logger.info("âš™ï¸ PHASE 2: ADAPTIVE EXECUTION (Multi-Planner)")
        logger.info("=" * 70)

        state.phase = AgentPhase.EXECUTING

        iteration = 0
        while iteration < max_iterations:
            # ğŸ†• å†³å®šæ˜¯å¦ç»§ç»­
            if not self.adaptive_planner.should_continue(analysis_state, question):
                logger.info("ğŸ“Œ Analysis complete (adaptive decision)")
                break

            # ğŸ†• æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨
            planner_type = self._select_planner(analysis_state, question)

            if planner_type == 'focus_driven':
                logger.info(f"\nğŸ¯ Using FOCUS-DRIVEN planner (iteration {iteration + 1})...")
                next_steps = self.focus_planner.generate_focus_driven_plan(
                    analysis_state,
                    question
                )

            elif planner_type == 'comparative':
                logger.info(f"\nğŸ“Š Using COMPARATIVE planner (iteration {iteration + 1})...")
                next_steps = self.comparative_planner.generate_comparative_plan(
                    analysis_state,
                    question
                )

            else:
                logger.info(f"\nğŸ”„ Using ADAPTIVE planner (iteration {iteration + 1})...")
                next_steps = self.adaptive_planner.plan_next_steps(
                    analysis_state,
                    question,
                    max_steps=2
                )

            if not next_steps:
                logger.info("ğŸ“Œ No more steps available")
                break

            # æ‰§è¡Œè§„åˆ’çš„æ­¥éª¤
            for candidate_step in next_steps:
                if iteration >= max_iterations:
                    break

                logger.info(f"\nğŸ”¹ Step {iteration + 1}: {candidate_step.purpose}")
                logger.info(f"   Type: {candidate_step.step_type}")
                logger.info(f"   Priority: {candidate_step.priority:.1f}")
                if hasattr(candidate_step, 'llm_score') and candidate_step.llm_score > 0:
                    logger.info(f"   LLM score: {candidate_step.llm_score:.2f}")

                # ğŸ†• è½¬æ¢ä¸ºReasoningStep
                reasoning_step = self._convert_candidate_to_reasoning(
                    candidate_step,
                    iteration + 1,
                    analysis_state
                )

                # æ‰§è¡Œ
                exec_result = self._execute_step(reasoning_step, state)

                if not exec_result['success']:
                    logger.error(f"   âŒ Failed: {exec_result.get('error')}")

                    if state.replanning_count < state.max_replanning:
                        logger.info(f"   ğŸ”„ Replanning...")
                        state.replanning_count += 1

                    continue

                # ğŸ†• ç»“æ„åŒ–åæ€
                structured_reflection = self.reflector.reflect(
                    step_number=reasoning_step.step_number,
                    purpose=reasoning_step.purpose,
                    expected_result=reasoning_step.expected_result,
                    actual_result=reasoning_step.actual_result,
                    question_context=question
                )

                reasoning_step.reflection = structured_reflection.summary
                reasoning_step.validation_passed = (
                        structured_reflection.validation_status.value in ['passed', 'partial']
                )

                state.structured_reflections.append(structured_reflection)
                state.reflections.append(structured_reflection.summary)

                logger.info(f"   ğŸ“Š Reflection: {structured_reflection.summary}")
                logger.info(f"   ğŸ“ˆ Confidence: {structured_reflection.confidence_score:.3f}")

                # ğŸ†• æ›´æ–°åˆ†æçŠ¶æ€
                self._update_analysis_state(
                    analysis_state,
                    reasoning_step,
                    exec_result,
                    candidate_step
                )

                state.executed_steps.append(reasoning_step)
                iteration += 1

        # ===== PHASE 3: ANSWER SYNTHESIS =====
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“ PHASE 3: ANSWER SYNTHESIS")
        logger.info("=" * 70)

        final_answer = self._synthesize_answer(state)

        execution_time = time.time() - start_time

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'question': question,
            'answer': final_answer,

            'entities_recognized': [
                {
                    'text': m.text,
                    'type': m.entity_type,
                    'confidence': m.confidence,
                    'match_type': m.match_type
                }
                for m in state.entity_matches[:10]
            ],

            'reasoning_plan': [self._step_to_dict(s) for s in state.executed_steps],
            'executed_steps': [self._step_to_dict(s) for s in state.executed_steps],

            'reflections': state.reflections,
            'structured_reflections': [
                {
                    'step': r.step_number,
                    'status': r.validation_status.value,
                    'confidence': r.confidence_score,
                    'uncertainty': r.uncertainty.overall_uncertainty,
                    'should_replan': r.should_replan
                }
                for r in state.structured_reflections
            ],

            # ğŸ†• è‡ªé€‚åº”è§„åˆ’ä¿¡æ¯
            'adaptive_planning': {
                'target_depth': target_depth.value,
                'final_depth': len(state.executed_steps),
                'modalities_covered': analysis_state.modalities_covered,
                'entities_discovered': {
                    k: len(v) for k, v in analysis_state.discovered_entities.items()
                },
                'primary_focus': getattr(analysis_state, 'primary_focus', None)
            },

            'replanning_count': state.replanning_count,
            'confidence_score': state.confidence_score,
            'execution_time': execution_time,
            'total_steps': len(state.executed_steps),
            'schema_paths_used': state.schema_paths_used
        }

        logger.info(f"\nâœ… Completed in {execution_time:.2f}s")
        logger.info(f"   â€¢ Steps executed: {len(state.executed_steps)}")
        logger.info(f"   â€¢ Confidence: {state.confidence_score:.3f}")
        logger.info(f"   â€¢ Modalities: {', '.join(analysis_state.modalities_covered)}")

        return result

    # ==================== è¾…åŠ©æ–¹æ³• ====================
    def _select_planner(self, state, question: str) -> str:
        """
        æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨ (å¢å¼ºç‰ˆ - æ”¯æŒæ— entityçš„systematicæ¨¡å¼)

        ğŸ”§ å…³é”®æ”¹è¿›: æ£€æµ‹systematicå…³é”®è¯ï¼Œå³ä½¿æ²¡æœ‰åˆå§‹entities
        """
        q_lower = question.lower()

        # ğŸ” æ¯”è¾ƒæŸ¥è¯¢ â†’ Comparative
        compare_keywords = ['compare', 'versus', 'vs ', 'vs.', 'difference between', 'contrast']
        if any(kw in q_lower for kw in compare_keywords):
            logger.info(f"   Comparison keywords detected â†’ comparative")
            return 'comparative'

        # ğŸ”§ æ–°å¢: ç³»ç»Ÿç­›é€‰å…³é”®è¯ (ä¸ä¾èµ–åˆå§‹entities)
        systematic_keywords = [
            'which regions', 'which brain', 'find all', 'identify all',
            'screen', 'systematic', 'highest', 'top regions',
            'mismatch', 'show', 'exhibit', 'demonstrate'
        ]

        # æ£€æµ‹systematicæ¨¡å¼
        has_which = 'which' in q_lower
        has_highest = any(w in q_lower for w in ['highest', 'top', 'most', 'strongest'])
        has_mismatch = 'mismatch' in q_lower
        has_show = any(w in q_lower for w in ['show', 'exhibit', 'demonstrate', 'display'])

        # ğŸ¯ å…³é”®: Systematicæ¨¡å¼åˆ¤æ–­
        if has_which and (has_highest or has_mismatch or has_show):
            logger.info(f"   Systematic screening keywords detected â†’ comparative")
            logger.info(f"     Keywords: which={has_which}, highest={has_highest}, mismatch={has_mismatch}")
            return 'comparative'

        # æˆ–è€…ç›´æ¥æ£€æµ‹ç»„åˆ
        if any(kw in q_lower for kw in systematic_keywords):
            # è¿›ä¸€æ­¥ç¡®è®¤æ˜¯å¦æ˜¯ç­›é€‰ç±»é—®é¢˜
            screening_patterns = [
                'which.*show', 'which.*have', 'which.*exhibit',
                'find.*regions', 'identify.*regions',
                'highest.*mismatch', 'top.*mismatch'
            ]
            import re
            for pattern in screening_patterns:
                if re.search(pattern, q_lower):
                    logger.info(f"   Systematic pattern detected: {pattern} â†’ comparative")
                    return 'comparative'

        # ğŸ”§ Focus-driven: æœ‰regionsçš„æ·±åº¦æŸ¥è¯¢
        if 'Region' in state.discovered_entities:
            n_regions = len(state.discovered_entities.get('Region', []))
            if n_regions > 0:
                logger.info(f"   {n_regions} regions found â†’ focus-driven")
                return 'focus_driven'

        # ğŸ”§ Focus-driven: GeneæŸ¥è¯¢ä¸”æœ‰æ·±åº¦æ„å›¾
        if 'GeneMarker' in state.discovered_entities:
            deep_intent_keywords = ['tell me about', 'about', 'analyze', 'characterize', 'comprehensive']
            if any(kw in q_lower for kw in deep_intent_keywords):
                logger.info(f"   Gene query with deep intent â†’ focus-driven")
                return 'focus_driven'

        # é»˜è®¤: Adaptive
        logger.info(f"   Default â†’ adaptive")
        return 'adaptive'

    def _classify_question_intent(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜æ„å›¾"""
        question_lower = question.lower()

        if any(w in question_lower for w in ['compare', 'difference', 'versus', 'vs']):
            return 'comparison'
        elif any(w in question_lower for w in ['comprehensive', 'detailed', 'everything']):
            return 'comprehensive'
        elif any(w in question_lower for w in ['why', 'explain', 'how']):
            return 'explanatory'
        elif any(w in question_lower for w in ['which', 'find', 'identify']):
            return 'screening'
        else:
            return 'simple_query'



    def _update_analysis_state(self,
                               analysis_state,
                               step: ReasoningStep,
                               result: Dict,
                               candidate):
        """æ›´æ–°åˆ†æçŠ¶æ€"""
        # è®°å½•æ‰§è¡Œçš„æ­¥éª¤
        analysis_state.executed_steps.append({
            'purpose': step.purpose,
            'modality': step.modality,
            'row_count': len(result.get('data', [])),
            'step_id': candidate.step_id
        })

        # æ›´æ–°modalityè¦†ç›–
        if step.modality and step.modality not in analysis_state.modalities_covered:
            analysis_state.modalities_covered.append(step.modality)

        # ğŸ†• æå–æ–°å‘ç°çš„å®ä½“
        data = result.get('data', [])
        if not data:
            return

        first_row = data[0]

        # æå–regions
        if 'region' in first_row or 'acronym' in first_row:
            regions = list(set([
                row.get('region') or row.get('acronym')
                for row in data
                if row.get('region') or row.get('acronym')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Region', [])
            for r in regions:
                if r and r not in existing:
                    existing.append(r)

        # æå–clusters
        if 'cluster' in first_row or 'cluster_name' in first_row:
            clusters = list(set([
                row.get('cluster') or row.get('cluster_name')
                for row in data
                if row.get('cluster') or row.get('cluster_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Cluster', [])
            for c in clusters:
                if c and c not in existing:
                    existing.append(c)

        # æå–subclasses
        if 'subclass' in first_row or 'subclass_name' in first_row:
            subclasses = list(set([
                row.get('subclass') or row.get('subclass_name')
                for row in data
                if row.get('subclass') or row.get('subclass_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Subclass', [])
            for s in subclasses:
                if s and s not in existing:
                    existing.append(s)

        # ğŸ†• æå–projection targets
        if 'target' in first_row or 'target_region' in first_row:
            targets = list(set([
                row.get('target') or row.get('target_region')
                for row in data
                if row.get('target') or row.get('target_region')
            ]))

            existing = analysis_state.discovered_entities.setdefault('ProjectionTarget', [])
            for t in targets:
                if t and t not in existing:
                    existing.append(t)

            if targets:
                logger.info(f"   ğŸ“ Discovered {len(targets)} projection targets")

    def _classify_question_intent(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜æ„å›¾"""
        question_lower = question.lower()

        if any(w in question_lower for w in ['compare', 'difference', 'versus', 'vs']):
            return 'comparison'
        elif any(w in question_lower for w in ['comprehensive', 'detailed', 'everything']):
            return 'comprehensive'
        elif any(w in question_lower for w in ['why', 'explain', 'how']):
            return 'explanatory'
        else:
            return 'simple_query'

    def _convert_candidate_to_reasoning(self, candidate, step_number, analysis_state):
        """è½¬æ¢CandidateStep (ä¿®å¤ç‰ˆ)"""
        params = candidate.parameters.copy()

        # ğŸ”§ æ™ºèƒ½åˆ¤æ–­action
        has_cypher = bool(candidate.cypher_template and candidate.cypher_template.strip())

        if not has_cypher:
            # ç‰¹æ®Šæ­¥éª¤
            if 'statistical' in candidate.step_type.lower() or 'fdr' in candidate.step_id.lower():
                action = 'execute_statistical'
            elif 'multi-modal' in candidate.step_type.lower() or 'mismatch' in candidate.step_id.lower():
                action = 'execute_fingerprint'
            else:
                action = 'execute_cypher'
        else:
            action = 'execute_cypher'

        return ReasoningStep(
            step_number=step_number,
            purpose=candidate.purpose,
            action=action,  # ğŸ”§ æ­£ç¡®çš„action
            rationale=candidate.rationale,
            expected_result=candidate.expected_data,
            query_or_params={
                'query': candidate.cypher_template,
                'params': params
            },
            modality=candidate.step_type,
            depends_on=getattr(candidate, 'depends_on', [])
        )

    def _update_analysis_state(self,
                               analysis_state: 'AnalysisState',
                               step: ReasoningStep,
                               result: Dict,
                               candidate: 'CandidateStep'):
        """
        æ›´æ–°åˆ†æçŠ¶æ€
        """
        # è®°å½•æ‰§è¡Œçš„æ­¥éª¤
        analysis_state.executed_steps.append({
            'purpose': step.purpose,
            'modality': step.modality,
            'row_count': len(result.get('data', [])),
            'step_id': candidate.step_id
        })

        # æ›´æ–°modalityè¦†ç›–
        if step.modality and step.modality not in analysis_state.modalities_covered:
            analysis_state.modalities_covered.append(step.modality)

        # ğŸ†• æå–æ–°å‘ç°çš„å®ä½“
        data = result.get('data', [])
        if not data:
            return

        first_row = data[0]

        # æå–regions
        if 'region' in first_row or 'acronym' in first_row:
            regions = list(set([
                row.get('region') or row.get('acronym')
                for row in data
                if row.get('region') or row.get('acronym')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Region', [])
            for r in regions:
                if r not in existing:
                    existing.append(r)

        # æå–clusters
        if 'cluster' in first_row or 'cluster_name' in first_row:
            clusters = list(set([
                row.get('cluster') or row.get('cluster_name')
                for row in data
                if row.get('cluster') or row.get('cluster_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Cluster', [])
            for c in clusters:
                if c not in existing:
                    existing.append(c)

        # æå–subclasses
        if 'subclass' in first_row or 'subclass_name' in first_row:
            subclasses = list(set([
                row.get('subclass') or row.get('subclass_name')
                for row in data
                if row.get('subclass') or row.get('subclass_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Subclass', [])
            for s in subclasses:
                if s not in existing:
                    existing.append(s)

        # ğŸ†• æå–projection targets
        if 'target' in first_row or 'target_region' in first_row:
            targets = list(set([
                row.get('target') or row.get('target_region')
                for row in data
                if row.get('target') or row.get('target_region')
            ]))

            existing = analysis_state.discovered_entities.setdefault('ProjectionTarget', [])
            for t in targets:
                if t not in existing:
                    existing.append(t)

            logger.info(f"   ğŸ“ Discovered {len(targets)} projection targets")

    def _determine_analysis_depth(self, question: str) -> AnalysisDepth:
        """æ ¹æ®é—®é¢˜ç¡®å®šåˆ†ææ·±åº¦"""

        question_lower = question.lower()

        # Deep: comprehensive, detailed, everything, full, complete
        if any(kw in question_lower for kw in ['comprehensive', 'detailed', 'everything', 'complete', 'full']):
            return AnalysisDepth.DEEP

        # Shallow: simple, basic, quick, overview
        if any(kw in question_lower for kw in ['simple', 'basic', 'quick', 'overview', 'briefly']):
            return AnalysisDepth.SHALLOW

        # Default: Medium
        return AnalysisDepth.MEDIUM


    def _enhanced_planning_phase(self, state: EnhancedAgentState) -> Dict[str, Any]:
        """
        å¢å¼ºçš„è§„åˆ’é˜¶æ®µ

        æ­¥éª¤:
        1. æ™ºèƒ½å®ä½“è¯†åˆ« (æ— hardcodedåˆ—è¡¨!)
        2. å®ä½“èšç±»
        3. åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
        4. LLMç²¾åŒ–
        """
        try:
            # Step 1: å®ä½“è¯†åˆ«
            logger.info("  [1/4] Intelligent entity recognition...")
            entity_matches = self.entity_recognizer.recognize_entities(state.question)
            state.entity_matches = entity_matches

            logger.info(f"     Found {len(entity_matches)} entity matches")
            for match in entity_matches[:5]:
                logger.info(f"       â€¢ {match.text} ({match.entity_type}) [{match.confidence:.2f}]")

            # Step 2: å®ä½“èšç±»
            logger.info("  [2/4] Entity clustering...")
            entity_clusters = self.entity_clusterer.cluster_entities(
                entity_matches,
                state.question
            )
            state.entity_clusters = entity_clusters

            logger.info(f"     Created {len(entity_clusters)} entity clusters")
            for cluster in entity_clusters:
                logger.info(f"       â€¢ {cluster.cluster_type}: {cluster.primary_entity.text}")

            # Step 3: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
            logger.info("  [3/4] Dynamic schema path planning...")
            query_plans = self.path_planner.generate_plan(entity_clusters, state.question)

            logger.info(f"     Generated {len(query_plans)} query plans")

            # è®°å½•ä½¿ç”¨çš„schemaè·¯å¾„
            for plan in query_plans:
                if plan.schema_path.hops:
                    state.schema_paths_used.append({
                        'start': plan.schema_path.start_label,
                        'end': plan.schema_path.end_label,
                        'hops': len(plan.schema_path.hops),
                        'score': plan.schema_path.score
                    })

            # Step 4: LLMç²¾åŒ–
            logger.info("  [4/4] LLM plan refinement...")
            refined_steps = self._llm_refine_plans(query_plans, state)
            state.reasoning_plan = refined_steps

            # ä¿å­˜å®ä½“åˆ°state (å…¼å®¹åŸæœ‰æ ¼å¼)
            state.entities = [
                {
                    'text': m.text,
                    'type': m.entity_type,
                    'confidence': m.confidence
                }
                for m in entity_matches[:10]
            ]

            return {'success': True}

        except Exception as e:
            logger.error(f"Enhanced planning failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _llm_refine_plans(self,
                          query_plans: List,
                          state: EnhancedAgentState) -> List[ReasoningStep]:
        """
        LLMç²¾åŒ–æŸ¥è¯¢è®¡åˆ’

        å°†åŠ¨æ€ç”Ÿæˆçš„QueryPlanè½¬æ¢ä¸ºReasoningStep,å¹¶è®©LLMè¡¥å……ç»†èŠ‚
        """
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        plans_dict = []
        for qp in query_plans:
            plans_dict.append({
                'step': qp.step_number,
                'purpose': qp.purpose,
                'action': qp.action,
                'query': qp.cypher_template,
                'parameters': qp.parameters,
                'modality': qp.modality,
                'depends_on': qp.depends_on,
                'schema_path_score': qp.schema_path.score if qp.schema_path else 0.0
            })

        prompt = f"""You are refining a reasoning plan for neuroscience knowledge graph analysis.

**Question:** {state.question}

**Recognized Entities:** {', '.join([e['text'] for e in state.entities])}

**Dynamically Generated Query Plans:**
{json.dumps(plans_dict, indent=2)}

Your task:
1. Review each query plan
2. Add detailed **expected_result** descriptions
3. Enhance **rationale** with domain knowledge
4. Verify Cypher query correctness
5. Add any missing steps if needed

Return a JSON object with key "steps" containing an array:
{{
  "steps": [
    {{
      "step_number": 1,
      "purpose": "...",
      "action": "execute_cypher",
      "rationale": "Detailed explanation",
      "expected_result": "Concrete prediction of what data will look like",
      "query_or_params": {{"query": "...", "params": {{}}}},
      "modality": "molecular/morphological/projection",
      "depends_on": []
    }},
    ...
  ]
}}

**Important:**
- Make rationale SPECIFIC and scientifically grounded
- Expected results should describe DATA PATTERNS (e.g., "10-20 clusters with neuron counts ranging 500-5000")
- Ensure query syntax is correct
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert neuroscientist and Neo4j query expert."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )

            result = json.loads(response.choices[0].message.content)

            # è½¬æ¢ä¸ºReasoningStep
            steps = []
            for step_dict in result.get('steps', []):
                query_or_params = step_dict.get('query_or_params', {})

                # å¤„ç†å‚æ•°æ›¿æ¢
                if isinstance(query_or_params, dict):
                    if 'query' not in query_or_params and 'query' in step_dict:
                        query_or_params = {'query': step_dict['query']}

                step = ReasoningStep(
                    step_number=step_dict.get('step_number', len(steps) + 1),
                    purpose=step_dict.get('purpose', ''),
                    action=step_dict.get('action', 'execute_cypher'),
                    rationale=step_dict.get('rationale', ''),
                    expected_result=step_dict.get('expected_result', ''),
                    query_or_params=query_or_params,
                    modality=step_dict.get('modality'),
                    depends_on=step_dict.get('depends_on', [])
                )
                steps.append(step)

            return steps

        except Exception as e:
            logger.error(f"LLM refinement failed: {e}")

            # Fallback: ç›´æ¥è½¬æ¢QueryPlan
            fallback_steps = []
            for qp in query_plans:
                step = ReasoningStep(
                    step_number=qp.step_number,
                    purpose=qp.purpose,
                    action=qp.action,
                    rationale="Automatically generated from schema path",
                    expected_result="Data matching query criteria",
                    query_or_params={'query': qp.cypher_template, 'params': qp.parameters},
                    modality=qp.modality,
                    depends_on=qp.depends_on
                )
                fallback_steps.append(step)

            return fallback_steps

    def _characterize_top_pairs(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        æ·±å…¥åˆ†ætop mismatch pairs (Case Study)

        ğŸ†• æ–°å¢åŠŸèƒ½:
        1. æå–top N pairs
        2. æŸ¥è¯¢æ¯ä¸ªpairçš„è¯¦ç»†æ•°æ®:
           - Morphological features
           - Projection targets
           - Molecular composition
        """
        n_top = params.get('n_top_pairs', 3)

        # ä»FDRç»“æœè·å–top pairs
        fdr_data = None
        for key, data in state.intermediate_data.items():
            if data and isinstance(data, list) and len(data) > 0:
                if 'fdr_significant' in data[0] and data[0].get('fdr_significant'):
                    fdr_data = data
                    break

        if not fdr_data:
            logger.warning("   No FDR significant pairs found, using top mismatch pairs")
            # Fallback: ä½¿ç”¨top mismatch
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    if 'mismatch_combined' in data[0]:
                        fdr_data = sorted(data, key=lambda x: x['mismatch_combined'], reverse=True)
                        break

        if not fdr_data:
            return {'success': False, 'error': 'No mismatch data found', 'data': []}

        # é€‰æ‹©top N pairs
        top_pairs = fdr_data[:n_top]

        logger.info(f"   Analyzing top {len(top_pairs)} pairs:")
        for pair in top_pairs:
            logger.info(f"     â€¢ {pair['region1']} vs {pair['region2']}: mismatch={pair['mismatch_combined']:.3f}")

        # è¯¦ç»†åˆ†ææ¯ä¸ªpair
        detailed_results = []

        for pair in top_pairs:
            region1 = pair['region1']
            region2 = pair['region2']

            logger.info(f"   Deep characterization: {region1} vs {region2}")

            # ğŸ”¹ 1. Morphological comparison
            morph_query = """
            MATCH (n:Neuron)-[:LOCATE_AT]->(r:Region)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS region,
                   count(n) AS neuron_count,
                   avg(n.axonal_length) AS avg_axon,
                   avg(n.dendritic_length) AS avg_dendrite,
                   avg(n.axonal_branches) AS avg_axon_branches,
                   avg(n.dendritic_branches) AS avg_dendrite_branches,
                   stdev(n.axonal_length) AS std_axon,
                   stdev(n.dendritic_length) AS std_dendrite
            """
            morph_result = self.db.run(morph_query, {'region1': region1, 'region2': region2})

            # ğŸ”¹ 2. Projection targets comparison
            proj_query = """
            MATCH (r:Region)-[p:PROJECT_TO]->(t:Region)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS source,
                   t.acronym AS target,
                   t.name AS target_name,
                   p.weight AS weight
            ORDER BY r.acronym, p.weight DESC
            LIMIT 30
            """
            proj_result = self.db.run(proj_query, {'region1': region1, 'region2': region2})

            # ğŸ”¹ 3. Molecular composition
            mol_query = """
            MATCH (r:Region)-[:HAS_CLUSTER]->(c:Cluster)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS region,
                   c.name AS cluster,
                   c.markers AS markers,
                   c.number_of_neurons AS neurons
            ORDER BY r.acronym, c.number_of_neurons DESC
            LIMIT 20
            """
            mol_result = self.db.run(mol_query, {'region1': region1, 'region2': region2})

            # æ•´åˆç»“æœ
            detailed_results.append({
                'pair': f"{region1}_vs_{region2}",
                'region1': region1,
                'region2': region2,
                'mismatch_score': pair['mismatch_combined'],
                'p_value': pair.get('p_value', 1.0),
                'q_value': pair.get('q_value', 1.0),
                'morphology': morph_result.get('data', []),
                'projections': proj_result.get('data', []),
                'molecular': mol_result.get('data', [])
            })

        logger.info(f"   âœ… Detailed characterization complete for {len(detailed_results)} pairs")

        return {
            'success': True,
            'data': detailed_results,
            'rows': len(detailed_results),
            'analysis_type': 'case_study'
        }

    # ==================== Execution ====================

    def _execute_step(self, step: ReasoningStep, state: EnhancedAgentState) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæ­¥éª¤ (ä¿®å¤ç‰ˆ - æ”¯æŒcase study)"""
        start_time = time.time()

        try:
            query = step.query_or_params.get('query', '').strip()
            params = step.query_or_params.get('params', {})

            # åˆ¤æ–­æ‰§è¡Œç±»å‹
            if not query:
                # ğŸ†• Case studyæ£€æµ‹
                if 'characterize' in step.purpose.lower() and 'top' in step.purpose.lower():
                    result = self._characterize_top_pairs(params, state)
                elif 'mismatch' in step.purpose.lower():
                    result = self._execute_fingerprint_step(step, state)
                elif 'statistical' in step.purpose.lower() or 'fdr' in step.purpose.lower():
                    result = self._execute_statistical_step(step, state)
                else:
                    result = {'success': False, 'error': 'Cannot determine execution type'}
            else:
                result = self._execute_cypher_step(step, state)

            step.actual_result = result
            step.execution_time = time.time() - start_time

            step_key = f"step_{step.step_number}"
            state.intermediate_data[step_key] = result.get('data', [])

            return result

        except Exception as e:
            logger.error(f"Step execution failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _execute_cypher_step(self, step: ReasoningStep, state: EnhancedAgentState) -> Dict[str, Any]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢æ­¥éª¤"""
        query = step.query_or_params.get('query', '').strip()
        params = step.query_or_params.get('params', {})

        # ğŸ”§ ç©ºæŸ¥è¯¢æ£€æŸ¥
        if not query:
            logger.warning(f"   Empty Cypher query - skipping")
            return {'success': False, 'error': 'Empty query', 'data': []}

        # å‚æ•°æ›¿æ¢
        if step.depends_on:
            params = self._resolve_parameters(step, state, params)

        # è‡ªåŠ¨æ·»åŠ LIMIT
        import re
        if not re.search(r'\bLIMIT\b', query, re.IGNORECASE):
            query = f"{query}\nLIMIT 100"

        return self.db.run(query, params)

    def _execute_statistical_step(self,
                                  step: ReasoningStep,
                                  state: EnhancedAgentState) -> Dict[str, Any]:
        """
        ğŸ†• æ‰§è¡Œç»Ÿè®¡æ­¥éª¤
        """
        params = step.query_or_params.get('params', {})
        test_type = params.get('test_type', 'permutation')

        logger.info(f"   ğŸ“Š Statistical test: {test_type}")

        try:
            if test_type == 'permutation':
                return self._permutation_test(params, state)

            elif test_type == 'fdr':
                return self._fdr_correction(params, state)

            elif test_type == 'correlation':
                return self._correlation_test(params, state)

            else:
                return {'success': False, 'error': f'Unknown test type: {test_type}'}

        except Exception as e:
            logger.error(f"Statistical test failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _execute_fingerprint_step(self,
                                  step: ReasoningStep,
                                  state: EnhancedAgentState) -> Dict[str, Any]:
        """
        ğŸ†• æ‰§è¡Œfingerprintè®¡ç®—æ­¥éª¤
        """
        params = step.query_or_params.get('params', {})
        analysis_type = params.get('analysis_type', 'cross_modal_mismatch')

        logger.info(f"   ğŸ”¬ Fingerprint analysis: {analysis_type}")

        if analysis_type == 'cross_modal_mismatch':
            return self._compute_mismatch_matrix(params, state)
        else:
            return {'success': False, 'error': f'Unknown analysis type: {analysis_type}'}

    def _resolve_parameters(self,
                            step: ReasoningStep,
                            state: EnhancedAgentState,
                            params: Dict) -> Dict:
        """è§£ææ­¥éª¤ä¾èµ–çš„å‚æ•°"""
        resolved = params.copy()

        # æŸ¥æ‰¾ä¾èµ–æ­¥éª¤çš„æ•°æ®
        for dep_num in step.depends_on:
            dep_key = f"step_{dep_num}"
            if dep_key in state.intermediate_data:
                dep_data = state.intermediate_data[dep_key]

                # æå–å¸¸ç”¨å­—æ®µ
                if dep_data:
                    # æå–region acronyms
                    regions = []
                    for row in dep_data:
                        if 'region' in row:
                            regions.append(row['region'])
                        elif 'acronym' in row:
                            regions.append(row['acronym'])

                    if regions:
                        resolved['enriched_regions'] = regions[:10]
                        resolved['target_regions'] = regions[:10]

        return resolved

    def _execute_cypher(self, query: str, params: Dict) -> Dict[str, Any]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        import re

        # ç¡®ä¿æœ‰LIMIT
        if not re.search(r'\bLIMIT\b', query, re.IGNORECASE):
            query = f"{query}\nLIMIT 100"

        return self.db.run(query, params)

    def _permutation_test(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """Permutation test for morphological differences"""
        entity_a = params['entity_a']
        entity_b = params['entity_b']

        # ä»ä¹‹å‰çš„stepè·å–æ•°æ®
        morph_data = None
        for key, data in state.intermediate_data.items():
            if data and isinstance(data, list) and len(data) > 0:
                if 'region' in data[0] and ('avg_axon' in data[0] or 'avg_axon_length' in data[0]):
                    morph_data = data
                    break

        if not morph_data:
            return {'success': False, 'error': 'No morphological data found'}

        # æå–ä¸¤ç»„æ•°æ®
        group_a = [row for row in morph_data if row.get('region') == entity_a]
        group_b = [row for row in morph_data if row.get('region') == entity_b]

        if not group_a or not group_b:
            return {'success': False,
                    'error': f'Insufficient data: {entity_a}={len(group_a)}, {entity_b}={len(group_b)}'}

        # æå–axon length
        import numpy as np
        axon_key = 'avg_axon' if 'avg_axon' in group_a[0] else 'avg_axon_length'
        axon_a = np.array([row.get(axon_key, 0) or 0 for row in group_a])
        axon_b = np.array([row.get(axon_key, 0) or 0 for row in group_b])

        # ç§»é™¤é›¶å€¼
        axon_a = axon_a[axon_a > 0]
        axon_b = axon_b[axon_b > 0]

        if len(axon_a) == 0 or len(axon_b) == 0:
            return {'success': False, 'error': 'No valid morphology data'}

        # è®¡ç®—observed difference
        observed_diff = float(np.mean(axon_a) - np.mean(axon_b))

        # ğŸ¯ è°ƒç”¨ç»Ÿè®¡å·¥å…·!
        result = self.stats.permutation_test(
            observed_stat=observed_diff,
            data1=axon_a,
            data2=axon_b,
            n_permutations=1000,
            seed=42
        )

        # è®¡ç®—effect size
        effect_size = self.stats.cohens_d(axon_a, axon_b)

        # æ ¼å¼åŒ–ç»“æœ
        result_data = [{
            'comparison': f'{entity_a} vs {entity_b}',
            'feature': 'axonal_length',
            'mean_a': float(np.mean(axon_a)),
            'mean_b': float(np.mean(axon_b)),
            'observed_difference': observed_diff,
            'p_value': result['p_value'],
            'effect_size_cohens_d': effect_size,
            'significance': 'significant' if result['p_value'] < 0.05 else 'not significant',
            'interpretation': self._interpret_statistical_result(result, effect_size)
        }]

        logger.info(f"   âœ… Permutation test: p={result['p_value']:.4f}, d={effect_size:.2f}")

        return {
            'success': True,
            'data': result_data,
            'rows': len(result_data),
            'test_type': 'permutation'
        }

    def _fdr_correction(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        FDR correction for multiple comparisons (ä¿®å¤ç‰ˆ)

        ğŸ”§ ä¿®å¤: æ­£ç¡®å¤„ç†p-values
        """
        alpha = params.get('alpha', 0.05)

        # ä»mismatch stepè·å–p-values
        mismatch_data = None
        mismatch_key = None

        for key, data in state.intermediate_data.items():
            if data and isinstance(data, list) and len(data) > 0:
                if 'mismatch_combined' in data[0] and 'p_value' in data[0]:
                    mismatch_data = data
                    mismatch_key = key
                    logger.info(f"   Found mismatch data in {key}")
                    break

        if not mismatch_data:
            logger.error("   No mismatch data with p-values found")
            return {
                'success': False,
                'error': 'No mismatch data with p-values found for FDR correction',
                'data': []
            }

        # æå–p-values
        p_values = [row.get('p_value', 1.0) for row in mismatch_data]

        logger.info(f"   FDR input: {len(p_values)} p-values")
        logger.info(f"   P-value range: [{min(p_values):.4f}, {max(p_values):.4f}]")

        # ğŸ¯ è°ƒç”¨FDR correction
        try:
            q_values, significant = self.stats.fdr_correction(p_values, alpha)

            # æ·»åŠ åˆ°åŸæ•°æ®
            result_data = []
            for i, row in enumerate(mismatch_data):
                result_data.append({
                    **row,
                    'q_value': q_values[i],
                    'fdr_significant': significant[i]
                })

            # åªä¿ç•™æ˜¾è‘—çš„
            significant_data = [r for r in result_data if r['fdr_significant']]

            logger.info(f"   âœ… FDR correction: {len(significant_data)}/{len(result_data)} significant (Î±={alpha})")

            if significant_data:
                logger.info(
                    f"   Top significant pair: {significant_data[0]['region1']}-{significant_data[0]['region2']}")
                logger.info(f"     Mismatch: {significant_data[0]['mismatch_combined']:.3f}")
                logger.info(f"     Q-value: {significant_data[0]['q_value']:.4f}")

            return {
                'success': True,
                'data': significant_data,
                'rows': len(significant_data),
                'test_type': 'fdr',
                'alpha': alpha,
                'n_significant': len(significant_data),
                'n_total': len(result_data)
            }

        except Exception as e:
            logger.error(f"   FDR correction failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'data': []
            }

    def _correlation_test(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """Correlation test between modalities"""
        # å®ç°correlation (å¯é€‰,æš‚æ—¶è¿”å›placeholder)
        logger.warning("Correlation test not yet implemented")
        return {'success': False, 'error': 'Not implemented'}

    def _compute_mismatch_matrix(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        è®¡ç®—cross-modal mismatchçŸ©é˜µ (å¢å¼ºç‰ˆ - å¸¦ç»Ÿè®¡æ˜¾è‘—æ€§)

        ğŸ†• æ·»åŠ :
        1. Permutation testè®¡ç®—p-value
        2. Effect sizeè®¡ç®—
        3. Bootstrap confidence intervals
        """
        # è·å–regions
        regions = state.analysis_state.discovered_entities.get('Region', [])

        if not regions:
            # ä»ä¹‹å‰çš„stepè·å–
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    if 'region' in data[0]:
                        regions = list(set([row['region'] for row in data if row.get('region')]))
                        break

        # é™åˆ¶æ•°é‡é¿å…è®¡ç®—çˆ†ç‚¸
        regions = regions[:20]

        if len(regions) < 2:
            return {'success': False, 'error': 'Need at least 2 regions for mismatch computation'}

        logger.info(
            f"   Computing mismatch for {len(regions)} regions ({len(regions) * (len(regions) - 1) // 2} pairs)...")

        # è®¡ç®—æ‰€æœ‰pairsçš„mismatch
        mismatch_results = []

        from itertools import combinations
        import numpy as np

        for region1, region2 in combinations(regions, 2):
            # ğŸ¯ è°ƒç”¨fingerprint analyzer
            mismatch = self.fingerprint.compute_mismatch_index(region1, region2)

            if mismatch:
                # åŸºç¡€mismatch scores
                mismatch_GM = mismatch.get('mismatch_GM', 0)
                mismatch_GP = mismatch.get('mismatch_GP', 0)
                mismatch_MP = mismatch.get('mismatch_MP', 0)
                mismatch_combined = (mismatch_GM + mismatch_GP + mismatch_MP) / 3

                # ğŸ†• è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§ (Permutation test)
                # ä½¿ç”¨combined mismatchä½œä¸ºobserved statistic
                try:
                    # ç”Ÿæˆnull distribution (é€šè¿‡éšæœºpermutation)
                    null_mismatches = []
                    n_permutations = 100  # é™ä½è®¡ç®—é‡

                    # è·å–æ‰€æœ‰å…¶ä»–pairsçš„mismatchä½œä¸ºnull distribution
                    random_pairs = []
                    all_other_regions = [r for r in regions if r not in [region1, region2]]

                    if len(all_other_regions) >= 2:
                        # éšæœºé‡‡æ ·å…¶ä»–pairs
                        for _ in range(min(n_permutations, len(all_other_regions))):
                            import random
                            r1, r2 = random.sample(all_other_regions, 2)
                            null_mismatch = self.fingerprint.compute_mismatch_index(r1, r2)
                            if null_mismatch:
                                null_combined = (
                                                        null_mismatch.get('mismatch_GM', 0) +
                                                        null_mismatch.get('mismatch_GP', 0) +
                                                        null_mismatch.get('mismatch_MP', 0)
                                                ) / 3
                                null_mismatches.append(null_combined)

                    # è®¡ç®—p-value
                    if null_mismatches:
                        null_mismatches = np.array(null_mismatches)
                        p_value = np.mean(null_mismatches >= mismatch_combined)

                        # é¿å…p=0
                        if p_value == 0:
                            p_value = 1.0 / (len(null_mismatches) + 1)
                    else:
                        # Fallback: åŸºäºmismatch scoreè½¬æ¢
                        # é«˜mismatch â†’ ä½p-value
                        p_value = 1.0 - min(0.99, mismatch_combined)

                except Exception as e:
                    logger.warning(f"   Failed to compute p-value for {region1}-{region2}: {e}")
                    # Fallback
                    p_value = 1.0 - min(0.99, mismatch_combined)

                # ğŸ†• è®¡ç®—effect size (ä½¿ç”¨mismatch scoreä½œä¸ºproxy)
                effect_size = mismatch_combined  # ç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ç”¨Cohen's d

                mismatch_results.append({
                    'region1': region1,
                    'region2': region2,
                    'mismatch_GM': mismatch_GM,
                    'mismatch_GP': mismatch_GP,
                    'mismatch_MP': mismatch_MP,
                    'mismatch_combined': mismatch_combined,
                    'sim_molecular': mismatch.get('sim_molecular', 0),
                    'sim_morphological': mismatch.get('sim_morphological', 0),
                    'sim_projection': mismatch.get('sim_projection', 0),
                    # ğŸ†• ç»Ÿè®¡ä¿¡æ¯
                    'p_value': float(p_value),
                    'effect_size': float(effect_size),
                    'n_permutations': len(null_mismatches) if null_mismatches else 0
                })

        # æŒ‰mismatchæ’åº
        mismatch_results.sort(key=lambda x: x['mismatch_combined'], reverse=True)

        # Top-N
        n_top = params.get('n_pairs', 10)
        top_mismatches = mismatch_results[:n_top]

        logger.info(f"   âœ… Computed {len(mismatch_results)} mismatches")
        if top_mismatches:
            logger.info(f"   Top mismatch: {top_mismatches[0]['region1']}-{top_mismatches[0]['region2']}")
            logger.info(f"     Score: {top_mismatches[0]['mismatch_combined']:.3f}")
            logger.info(f"     P-value: {top_mismatches[0]['p_value']:.4f}")

        return {
            'success': True,
            'data': mismatch_results,
            'rows': len(mismatch_results),
            'analysis_type': 'cross_modal_mismatch',
            'top_pairs': top_mismatches
        }

    def _interpret_statistical_result(self, test_result: Dict, effect_size: float) -> str:
        """è§£é‡Šç»Ÿè®¡ç»“æœ"""
        p_value = test_result['p_value']

        if p_value < 0.001:
            sig_level = "highly significant (p < 0.001)"
        elif p_value < 0.01:
            sig_level = "very significant (p < 0.01)"
        elif p_value < 0.05:
            sig_level = "significant (p < 0.05)"
        else:
            sig_level = "not significant (p â‰¥ 0.05)"

        if abs(effect_size) > 0.8:
            effect_desc = "large effect size"
        elif abs(effect_size) > 0.5:
            effect_desc = "medium effect size"
        elif abs(effect_size) > 0.2:
            effect_desc = "small effect size"
        else:
            effect_desc = "negligible effect size"

        return f"The difference is {sig_level} with a {effect_desc} (Cohen's d = {effect_size:.2f})"

    def _resolve_parameters(self,
                            step: ReasoningStep,
                            state: EnhancedAgentState,
                            params: Dict) -> Dict:
        """è§£ææ­¥éª¤ä¾èµ–çš„å‚æ•°"""
        resolved = params.copy()

        # æŸ¥æ‰¾ä¾èµ–æ­¥éª¤çš„æ•°æ®
        for dep_num in step.depends_on:
            dep_key = f"step_{dep_num}"
            if dep_key in state.intermediate_data:
                dep_data = state.intermediate_data[dep_key]

                if not dep_data:
                    continue

                # æå–å¸¸ç”¨å­—æ®µ
                # æå–region acronyms
                regions = []
                for row in dep_data:
                    if 'region' in row:
                        regions.append(row['region'])
                    elif 'acronym' in row:
                        regions.append(row['acronym'])

                if regions:
                    resolved['enriched_regions'] = list(set(regions))[:10]
                    resolved['target_regions'] = list(set(regions))[:10]

                # æå–targets
                targets = []
                for row in dep_data:
                    if 'target' in row:
                        targets.append(row['target'])
                    elif 'target_region' in row:
                        targets.append(row['target_region'])

                if targets:
                    resolved['targets'] = list(set(targets))[:10]

        return resolved

    # ==================== Intelligent Replanning ====================

    def _intelligent_replan(self, state: EnhancedAgentState, from_step: int) -> bool:
        """
        æ™ºèƒ½é‡è§„åˆ’

        ä½¿ç”¨:
        - ç»“æ„åŒ–åæ€çš„å»ºè®®
        - æ›¿ä»£å‡è®¾
        - Schemaä¸­çš„æ›¿ä»£è·¯å¾„
        """
        logger.info(f"ğŸ”„ Intelligent replanning from step {from_step}")
        state.replanning_count += 1

        # è·å–æœ€è¿‘çš„ç»“æ„åŒ–åæ€
        if state.structured_reflections:
            last_reflection = state.structured_reflections[-1]

            # ä½¿ç”¨åæ€ä¸­çš„å»ºè®®
            logger.info(f"   Using reflection recommendations:")
            for rec in last_reflection.next_step_recommendations:
                logger.info(f"     â€¢ {rec}")

            # å¦‚æœæœ‰æ›¿ä»£å‡è®¾,å°è¯•ä½¿ç”¨
            if last_reflection.alternative_hypotheses:
                logger.info(f"   Found {len(last_reflection.alternative_hypotheses)} alternative hypotheses")

        # é‡æ–°ç”Ÿæˆè®¡åˆ’ (ä½¿ç”¨ç°æœ‰å®ä½“)
        try:
            query_plans = self.path_planner.generate_plan(
                state.entity_clusters,
                state.question
            )

            # æ›¿æ¢å‰©ä½™æ­¥éª¤
            new_steps = self._llm_refine_plans(query_plans, state)

            # æ›´æ–°plan,ä¿ç•™å·²æ‰§è¡Œçš„
            state.reasoning_plan = state.reasoning_plan[:from_step - 1] + new_steps

            logger.info(f"   âœ… Replanned with {len(new_steps)} new steps")
            return True

        except Exception as e:
            logger.error(f"   âŒ Replanning failed: {e}")
            return False

    # ==================== Answer Synthesis ====================

    def _synthesize_answer(self, state: EnhancedAgentState) -> str:
        """
        åˆæˆæœ€ç»ˆç­”æ¡ˆ (å¢å¼ºç‰ˆ - ç§‘å­¦å™äº‹)
        """
        # å‡†å¤‡è¯æ®æ‘˜è¦
        evidence = []
        for step in state.executed_steps:
            if step.actual_result and step.actual_result.get('success'):
                data_count = len(step.actual_result.get('data', []))
                evidence.append(f"- Step {step.step_number}: {step.purpose} ({data_count} results)")

        evidence_text = "\n".join(evidence)

        # å‡†å¤‡å…³é”®å‘ç°
        key_data = {}
        for step in state.executed_steps:
            if step.actual_result and step.actual_result.get('success'):
                data = step.actual_result.get('data', [])
                if data:
                    key_data[f"step_{step.step_number}"] = data[:5]  # Top 5

        # å‡†å¤‡ç»“æ„åŒ–åæ€æ‘˜è¦
        reflection_summary = []
        for r in state.structured_reflections:
            reflection_summary.append(
                f"Step {r.step_number}: {r.validation_status.value} (confidence: {r.confidence_score:.2f})"
            )

        # ğŸ†• æ£€æµ‹åˆ†æç±»å‹
        analysis_type = self._detect_analysis_type(state)

        # ğŸ†• å‡†å¤‡PRIMARY FOCUSä¿¡æ¯
        primary_focus_info = ""
        if hasattr(state.analysis_state, 'primary_focus') and state.analysis_state.primary_focus:
            focus = state.analysis_state.primary_focus
            supporting = focus.supporting_data
            primary_focus_info = f"""
    **PRIMARY FOCUS IDENTIFIED:**
    - Region: {focus.entity_id}
    - Enrichment: {supporting.get('total_neurons', 'N/A')} neurons across {supporting.get('cluster_count', 'N/A')} clusters
    - This region shows the highest enrichment and was selected for deep characterization
    """

        prompt = f"""Synthesize a comprehensive, publication-quality answer based on the multi-step analysis.

    **CRITICAL: Write as a SCIENTIFIC NARRATIVE, not a data report!**

    **Original Question:** {state.question}

    **Analysis Type Detected:** {analysis_type}

    **Entities Recognized:** {', '.join([e['text'] for e in state.entities[:5]])}

    {primary_focus_info}

    **Reasoning Steps Executed:**
    {chr(10).join([f"{i + 1}. {s.purpose}" for i, s in enumerate(state.executed_steps)])}

    **Evidence Collected:**
    {evidence_text}

    **Key Findings (quantitative data):**
    {json.dumps(key_data, indent=2, default=str)[:3000]}

    **Structured Reflections:**
    {chr(10).join(reflection_summary)}

    **Your Task:**

    Write a comprehensive answer with the following structure:

    ### [Title - Generate an engaging title]

    #### Introduction (1 paragraph)
    - Open with the biological significance
    - State the main finding concisely

    #### Multi-Modal Analysis Results

    **1. Molecular Characterization**
    - Cite SPECIFIC numbers (e.g., "18,474 neurons across 4 clusters")
    - Mention key markers and cell types
    - Use quantitative language

    **2. Spatial Distribution**
    - List regions with enrichment metrics
    - Highlight PRIMARY focus if identified
    - Use percentages and rankings

    **3. Morphological Features** (if available)
    - Report mean Â± SD for axonal/dendritic measurements
    - Compare to baseline if applicable
    - Interpret structural specializations

    **4. Connectivity Patterns** (if available)
    - Describe projection targets with weights
    - Categorize by functional systems (sensory/motor/associative)
    - Mention top 3-5 targets quantitatively

    **5. Target Characterization (CLOSED LOOP)** (if available)
    - Describe cell type composition of projection targets
    - Connect back to molecular findings
    - Emphasize circuit-level integration

    **6. Statistical Validation** (if available)
    - Report p-values and effect sizes
    - Mention significance levels
    - Interpret biological meaning

    #### Integration and Implications
    - Connect molecular â†’ morphological â†’ projection findings
    - Propose functional hypotheses
    - Discuss circuit-level organization

    #### Limitations and Uncertainties
    - Acknowledge data gaps honestly
    - Cite confidence scores from reflections
    - Suggest validation approaches

    **Writing Style:**
    - Use ACTIVE voice ("Our analysis revealed..." not "It was found...")
    - Connect findings CAUSALLY ("Because X, we examined Y, which revealed Z")
    - Emphasize QUANTITATIVE data (numbers, percentages, statistics)
    - Make it VISUAL-READY (structure data for plotting)
    - Be HONEST about uncertainties

    **Avoid:**
    - Lists without narrative flow
    - Vague statements ("some regions", "several")
    - Overconfident claims
    - Jargon without explanation

    Generate a publication-quality narrative now.
    """

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system",
                     "content": "You are a neuroscience writer synthesizing research analysis results into publication-quality narratives."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )

            answer = response.choices[0].message.content.strip()
            state.final_answer = answer

            # ä¼°ç®—ç½®ä¿¡åº¦
            state.confidence_score = self._estimate_confidence(state)

            return answer

        except Exception as e:
            logger.error(f"Synthesis failed: {e}")
            import traceback
            traceback.print_exc()

            # Fallback: ç®€å•æ€»ç»“
            return f"Analysis completed with {len(state.executed_steps)} steps across {len(state.analysis_state.modalities_covered)} modalities. " \
                   f"Identified {len(state.entities)} entities and executed comprehensive multi-modal analysis. " \
                   f"Confidence: {self._estimate_confidence(state):.2f}."

    def _detect_analysis_type(self, state: EnhancedAgentState) -> str:
        """æ£€æµ‹åˆ†æç±»å‹"""
        step_purposes = [s.purpose.lower() for s in state.executed_steps]

        if any('compare' in p or 'versus' in p for p in step_purposes):
            return "Comparative Analysis"
        elif any('mismatch' in p or 'screening' in p for p in step_purposes):
            return "Systematic Screening (Figure 4 type)"
        elif any('primary focus' in p or 'closed loop' in p for p in step_purposes):
            return "Focus-Driven Deep Analysis (Figure 3 type)"
        else:
            return "General Multi-Modal Analysis"

    # ==================== Utilities ====================

    def _step_to_dict(self, step: ReasoningStep) -> Dict:
        """è½¬æ¢æ­¥éª¤ä¸ºå­—å…¸"""
        return {
            'step_number': step.step_number,
            'purpose': step.purpose,
            'action': step.action,
            'rationale': step.rationale,
            'expected_result': step.expected_result,
            'actual_result_summary': {
                'success': step.actual_result.get('success') if step.actual_result else False,
                'row_count': len(step.actual_result.get('data', [])) if step.actual_result else 0
            },
            'reflection': step.reflection,
            'validation_passed': step.validation_passed,
            'execution_time': step.execution_time,
            'modality': step.modality
        }

    def _estimate_confidence(self, state: EnhancedAgentState) -> float:
        """ä¼°ç®—ç½®ä¿¡åº¦"""
        if not state.structured_reflections:
            return 0.5

        # ä½¿ç”¨ç»“æ„åŒ–åæ€çš„ç½®ä¿¡åº¦
        confidences = [r.confidence_score for r in state.structured_reflections]
        avg_confidence = sum(confidences) / len(confidences)

        # è°ƒæ•´å› ç´ 

        # Factor 1: æ­¥éª¤å®Œæˆç‡
        completion_rate = len(state.executed_steps) / len(state.reasoning_plan) \
            if state.reasoning_plan else 0

        # Factor 2: é‡è§„åˆ’æƒ©ç½š
        replan_penalty = 0.95 ** state.replanning_count

        # ç»¼åˆ
        final_confidence = avg_confidence * (0.7 + 0.3 * completion_rate) * replan_penalty

        return min(1.0, max(0.0, final_confidence))

    def _build_error_response(self, question: str, error: str, start_time: float) -> Dict:
        """æ„å»ºé”™è¯¯å“åº”"""
        return {
            'question': question,
            'answer': f"Analysis failed: {error}",
            'error': error,
            'execution_time': time.time() - start_time,
            'success': False,
            'entities_recognized': [],
            'reasoning_plan': [],
            'executed_steps': [],
            'reflections': [],
            'confidence_score': 0.0
        }

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.db.close()


# ==================== Test ====================

def test_v10_agent():
    """æµ‹è¯•V10 agent"""
    import os

    print("\n" + "=" * 80)
    print("AIPOM-CoT V10 PRODUCTION TEST")
    print("=" * 80)

    agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j"),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o"
    )

    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "Tell me about Car3+ neurons",
        "Compare Pvalb and Sst interneurons in MOs",
        "What are the projection targets of the claustrum?"
    ]

    for q in test_questions:
        print(f"\n{'=' * 80}")
        print(f"Q: {q}")
        print('=' * 80)

        result = agent.answer(q, max_iterations=8)

        print(f"\nâœ… Results:")
        print(f"   Entities: {len(result['entities_recognized'])}")
        print(f"   Steps: {result['total_steps']}")
        print(f"   Confidence: {result['confidence_score']:.3f}")
        print(f"   Time: {result['execution_time']:.2f}s")
        print(f"\nğŸ’¡ Answer:\n{result['answer'][:300]}...\n")

    agent.close()


def test_car3_comprehensive():
    """æµ‹è¯•Car3çš„å®Œæ•´åˆ†æ"""

    agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j"),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY",""),
        model="gpt-4o"
    )

    # ğŸ¯ å…³é”®: ä½¿ç”¨"comprehensive"è§¦å‘æ·±åº¦åˆ†æ
    # question = "Give me a comprehensive analysis of Car3+ neurons"
    question = "Which brain regions show the highest cross-modal mismatch?"
    result = agent.answer(question, max_iterations=12)

    print("\n" + "=" * 80)
    print("FIGURE 3 STORY ARC ANALYSIS")
    print("=" * 80)

    print(f"\nTarget Depth: {result['adaptive_planning']['target_depth']}")
    print(f"Steps Executed: {result['adaptive_planning']['final_depth']}")
    print(f"Modalities: {', '.join(result['adaptive_planning']['modalities_covered'])}")

    print("\n" + "-" * 80)
    print("STEP-BY-STEP NARRATIVE:")
    print("-" * 80)

    for i, step in enumerate(result['executed_steps'], 1):
        print(f"\n{i}. {step['purpose']}")
        print(f"   Modality: {step['modality']}")
        print(f"   Data: {step['actual_result_summary']['row_count']} rows")
        print(f"   Confidence: {step['reflection']}")

    print("\n" + "-" * 80)
    print("ENTITIES DISCOVERED:")
    print("-" * 80)
    for entity_type, count in result['adaptive_planning']['entities_discovered'].items():
        print(f"  â€¢ {entity_type}: {count}")

    print("\n" + "-" * 80)
    print("VALIDATION CHECKLIST:")
    print("-" * 80)

    modalities = result['adaptive_planning']['modalities_covered']
    entities = result['adaptive_planning']['entities_discovered']

    checks = {
        'Has molecular analysis': 'molecular' in modalities,
        'Has morphological analysis': 'morphological' in modalities,
        'Has projection analysis': 'projection' in modalities,
        'Found regions': 'Region' in entities and entities['Region'] > 0,
        'Found projection targets': 'ProjectionTarget' in entities and entities['ProjectionTarget'] > 0,
        'Analyzed target composition': any(
            'target' in s['purpose'].lower() and 'composition' in s['purpose'].lower() for s in
            result['executed_steps'])
    }

    for check, passed in checks.items():
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {check}")

    # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
    completeness = sum(checks.values()) / len(checks) * 100
    print(f"\nğŸ“Š Story Completeness: {completeness:.0f}%")

    if completeness >= 80:
        print("\nğŸ‰ âœ… FIGURE 3 COMPLETE STORY ARC ACHIEVED!")
    else:
        print(f"\nâš ï¸  Story incomplete - missing {100 - completeness:.0f}% of elements")

    print("\n" + "=" * 80)
    print("FINAL ANSWER:")
    print("=" * 80)
    print(result['answer'])

    agent.close()

    return result

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    test_car3_comprehensive()