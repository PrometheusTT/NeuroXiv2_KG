"""
AIPOM-CoT V10 PRODUCTION
========================
å®Œæ•´é›†æˆæ‰€æœ‰P0å’ŒP1ç»„ä»¶:
âœ… P0-1: æ™ºèƒ½å®ä½“è¯†åˆ« (IntelligentEntityRecognizer)
âœ… P0-2: Benchmarkè¯„ä¼°ç³»ç»Ÿ (BenchmarkRunner)
âœ… P1-1: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’ (DynamicSchemaPathPlanner)
âœ… P1-2: ç»“æ„åŒ–åæ€ (StructuredReflector)

è¿™æ˜¯ç”Ÿäº§å°±ç»ªç‰ˆæœ¬,å¯ä»¥ç›´æ¥ç”¨äº:
- å®Œæ•´Benchmarkè¯„ä¼°
- è®ºæ–‡Figure 3/4/5å¤ç°
- ä¸baselineå¯¹æ¯”

Author: Claude & PrometheusTT
Date: 2025-01-12
"""

import json
import logging
import os
import time
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field
import re
from pathlib import Path

import numpy as np
from scipy import stats
from neo4j_exec import Neo4jExec
from adaptive_planner import AdaptivePlanner, AnalysisDepth, AnalysisState
from aipom_cot_true_agent_v2 import (
    RealSchemaCache,
    StatisticalTools,
    AgentPhase,
    AgentState,
    ReasoningStep
)

# å¯¼å…¥æ–°ç»„ä»¶
from intelligent_entity_recognition import (
    IntelligentEntityRecognizer,
    EntityClusteringEngine
)
from schema_path_planner import DynamicSchemaPathPlanner
from structured_reflection import StructuredReflector

try:
    from openai import OpenAI
except ImportError:
    raise ImportError("Please install openai: pip install openai")

logger = logging.getLogger(__name__)


# ==================== Enhanced Agent State ====================

@dataclass
class EnhancedAgentState(AgentState):
    """æ‰©å±•çš„AgentçŠ¶æ€"""

    # æ–°å¢å­—æ®µ
    entity_matches: List = field(default_factory=list)  # EntityMatchåˆ—è¡¨
    entity_clusters: List = field(default_factory=list)  # EntityClusteråˆ—è¡¨
    structured_reflections: List = field(default_factory=list)  # StructuredReflectionåˆ—è¡¨
    schema_paths_used: List = field(default_factory=list)  # ä½¿ç”¨çš„schemaè·¯å¾„

class RealFingerprintAnalyzer:
    """
    Multi-modal fingerprint analysis adapted to REAL schema

    Key changes from V8:
    - Molecular: Use Cluster nodes and HAS_CLUSTER relationships
    - Morphological: Aggregate from Neuron nodes via LOCATE_AT
    - Projection: Use PROJECT_TO (unchanged, but verify properties)
    """

    def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
        self.db = db
        self.schema = schema
        self._cluster_cache = None
        self._target_cache = None

    def compute_region_fingerprint(self, region: str) -> Optional[Dict[str, np.ndarray]]:
        """
        Compute tri-modal fingerprint for a region

        Returns:
            {
                'molecular': np.ndarray,    # Cluster composition
                'morphological': np.ndarray, # Aggregated neuron features
                'projection': np.ndarray     # Target distribution
            }
        """
        fingerprint = {}

        # Molecular fingerprint
        mol_fp = self._compute_molecular_fingerprint(region)
        if mol_fp is not None:
            fingerprint['molecular'] = mol_fp

        # Morphological fingerprint
        mor_fp = self._compute_morphological_fingerprint(region)
        if mor_fp is not None:
            fingerprint['morphological'] = mor_fp

        # Projection fingerprint
        proj_fp = self._compute_projection_fingerprint(region)
        if proj_fp is not None:
            fingerprint['projection'] = proj_fp

        return fingerprint if len(fingerprint) > 0 else None

    def compute_molecular_fingerprint(self, region: str) -> Optional[np.ndarray]:
        """
        Molecular fingerprint = cluster composition

        Uses REAL schema:
        MATCH (r:Region {acronym: $region})-[h:HAS_CLUSTER]->(c:Cluster)
        """
        query = """
        MATCH (r:Region {acronym: $acronym})-[h:HAS_CLUSTER]->(c:Cluster)
        RETURN c.name AS cluster_name,
               c.markers AS markers,
               c.number_of_neurons AS neuron_count
        ORDER BY c.name
        """

        result = self.db.run(query, {'acronym': region})

        if not result['success'] or not result['data']:
            return None

        # Get all clusters
        all_clusters = self._get_all_clusters()

        # Build vector: neuron count for each cluster
        cluster_dict = {
            row['cluster_name']: row['neuron_count'] or 0
            for row in result['data']
        }

        vector = np.array([cluster_dict.get(c, 0.0) for c in all_clusters])

        # Normalize
        total = np.sum(vector)
        if total > 0:
            vector = vector / total

        return vector

    def compute_morphological_fingerprint(self, region: str) -> Optional[np.ndarray]:
        """
        Morphological fingerprint = aggregated neuron features

        Uses REAL schema:
        MATCH (n:Neuron)-[:LOCATE_AT]->(r:Region {acronym: $region})
        RETURN avg(n.axonal_length), avg(n.dendritic_length), ...
        """
        query = """
        MATCH (n:Neuron)-[:LOCATE_AT]->(r:Region {acronym: $acronym})
        RETURN 
            avg(n.axonal_length) AS avg_axon_len,
            avg(n.dendritic_length) AS avg_dend_len,
            avg(n.axonal_surface) AS avg_axon_surf,
            avg(n.dendritic_surface) AS avg_dend_surf,
            avg(n.number_of_stems) AS avg_stems,
            avg(n.soma_surface) AS avg_soma
        """

        result = self.db.run(query, {'acronym': region})

        if not result['success'] or not result['data'] or not result['data'][0]:
            return None

        data = result['data'][0]

        # Build feature vector
        vector = np.array([
            data.get('avg_axon_len') or 0.0,
            data.get('avg_dend_len') or 0.0,
            data.get('avg_axon_surf') or 0.0,
            data.get('avg_dend_surf') or 0.0,
            data.get('avg_stems') or 0.0,
            data.get('avg_soma') or 0.0
        ], dtype=float)

        # L2 normalize
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm

        return vector

    def compute_projection_fingerprint(self, region: str) -> Optional[np.ndarray]:
        """
        Projection fingerprint = target distribution

        Uses PROJECT_TO relationship (same as before)
        """
        query = """
        MATCH (r:Region {acronym: $acronym})-[p:PROJECT_TO]->(t:Region)
        RETURN t.acronym AS target, p.weight AS weight
        ORDER BY t.acronym
        """

        result = self.db.run(query, {'acronym': region})

        if not result['success'] or not result['data']:
            return None

        all_targets = self._get_all_targets()

        target_dict = {row['target']: row['weight'] or 0.0 for row in result['data']}
        vector = np.array([target_dict.get(t, 0.0) for t in all_targets])

        # Normalize
        total = np.sum(vector)
        if total > 0:
            vector = vector / total

        return vector

    def compute_similarity(self, fp1: np.ndarray, fp2: np.ndarray,
                          metric: str = 'cosine') -> float:
        """Compute similarity between fingerprints"""
        if metric == 'cosine':
            norm1, norm2 = np.linalg.norm(fp1), np.linalg.norm(fp2)
            if norm1 == 0 or norm2 == 0:
                return 0.0
            return float(np.dot(fp1, fp2) / (norm1 * norm2))
        elif metric == 'correlation':
            if len(fp1) < 2:
                return 0.0
            r, _ = stats.pearsonr(fp1, fp2)
            return float(r)
        else:
            return 0.0

    def compute_mismatch_index(self, region1: str, region2: str) -> Optional[Dict[str, float]]:
        """
        Compute cross-modal mismatch (Figure 4 metric)

        MM_GM = |sim_molecular - sim_morphological|
        MM_GP = |sim_molecular - sim_projection|
        """
        fp1 = self.compute_region_fingerprint(region1)
        fp2 = self.compute_region_fingerprint(region2)

        if fp1 is None or fp2 is None:
            return None

        sim_mol = self.compute_similarity(fp1['molecular'], fp2['molecular'])
        sim_mor = self.compute_similarity(fp1['morphological'], fp2['morphological'])
        sim_proj = self.compute_similarity(fp1['projection'], fp2['projection'])

        return {
            'sim_molecular': sim_mol,
            'sim_morphological': sim_mor,
            'sim_projection': sim_proj,
            'mismatch_GM': abs(sim_mol - sim_mor),
            'mismatch_GP': abs(sim_mol - sim_proj),
            'mismatch_MP': abs(sim_mor - sim_proj)
        }

    def _get_all_clusters(self) -> List[str]:
        """Get all cluster names for consistent dimensions"""
        if self._cluster_cache is not None:
            return self._cluster_cache

        query = "MATCH (c:Cluster) RETURN c.name AS name ORDER BY c.name LIMIT 100"
        result = self.db.run(query)

        if result['success'] and result['data']:
            self._cluster_cache = [row['name'] for row in result['data']]
        else:
            self._cluster_cache = []

        return self._cluster_cache

    def _get_all_targets(self) -> List[str]:
        """Get all projection targets"""
        if self._target_cache is not None:
            return self._target_cache

        query = """
        MATCH ()-[:PROJECT_TO]->(t:Region)
        RETURN DISTINCT t.acronym AS target
        ORDER BY target
        LIMIT 100
        """
        result = self.db.run(query)

        if result['success'] and result['data']:
            self._target_cache = [row['target'] for row in result['data']]
        else:
            self._target_cache = []

        return self._target_cache

    def get_region_fingerprint(self, region: str) -> Dict:
        """
        è·å–å•ä¸ªregionçš„å®Œæ•´fingerprint

        ğŸ†• æ–°å¢æ–¹æ³• - æ”¯æŒé«˜æ€§èƒ½ç‰ˆæœ¬çš„æ‰¹é‡è®¡ç®—

        Args:
            region: è„‘åŒºacronym

        Returns:
            {
                'molecular': [array],
                'morphological': [array],
                'projection': [array]
            }
        """
        try:
            # è®¡ç®—ä¸‰ç§fingerprint
            molecular = self._compute_molecular_fingerprint(region)
            morphological = self._compute_morphological_fingerprint(region)
            projection = self._compute_projection_fingerprint(region)

            # éªŒè¯
            if molecular is None or morphological is None or projection is None:
                return None

            # è½¬æ¢ä¸ºlist (ç¡®ä¿JSONå¯åºåˆ—åŒ–)
            return {
                'molecular': molecular.tolist() if hasattr(molecular, 'tolist') else list(molecular),
                'morphological': morphological.tolist() if hasattr(morphological, 'tolist') else list(morphological),
                'projection': projection.tolist() if hasattr(projection, 'tolist') else list(projection)
            }

        except Exception as e:
            logger.error(f"Failed to get fingerprint for {region}: {e}")
            return None
# ==================== Production Agent V10 ====================

class AIPOMCoTV10:
    """
    AIPOM-CoT V10 ç”Ÿäº§ç‰ˆæœ¬

    å®Œæ•´åŠŸèƒ½:
    1. æ™ºèƒ½å®ä½“è¯†åˆ« (æ— éœ€hardcodedåˆ—è¡¨)
    2. åŠ¨æ€Schemaè·¯å¾„è§„åˆ’ (å›¾ç®—æ³•)
    3. ç»“æ„åŒ–åæ€ (é‡åŒ–è¯„ä¼°)
    4. å®Œæ•´ç»Ÿè®¡å·¥å…·
    5. å¤šæ¨¡æ€åˆ†æ
    6. è‡ªé€‚åº”é‡è§„åˆ’
    """

    def __init__(self,
                 neo4j_uri: str,
                 neo4j_user: str,
                 neo4j_pwd: str,
                 database: str,
                 schema_json_path: str,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4o"):

        # æ•°æ®åº“è¿æ¥
        self.db = Neo4jExec(neo4j_uri, neo4j_user, neo4j_pwd, database=database)

        # Schema
        self.schema = RealSchemaCache(schema_json_path)

        # ===== æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– =====

        # P0-1: æ™ºèƒ½å®ä½“è¯†åˆ«
        logger.info("ğŸ” Initializing intelligent entity recognition...")
        self.entity_recognizer = IntelligentEntityRecognizer(self.db, self.schema)
        self.entity_clusterer = EntityClusteringEngine(self.db, self.schema)

        # P1-1: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
        logger.info("ğŸ—ºï¸  Initializing dynamic schema path planning...")
        self.path_planner = DynamicSchemaPathPlanner(self.schema)

        # P1-2: ç»“æ„åŒ–åæ€
        logger.info("ğŸ¤” Initializing structured reflection...")
        self.reflector = StructuredReflector()

        # åŸæœ‰ç»„ä»¶
        self.stats = StatisticalTools()
        self.fingerprint = RealFingerprintAnalyzer(self.db, self.schema)

        # OpenAI
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model


        self.adaptive_planner = AdaptivePlanner(self.schema, self.path_planner,self.client)
        # ğŸ†• æ·»åŠ Focus-Driven Planner
        logger.info("ğŸ¯ Initializing focus-driven planning...")
        from focus_driven_planner import FocusDrivenPlanner
        self.focus_planner = FocusDrivenPlanner(self.schema, self.db)

        # ğŸ†• æ·»åŠ Comparative Analysis Planner
        logger.info("ğŸ“Š Initializing comparative analysis planning...")
        from comparative_analysis_planner import ComparativeAnalysisPlanner
        self.comparative_planner = ComparativeAnalysisPlanner(
            self.db,
            self.fingerprint,
            self.stats
        )

        logger.info("âœ… AIPOM-CoT V10 initialized successfully!")
        logger.info(f"   â€¢ Entity recognition: Ready")
        logger.info(f"   â€¢ Schema path planning: Ready")
        logger.info(f"   â€¢ Structured reflection: Ready")

    # ==================== Main Entry Point ====================

    """
    å®Œæ•´çš„answeræ–¹æ³•å®ç° - é›†æˆè‡ªé€‚åº”è§„åˆ’
    """

    def answer(self, question: str, max_iterations: int = 15) -> Dict[str, Any]:
        """
        ä¸»å…¥å£: å›ç­”é—®é¢˜ (å®Œæ•´ç‰ˆ)

        å®Œæ•´æµç¨‹:
        1. æ™ºèƒ½å®ä½“è¯†åˆ«
        2. å®ä½“èšç±»
        3. ç¡®å®šåˆ†ææ·±åº¦
        4. æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨ (Adaptive/Focus-Driven/Comparative)
        5. è‡ªé€‚åº”æ‰§è¡Œå¾ªç¯ (åŒ…å«ç»Ÿè®¡åˆ†æ)
        6. ç­”æ¡ˆåˆæˆ (ç§‘å­¦å™äº‹)
        """
        logger.info(f"ğŸ¯ Question: {question}")
        start_time = time.time()

        state = EnhancedAgentState(question=question)

        # ===== PHASE 1: INTELLIGENT PLANNING =====
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“‹ PHASE 1: INTELLIGENT PLANNING (Enhanced)")
        logger.info("=" * 70)

        state.phase = AgentPhase.PLANNING

        # Step 1-2: å®ä½“è¯†åˆ« + èšç±»
        logger.info("  [1/4] Intelligent entity recognition...")
        entity_matches = self.entity_recognizer.recognize_entities(question)
        state.entity_matches = entity_matches

        logger.info(f"     Found {len(entity_matches)} entity matches")
        for match in entity_matches[:5]:
            logger.info(f"       â€¢ {match.text} ({match.entity_type}) [{match.confidence:.2f}]")

        logger.info("  [2/4] Entity clustering...")
        entity_clusters = self.entity_clusterer.cluster_entities(entity_matches, question)
        state.entity_clusters = entity_clusters

        logger.info(f"     Created {len(entity_clusters)} entity clusters")
        for cluster in entity_clusters:
            logger.info(f"       â€¢ {cluster.cluster_type}: {cluster.primary_entity.text}")

        # ğŸ†• Step 3: ç¡®å®šåˆ†ææ·±åº¦
        from adaptive_planner import determine_analysis_depth, AnalysisState

        logger.info("  [3/4] Determining analysis depth...")
        target_depth = determine_analysis_depth(question)
        logger.info(f"     Target depth: {target_depth.value}")

        # ğŸ†• Step 4: åˆå§‹åŒ–åˆ†æçŠ¶æ€
        logger.info("  [4/4] Initializing analysis state...")

        analysis_state = AnalysisState(
            discovered_entities={},
            executed_steps=[],
            modalities_covered=[],
            current_focus='gene' if entity_clusters and entity_clusters[0].cluster_type == 'gene_marker' else 'region',
            target_depth=target_depth,
            question_intent=self._classify_question_intent(question)
        )

        # å¡«å……åˆå§‹å®ä½“
        for cluster in entity_clusters:
            entity_type = cluster.primary_entity.entity_type
            entity_id = cluster.primary_entity.entity_id

            analysis_state.discovered_entities.setdefault(entity_type, []).append(entity_id)

            for related in cluster.related_entities:
                analysis_state.discovered_entities.setdefault(
                    related.entity_type, []
                ).append(related.entity_id)

        # å…¼å®¹æ€§
        state.entities = [
            {'text': m.text, 'type': m.entity_type, 'confidence': m.confidence}
            for m in entity_matches[:10]
        ]

        # ğŸ†• å­˜å‚¨analysis_stateåˆ°state
        state.analysis_state = analysis_state

        logger.info(f"âœ… Planning complete")
        logger.info(f"   â€¢ Target depth: {target_depth.value}")
        logger.info(f"   â€¢ Initial entities: {list(analysis_state.discovered_entities.keys())}")

        # ===== PHASE 2: ADAPTIVE EXECUTION =====
        logger.info("\n" + "=" * 70)
        logger.info("âš™ï¸ PHASE 2: ADAPTIVE EXECUTION (Multi-Planner)")
        logger.info("=" * 70)

        state.phase = AgentPhase.EXECUTING

        iteration = 0
        while iteration < max_iterations:
            # ğŸ†• å†³å®šæ˜¯å¦ç»§ç»­
            if not self.adaptive_planner.should_continue(analysis_state, question):
                logger.info("ğŸ“Œ Analysis complete (adaptive decision)")
                break

            # ğŸ†• æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨
            planner_type = self._select_planner(analysis_state, question)

            if planner_type == 'focus_driven':
                logger.info(f"\nğŸ¯ Using FOCUS-DRIVEN planner (iteration {iteration + 1})...")
                next_steps = self.focus_planner.generate_focus_driven_plan(
                    analysis_state,
                    question
                )

            elif planner_type == 'comparative':
                logger.info(f"\nğŸ“Š Using COMPARATIVE planner (iteration {iteration + 1})...")
                next_steps = self.comparative_planner.generate_comparative_plan(
                    analysis_state,
                    question
                )

            else:
                logger.info(f"\nğŸ”„ Using ADAPTIVE planner (iteration {iteration + 1})...")
                next_steps = self.adaptive_planner.plan_next_steps(
                    analysis_state,
                    question,
                    max_steps=2
                )

            if not next_steps:
                logger.info("ğŸ“Œ No more steps available")
                break

            # æ‰§è¡Œè§„åˆ’çš„æ­¥éª¤
            for candidate_step in next_steps:
                if iteration >= max_iterations:
                    break

                logger.info(f"\nğŸ”¹ Step {iteration + 1}: {candidate_step.purpose}")
                logger.info(f"   Type: {candidate_step.step_type}")
                logger.info(f"   Priority: {candidate_step.priority:.1f}")
                if hasattr(candidate_step, 'llm_score') and candidate_step.llm_score > 0:
                    logger.info(f"   LLM score: {candidate_step.llm_score:.2f}")

                # ğŸ†• è½¬æ¢ä¸ºReasoningStep
                reasoning_step = self._convert_candidate_to_reasoning(
                    candidate_step,
                    iteration + 1,
                    analysis_state
                )

                # æ‰§è¡Œ
                exec_result = self._execute_step(reasoning_step, state)

                if not exec_result['success']:
                    logger.error(f"   âŒ Failed: {exec_result.get('error')}")

                    if state.replanning_count < state.max_replanning:
                        logger.info(f"   ğŸ”„ Replanning...")
                        state.replanning_count += 1

                    continue

                # ğŸ†• ç»“æ„åŒ–åæ€
                structured_reflection = self.reflector.reflect(
                    step_number=reasoning_step.step_number,
                    purpose=reasoning_step.purpose,
                    expected_result=reasoning_step.expected_result,
                    actual_result=reasoning_step.actual_result,
                    question_context=question
                )

                reasoning_step.reflection = structured_reflection.summary
                reasoning_step.validation_passed = (
                        structured_reflection.validation_status.value in ['passed', 'partial']
                )

                state.structured_reflections.append(structured_reflection)
                state.reflections.append(structured_reflection.summary)

                logger.info(f"   ğŸ“Š Reflection: {structured_reflection.summary}")
                logger.info(f"   ğŸ“ˆ Confidence: {structured_reflection.confidence_score:.3f}")

                # ğŸ†• æ›´æ–°åˆ†æçŠ¶æ€
                self._update_analysis_state(
                    analysis_state,
                    reasoning_step,
                    exec_result,
                    candidate_step
                )

                state.executed_steps.append(reasoning_step)
                iteration += 1

        # ===== PHASE 3: ANSWER SYNTHESIS =====
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“ PHASE 3: ANSWER SYNTHESIS")
        logger.info("=" * 70)

        final_answer = self._synthesize_answer(state)

        execution_time = time.time() - start_time

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'question': question,
            'answer': final_answer,

            'entities_recognized': [
                {
                    'text': m.text,
                    'type': m.entity_type,
                    'confidence': m.confidence,
                    'match_type': m.match_type
                }
                for m in state.entity_matches[:10]
            ],

            'reasoning_plan': [self._step_to_dict(s) for s in state.executed_steps],
            'executed_steps': [self._step_to_dict(s) for s in state.executed_steps],

            'reflections': state.reflections,
            'structured_reflections': [
                {
                    'step': r.step_number,
                    'status': r.validation_status.value,
                    'confidence': r.confidence_score,
                    'uncertainty': r.uncertainty.overall_uncertainty,
                    'should_replan': r.should_replan
                }
                for r in state.structured_reflections
            ],

            # ğŸ†• è‡ªé€‚åº”è§„åˆ’ä¿¡æ¯
            'adaptive_planning': {
                'target_depth': target_depth.value,
                'final_depth': len(state.executed_steps),
                'modalities_covered': analysis_state.modalities_covered,
                'entities_discovered': {
                    k: len(v) for k, v in analysis_state.discovered_entities.items()
                },
                'primary_focus': getattr(analysis_state, 'primary_focus', None)
            },

            'replanning_count': state.replanning_count,
            'confidence_score': state.confidence_score,
            'execution_time': execution_time,
            'total_steps': len(state.executed_steps),
            'schema_paths_used': state.schema_paths_used
        }

        logger.info(f"\nâœ… Completed in {execution_time:.2f}s")
        logger.info(f"   â€¢ Steps executed: {len(state.executed_steps)}")
        logger.info(f"   â€¢ Confidence: {state.confidence_score:.3f}")
        logger.info(f"   â€¢ Modalities: {', '.join(analysis_state.modalities_covered)}")

        return result

    # ==================== è¾…åŠ©æ–¹æ³• ====================
    def _select_planner(self, state, question: str) -> str:
        """
        æ™ºèƒ½é€‰æ‹©è§„åˆ’å™¨ (å¢å¼ºç‰ˆ - æ”¯æŒæ— entityçš„systematicæ¨¡å¼)

        ğŸ”§ å…³é”®æ”¹è¿›: æ£€æµ‹systematicå…³é”®è¯ï¼Œå³ä½¿æ²¡æœ‰åˆå§‹entities
        """
        q_lower = question.lower()

        # ğŸ” æ¯”è¾ƒæŸ¥è¯¢ â†’ Comparative
        compare_keywords = ['compare', 'versus', 'vs ', 'vs.', 'difference between', 'contrast']
        if any(kw in q_lower for kw in compare_keywords):
            logger.info(f"   Comparison keywords detected â†’ comparative")
            return 'comparative'

        # ğŸ”§ æ–°å¢: ç³»ç»Ÿç­›é€‰å…³é”®è¯ (ä¸ä¾èµ–åˆå§‹entities)
        systematic_keywords = [
            'which regions', 'which brain', 'find all', 'identify all',
            'screen', 'systematic', 'highest', 'top regions',
            'mismatch', 'show', 'exhibit', 'demonstrate'
        ]

        # æ£€æµ‹systematicæ¨¡å¼
        has_which = 'which' in q_lower
        has_highest = any(w in q_lower for w in ['highest', 'top', 'most', 'strongest'])
        has_mismatch = 'mismatch' in q_lower
        has_show = any(w in q_lower for w in ['show', 'exhibit', 'demonstrate', 'display'])

        # ğŸ¯ å…³é”®: Systematicæ¨¡å¼åˆ¤æ–­
        if has_which and (has_highest or has_mismatch or has_show):
            logger.info(f"   Systematic screening keywords detected â†’ comparative")
            logger.info(f"     Keywords: which={has_which}, highest={has_highest}, mismatch={has_mismatch}")
            return 'comparative'

        # æˆ–è€…ç›´æ¥æ£€æµ‹ç»„åˆ
        if any(kw in q_lower for kw in systematic_keywords):
            # è¿›ä¸€æ­¥ç¡®è®¤æ˜¯å¦æ˜¯ç­›é€‰ç±»é—®é¢˜
            screening_patterns = [
                'which.*show', 'which.*have', 'which.*exhibit',
                'find.*regions', 'identify.*regions',
                'highest.*mismatch', 'top.*mismatch'
            ]
            import re
            for pattern in screening_patterns:
                if re.search(pattern, q_lower):
                    logger.info(f"   Systematic pattern detected: {pattern} â†’ comparative")
                    return 'comparative'

        # ğŸ”§ Focus-driven: æœ‰regionsçš„æ·±åº¦æŸ¥è¯¢
        if 'Region' in state.discovered_entities:
            n_regions = len(state.discovered_entities.get('Region', []))
            if n_regions > 0:
                logger.info(f"   {n_regions} regions found â†’ focus-driven")
                return 'focus_driven'

        # ğŸ”§ Focus-driven: GeneæŸ¥è¯¢ä¸”æœ‰æ·±åº¦æ„å›¾
        if 'GeneMarker' in state.discovered_entities:
            deep_intent_keywords = ['tell me about', 'about', 'analyze', 'characterize', 'comprehensive']
            if any(kw in q_lower for kw in deep_intent_keywords):
                logger.info(f"   Gene query with deep intent â†’ focus-driven")
                return 'focus_driven'

        # é»˜è®¤: Adaptive
        logger.info(f"   Default â†’ adaptive")
        return 'adaptive'

    def _classify_question_intent(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜æ„å›¾"""
        question_lower = question.lower()

        if any(w in question_lower for w in ['compare', 'difference', 'versus', 'vs']):
            return 'comparison'
        elif any(w in question_lower for w in ['comprehensive', 'detailed', 'everything']):
            return 'comprehensive'
        elif any(w in question_lower for w in ['why', 'explain', 'how']):
            return 'explanatory'
        elif any(w in question_lower for w in ['which', 'find', 'identify']):
            return 'screening'
        else:
            return 'simple_query'



    def _update_analysis_state(self,
                               analysis_state,
                               step: ReasoningStep,
                               result: Dict,
                               candidate):
        """æ›´æ–°åˆ†æçŠ¶æ€"""
        # è®°å½•æ‰§è¡Œçš„æ­¥éª¤
        analysis_state.executed_steps.append({
            'purpose': step.purpose,
            'modality': step.modality,
            'row_count': len(result.get('data', [])),
            'step_id': candidate.step_id
        })

        # æ›´æ–°modalityè¦†ç›–
        if step.modality and step.modality not in analysis_state.modalities_covered:
            analysis_state.modalities_covered.append(step.modality)

        # ğŸ†• æå–æ–°å‘ç°çš„å®ä½“
        data = result.get('data', [])
        if not data:
            return

        first_row = data[0]

        # æå–regions
        if 'region' in first_row or 'acronym' in first_row:
            regions = list(set([
                row.get('region') or row.get('acronym')
                for row in data
                if row.get('region') or row.get('acronym')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Region', [])
            for r in regions:
                if r and r not in existing:
                    existing.append(r)

        # æå–clusters
        if 'cluster' in first_row or 'cluster_name' in first_row:
            clusters = list(set([
                row.get('cluster') or row.get('cluster_name')
                for row in data
                if row.get('cluster') or row.get('cluster_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Cluster', [])
            for c in clusters:
                if c and c not in existing:
                    existing.append(c)

        # æå–subclasses
        if 'subclass' in first_row or 'subclass_name' in first_row:
            subclasses = list(set([
                row.get('subclass') or row.get('subclass_name')
                for row in data
                if row.get('subclass') or row.get('subclass_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Subclass', [])
            for s in subclasses:
                if s and s not in existing:
                    existing.append(s)

        # ğŸ†• æå–projection targets
        if 'target' in first_row or 'target_region' in first_row:
            targets = list(set([
                row.get('target') or row.get('target_region')
                for row in data
                if row.get('target') or row.get('target_region')
            ]))

            existing = analysis_state.discovered_entities.setdefault('ProjectionTarget', [])
            for t in targets:
                if t and t not in existing:
                    existing.append(t)

            if targets:
                logger.info(f"   ğŸ“ Discovered {len(targets)} projection targets")

    def _classify_question_intent(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜æ„å›¾"""
        question_lower = question.lower()

        if any(w in question_lower for w in ['compare', 'difference', 'versus', 'vs']):
            return 'comparison'
        elif any(w in question_lower for w in ['comprehensive', 'detailed', 'everything']):
            return 'comprehensive'
        elif any(w in question_lower for w in ['why', 'explain', 'how']):
            return 'explanatory'
        else:
            return 'simple_query'

    def _convert_candidate_to_reasoning(self, candidate, step_number, analysis_state):
        """è½¬æ¢CandidateStep (ä¿®å¤ç‰ˆ)"""
        params = candidate.parameters.copy()

        # ğŸ”§ æ™ºèƒ½åˆ¤æ–­action
        has_cypher = bool(candidate.cypher_template and candidate.cypher_template.strip())

        if not has_cypher:
            # ç‰¹æ®Šæ­¥éª¤
            if 'statistical' in candidate.step_type.lower() or 'fdr' in candidate.step_id.lower():
                action = 'execute_statistical'
            elif 'multi-modal' in candidate.step_type.lower() or 'mismatch' in candidate.step_id.lower():
                action = 'execute_fingerprint'
            else:
                action = 'execute_cypher'
        else:
            action = 'execute_cypher'

        return ReasoningStep(
            step_number=step_number,
            purpose=candidate.purpose,
            action=action,  # ğŸ”§ æ­£ç¡®çš„action
            rationale=candidate.rationale,
            expected_result=candidate.expected_data,
            query_or_params={
                'query': candidate.cypher_template,
                'params': params
            },
            modality=candidate.step_type,
            depends_on=getattr(candidate, 'depends_on', [])
        )

    def _update_analysis_state(self,
                               analysis_state: 'AnalysisState',
                               step: ReasoningStep,
                               result: Dict,
                               candidate: 'CandidateStep'):
        """
        æ›´æ–°åˆ†æçŠ¶æ€
        """
        # è®°å½•æ‰§è¡Œçš„æ­¥éª¤
        analysis_state.executed_steps.append({
            'purpose': step.purpose,
            'modality': step.modality,
            'row_count': len(result.get('data', [])),
            'step_id': candidate.step_id
        })

        # æ›´æ–°modalityè¦†ç›–
        if step.modality and step.modality not in analysis_state.modalities_covered:
            analysis_state.modalities_covered.append(step.modality)

        # ğŸ†• æå–æ–°å‘ç°çš„å®ä½“
        data = result.get('data', [])
        if not data:
            return

        first_row = data[0]

        # æå–regions
        if 'region' in first_row or 'acronym' in first_row:
            regions = list(set([
                row.get('region') or row.get('acronym')
                for row in data
                if row.get('region') or row.get('acronym')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Region', [])
            for r in regions:
                if r not in existing:
                    existing.append(r)

        # æå–clusters
        if 'cluster' in first_row or 'cluster_name' in first_row:
            clusters = list(set([
                row.get('cluster') or row.get('cluster_name')
                for row in data
                if row.get('cluster') or row.get('cluster_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Cluster', [])
            for c in clusters:
                if c not in existing:
                    existing.append(c)

        # æå–subclasses
        if 'subclass' in first_row or 'subclass_name' in first_row:
            subclasses = list(set([
                row.get('subclass') or row.get('subclass_name')
                for row in data
                if row.get('subclass') or row.get('subclass_name')
            ]))

            existing = analysis_state.discovered_entities.setdefault('Subclass', [])
            for s in subclasses:
                if s not in existing:
                    existing.append(s)

        # ğŸ†• æå–projection targets
        if 'target' in first_row or 'target_region' in first_row:
            targets = list(set([
                row.get('target') or row.get('target_region')
                for row in data
                if row.get('target') or row.get('target_region')
            ]))

            existing = analysis_state.discovered_entities.setdefault('ProjectionTarget', [])
            for t in targets:
                if t not in existing:
                    existing.append(t)

            logger.info(f"   ğŸ“ Discovered {len(targets)} projection targets")

    def _determine_analysis_depth(self, question: str) -> AnalysisDepth:
        """æ ¹æ®é—®é¢˜ç¡®å®šåˆ†ææ·±åº¦"""

        question_lower = question.lower()

        # Deep: comprehensive, detailed, everything, full, complete
        if any(kw in question_lower for kw in ['comprehensive', 'detailed', 'everything', 'complete', 'full']):
            return AnalysisDepth.DEEP

        # Shallow: simple, basic, quick, overview
        if any(kw in question_lower for kw in ['simple', 'basic', 'quick', 'overview', 'briefly']):
            return AnalysisDepth.SHALLOW

        # Default: Medium
        return AnalysisDepth.MEDIUM


    def _enhanced_planning_phase(self, state: EnhancedAgentState) -> Dict[str, Any]:
        """
        å¢å¼ºçš„è§„åˆ’é˜¶æ®µ

        æ­¥éª¤:
        1. æ™ºèƒ½å®ä½“è¯†åˆ« (æ— hardcodedåˆ—è¡¨!)
        2. å®ä½“èšç±»
        3. åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
        4. LLMç²¾åŒ–
        """
        try:
            # Step 1: å®ä½“è¯†åˆ«
            logger.info("  [1/4] Intelligent entity recognition...")
            entity_matches = self.entity_recognizer.recognize_entities(state.question)
            state.entity_matches = entity_matches

            logger.info(f"     Found {len(entity_matches)} entity matches")
            for match in entity_matches[:5]:
                logger.info(f"       â€¢ {match.text} ({match.entity_type}) [{match.confidence:.2f}]")

            # Step 2: å®ä½“èšç±»
            logger.info("  [2/4] Entity clustering...")
            entity_clusters = self.entity_clusterer.cluster_entities(
                entity_matches,
                state.question
            )
            state.entity_clusters = entity_clusters

            logger.info(f"     Created {len(entity_clusters)} entity clusters")
            for cluster in entity_clusters:
                logger.info(f"       â€¢ {cluster.cluster_type}: {cluster.primary_entity.text}")

            # Step 3: åŠ¨æ€Schemaè·¯å¾„è§„åˆ’
            logger.info("  [3/4] Dynamic schema path planning...")
            query_plans = self.path_planner.generate_plan(entity_clusters, state.question)

            logger.info(f"     Generated {len(query_plans)} query plans")

            # è®°å½•ä½¿ç”¨çš„schemaè·¯å¾„
            for plan in query_plans:
                if plan.schema_path.hops:
                    state.schema_paths_used.append({
                        'start': plan.schema_path.start_label,
                        'end': plan.schema_path.end_label,
                        'hops': len(plan.schema_path.hops),
                        'score': plan.schema_path.score
                    })

            # Step 4: LLMç²¾åŒ–
            logger.info("  [4/4] LLM plan refinement...")
            refined_steps = self._llm_refine_plans(query_plans, state)
            state.reasoning_plan = refined_steps

            # ä¿å­˜å®ä½“åˆ°state (å…¼å®¹åŸæœ‰æ ¼å¼)
            state.entities = [
                {
                    'text': m.text,
                    'type': m.entity_type,
                    'confidence': m.confidence
                }
                for m in entity_matches[:10]
            ]

            return {'success': True}

        except Exception as e:
            logger.error(f"Enhanced planning failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _llm_refine_plans(self,
                          query_plans: List,
                          state: EnhancedAgentState) -> List[ReasoningStep]:
        """
        LLMç²¾åŒ–æŸ¥è¯¢è®¡åˆ’

        å°†åŠ¨æ€ç”Ÿæˆçš„QueryPlanè½¬æ¢ä¸ºReasoningStep,å¹¶è®©LLMè¡¥å……ç»†èŠ‚
        """
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        plans_dict = []
        for qp in query_plans:
            plans_dict.append({
                'step': qp.step_number,
                'purpose': qp.purpose,
                'action': qp.action,
                'query': qp.cypher_template,
                'parameters': qp.parameters,
                'modality': qp.modality,
                'depends_on': qp.depends_on,
                'schema_path_score': qp.schema_path.score if qp.schema_path else 0.0
            })

        prompt = f"""You are refining a reasoning plan for neuroscience knowledge graph analysis.

**Question:** {state.question}

**Recognized Entities:** {', '.join([e['text'] for e in state.entities])}

**Dynamically Generated Query Plans:**
{json.dumps(plans_dict, indent=2)}

Your task:
1. Review each query plan
2. Add detailed **expected_result** descriptions
3. Enhance **rationale** with domain knowledge
4. Verify Cypher query correctness
5. Add any missing steps if needed

Return a JSON object with key "steps" containing an array:
{{
  "steps": [
    {{
      "step_number": 1,
      "purpose": "...",
      "action": "execute_cypher",
      "rationale": "Detailed explanation",
      "expected_result": "Concrete prediction of what data will look like",
      "query_or_params": {{"query": "...", "params": {{}}}},
      "modality": "molecular/morphological/projection",
      "depends_on": []
    }},
    ...
  ]
}}

**Important:**
- Make rationale SPECIFIC and scientifically grounded
- Expected results should describe DATA PATTERNS (e.g., "10-20 clusters with neuron counts ranging 500-5000")
- Ensure query syntax is correct
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert neuroscientist and Neo4j query expert."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )

            result = json.loads(response.choices[0].message.content)

            # è½¬æ¢ä¸ºReasoningStep
            steps = []
            for step_dict in result.get('steps', []):
                query_or_params = step_dict.get('query_or_params', {})

                # å¤„ç†å‚æ•°æ›¿æ¢
                if isinstance(query_or_params, dict):
                    if 'query' not in query_or_params and 'query' in step_dict:
                        query_or_params = {'query': step_dict['query']}

                step = ReasoningStep(
                    step_number=step_dict.get('step_number', len(steps) + 1),
                    purpose=step_dict.get('purpose', ''),
                    action=step_dict.get('action', 'execute_cypher'),
                    rationale=step_dict.get('rationale', ''),
                    expected_result=step_dict.get('expected_result', ''),
                    query_or_params=query_or_params,
                    modality=step_dict.get('modality'),
                    depends_on=step_dict.get('depends_on', [])
                )
                steps.append(step)

            return steps

        except Exception as e:
            logger.error(f"LLM refinement failed: {e}")

            # Fallback: ç›´æ¥è½¬æ¢QueryPlan
            fallback_steps = []
            for qp in query_plans:
                step = ReasoningStep(
                    step_number=qp.step_number,
                    purpose=qp.purpose,
                    action=qp.action,
                    rationale="Automatically generated from schema path",
                    expected_result="Data matching query criteria",
                    query_or_params={'query': qp.cypher_template, 'params': qp.parameters},
                    modality=qp.modality,
                    depends_on=qp.depends_on
                )
                fallback_steps.append(step)

            return fallback_steps

    def _characterize_top_pairs(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        æ·±å…¥åˆ†ætop mismatch pairs (Case Study)

        ğŸ†• æ–°å¢åŠŸèƒ½:
        1. æå–top N pairs
        2. æŸ¥è¯¢æ¯ä¸ªpairçš„è¯¦ç»†æ•°æ®:
           - Morphological features
           - Projection targets
           - Molecular composition
        """
        n_top = params.get('n_top_pairs', 3)

        # ä»FDRç»“æœè·å–top pairs
        fdr_data = None
        for key, data in state.intermediate_data.items():
            if data and isinstance(data, list) and len(data) > 0:
                if 'fdr_significant' in data[0] and data[0].get('fdr_significant'):
                    fdr_data = data
                    break

        if not fdr_data:
            logger.warning("   No FDR significant pairs found, using top mismatch pairs")
            # Fallback: ä½¿ç”¨top mismatch
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    if 'mismatch_combined' in data[0]:
                        fdr_data = sorted(data, key=lambda x: x['mismatch_combined'], reverse=True)
                        break

        if not fdr_data:
            return {'success': False, 'error': 'No mismatch data found', 'data': []}

        # é€‰æ‹©top N pairs
        top_pairs = fdr_data[:n_top]

        logger.info(f"   Analyzing top {len(top_pairs)} pairs:")
        for pair in top_pairs:
            logger.info(f"     â€¢ {pair['region1']} vs {pair['region2']}: mismatch={pair['mismatch_combined']:.3f}")

        # è¯¦ç»†åˆ†ææ¯ä¸ªpair
        detailed_results = []

        for pair in top_pairs:
            region1 = pair['region1']
            region2 = pair['region2']

            logger.info(f"   Deep characterization: {region1} vs {region2}")

            # ğŸ”¹ 1. Morphological comparison
            morph_query = """
            MATCH (n:Neuron)-[:LOCATE_AT]->(r:Region)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS region,
                   count(n) AS neuron_count,
                   avg(n.axonal_length) AS avg_axon,
                   avg(n.dendritic_length) AS avg_dendrite,
                   avg(n.axonal_branches) AS avg_axon_branches,
                   avg(n.dendritic_branches) AS avg_dendrite_branches,
                   stdev(n.axonal_length) AS std_axon,
                   stdev(n.dendritic_length) AS std_dendrite
            """
            morph_result = self.db.run(morph_query, {'region1': region1, 'region2': region2})

            # ğŸ”¹ 2. Projection targets comparison
            proj_query = """
            MATCH (r:Region)-[p:PROJECT_TO]->(t:Region)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS source,
                   t.acronym AS target,
                   t.name AS target_name,
                   p.weight AS weight
            ORDER BY r.acronym, p.weight DESC
            LIMIT 30
            """
            proj_result = self.db.run(proj_query, {'region1': region1, 'region2': region2})

            # ğŸ”¹ 3. Molecular composition
            mol_query = """
            MATCH (r:Region)-[:HAS_CLUSTER]->(c:Cluster)
            WHERE r.acronym IN [$region1, $region2]
            RETURN r.acronym AS region,
                   c.name AS cluster,
                   c.markers AS markers,
                   c.number_of_neurons AS neurons
            ORDER BY r.acronym, c.number_of_neurons DESC
            LIMIT 20
            """
            mol_result = self.db.run(mol_query, {'region1': region1, 'region2': region2})

            # æ•´åˆç»“æœ
            detailed_results.append({
                'pair': f"{region1}_vs_{region2}",
                'region1': region1,
                'region2': region2,
                'mismatch_score': pair['mismatch_combined'],
                'p_value': pair.get('p_value', 1.0),
                'q_value': pair.get('q_value', 1.0),
                'morphology': morph_result.get('data', []),
                'projections': proj_result.get('data', []),
                'molecular': mol_result.get('data', [])
            })

        logger.info(f"   âœ… Detailed characterization complete for {len(detailed_results)} pairs")

        return {
            'success': True,
            'data': detailed_results,
            'rows': len(detailed_results),
            'analysis_type': 'case_study'
        }

    # ==================== Execution ====================

    def _execute_step(self, step: ReasoningStep, state: EnhancedAgentState) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæ­¥éª¤ (ä¿®å¤ç‰ˆ - æ”¯æŒcase study)"""
        start_time = time.time()

        try:
            query = step.query_or_params.get('query', '').strip()
            params = step.query_or_params.get('params', {})

            # åˆ¤æ–­æ‰§è¡Œç±»å‹
            if not query:
                # ğŸ†• Case studyæ£€æµ‹
                if 'characterize' in step.purpose.lower() and 'top' in step.purpose.lower():
                    result = self._characterize_top_pairs(params, state)
                elif 'mismatch' in step.purpose.lower():
                    result = self._execute_fingerprint_step(step, state)
                elif 'statistical' in step.purpose.lower() or 'fdr' in step.purpose.lower():
                    result = self._execute_statistical_step(step, state)
                else:
                    result = {'success': False, 'error': 'Cannot determine execution type'}
            else:
                result = self._execute_cypher_step(step, state)

            step.actual_result = result
            step.execution_time = time.time() - start_time

            step_key = f"step_{step.step_number}"
            state.intermediate_data[step_key] = result.get('data', [])

            return result

        except Exception as e:
            logger.error(f"Step execution failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _execute_cypher_step(self, step: ReasoningStep, state: EnhancedAgentState) -> Dict[str, Any]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢æ­¥éª¤"""
        query = step.query_or_params.get('query', '').strip()
        params = step.query_or_params.get('params', {})

        # ğŸ”§ ç©ºæŸ¥è¯¢æ£€æŸ¥
        if not query:
            logger.warning(f"   Empty Cypher query - skipping")
            return {'success': False, 'error': 'Empty query', 'data': []}

        # å‚æ•°æ›¿æ¢
        if step.depends_on:
            params = self._resolve_parameters(step, state, params)

        # è‡ªåŠ¨æ·»åŠ LIMIT
        import re
        if not re.search(r'\bLIMIT\b', query, re.IGNORECASE):
            query = f"{query}\nLIMIT 100"

        return self.db.run(query, params)

    def _execute_statistical_step(self,
                                  step: ReasoningStep,
                                  state: EnhancedAgentState) -> Dict[str, Any]:
        """
        ğŸ†• æ‰§è¡Œç»Ÿè®¡æ­¥éª¤
        """
        params = step.query_or_params.get('params', {})
        test_type = params.get('test_type', 'permutation')

        logger.info(f"   ğŸ“Š Statistical test: {test_type}")

        try:
            if test_type == 'permutation':
                return self._permutation_test(params, state)

            elif test_type == 'fdr':
                return self._fdr_correction(params, state)

            elif test_type == 'correlation':
                return self._correlation_test(params, state)

            else:
                return {'success': False, 'error': f'Unknown test type: {test_type}'}

        except Exception as e:
            logger.error(f"Statistical test failed: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _execute_fingerprint_step(self,
                                  step: ReasoningStep,
                                  state: EnhancedAgentState) -> Dict[str, Any]:
        """
        ğŸ†• æ‰§è¡Œfingerprintè®¡ç®—æ­¥éª¤
        """
        params = step.query_or_params.get('params', {})
        analysis_type = params.get('analysis_type', 'cross_modal_mismatch')

        logger.info(f"   ğŸ”¬ Fingerprint analysis: {analysis_type}")

        if analysis_type == 'cross_modal_mismatch':
            return self._compute_mismatch_matrix(params, state)
        else:
            return {'success': False, 'error': f'Unknown analysis type: {analysis_type}'}

    def _resolve_parameters(self,
                            step: ReasoningStep,
                            state: EnhancedAgentState,
                            params: Dict) -> Dict:
        """è§£ææ­¥éª¤ä¾èµ–çš„å‚æ•°"""
        resolved = params.copy()

        # æŸ¥æ‰¾ä¾èµ–æ­¥éª¤çš„æ•°æ®
        for dep_num in step.depends_on:
            dep_key = f"step_{dep_num}"
            if dep_key in state.intermediate_data:
                dep_data = state.intermediate_data[dep_key]

                # æå–å¸¸ç”¨å­—æ®µ
                if dep_data:
                    # æå–region acronyms
                    regions = []
                    for row in dep_data:
                        if 'region' in row:
                            regions.append(row['region'])
                        elif 'acronym' in row:
                            regions.append(row['acronym'])

                    if regions:
                        resolved['enriched_regions'] = regions[:10]
                        resolved['target_regions'] = regions[:10]

        return resolved

    def _execute_cypher(self, query: str, params: Dict) -> Dict[str, Any]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        import re

        # ç¡®ä¿æœ‰LIMIT
        if not re.search(r'\bLIMIT\b', query, re.IGNORECASE):
            query = f"{query}\nLIMIT 100"

        return self.db.run(query, params)

    def _permutation_test(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """Permutation test for morphological differences"""
        entity_a = params['entity_a']
        entity_b = params['entity_b']

        # ä»ä¹‹å‰çš„stepè·å–æ•°æ®
        morph_data = None
        for key, data in state.intermediate_data.items():
            if data and isinstance(data, list) and len(data) > 0:
                if 'region' in data[0] and ('avg_axon' in data[0] or 'avg_axon_length' in data[0]):
                    morph_data = data
                    break

        if not morph_data:
            return {'success': False, 'error': 'No morphological data found'}

        # æå–ä¸¤ç»„æ•°æ®
        group_a = [row for row in morph_data if row.get('region') == entity_a]
        group_b = [row for row in morph_data if row.get('region') == entity_b]

        if not group_a or not group_b:
            return {'success': False,
                    'error': f'Insufficient data: {entity_a}={len(group_a)}, {entity_b}={len(group_b)}'}

        # æå–axon length
        import numpy as np
        axon_key = 'avg_axon' if 'avg_axon' in group_a[0] else 'avg_axon_length'
        axon_a = np.array([row.get(axon_key, 0) or 0 for row in group_a])
        axon_b = np.array([row.get(axon_key, 0) or 0 for row in group_b])

        # ç§»é™¤é›¶å€¼
        axon_a = axon_a[axon_a > 0]
        axon_b = axon_b[axon_b > 0]

        if len(axon_a) == 0 or len(axon_b) == 0:
            return {'success': False, 'error': 'No valid morphology data'}

        # è®¡ç®—observed difference
        observed_diff = float(np.mean(axon_a) - np.mean(axon_b))

        # ğŸ¯ è°ƒç”¨ç»Ÿè®¡å·¥å…·!
        result = self.stats.permutation_test(
            observed_stat=observed_diff,
            data1=axon_a,
            data2=axon_b,
            n_permutations=1000,
            seed=42
        )

        # è®¡ç®—effect size
        effect_size = self.stats.cohens_d(axon_a, axon_b)

        # æ ¼å¼åŒ–ç»“æœ
        result_data = [{
            'comparison': f'{entity_a} vs {entity_b}',
            'feature': 'axonal_length',
            'mean_a': float(np.mean(axon_a)),
            'mean_b': float(np.mean(axon_b)),
            'observed_difference': observed_diff,
            'p_value': result['p_value'],
            'effect_size_cohens_d': effect_size,
            'significance': 'significant' if result['p_value'] < 0.05 else 'not significant',
            'interpretation': self._interpret_statistical_result(result, effect_size)
        }]

        logger.info(f"   âœ… Permutation test: p={result['p_value']:.4f}, d={effect_size:.2f}")

        return {
            'success': True,
            'data': result_data,
            'rows': len(result_data),
            'test_type': 'permutation'
        }

    def _fdr_correction(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        FDR correction (è¶…å¼ºè°ƒè¯•ç‰ˆ)

        ğŸ”§ å…¨é¢è°ƒè¯•å’Œå®¹é”™
        """
        alpha = params.get('alpha', 0.05)

        logger.info(f"   === FDR Correction Debug ===")
        logger.info(f"   Available data keys: {list(state.intermediate_data.keys())}")

        # ğŸ”§ å¢å¼ºæ•°æ®æŸ¥æ‰¾
        mismatch_data = None
        mismatch_key = None

        # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«'mismatch_combined'å’Œ'p_value'çš„æ•°æ®
        for key, data in state.intermediate_data.items():
            logger.debug(f"   Checking {key}: type={type(data)}, len={len(data) if isinstance(data, list) else 'N/A'}")

            if not data:
                continue

            if isinstance(data, list) and len(data) > 0:
                first_row = data[0]
                logger.debug(
                    f"     First row keys: {first_row.keys() if isinstance(first_row, dict) else 'Not a dict'}")

                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                has_mismatch = 'mismatch_combined' in first_row if isinstance(first_row, dict) else False
                has_pvalue = 'p_value' in first_row if isinstance(first_row, dict) else False

                logger.debug(f"     has_mismatch={has_mismatch}, has_pvalue={has_pvalue}")

                if has_mismatch and has_pvalue:
                    mismatch_data = data
                    mismatch_key = key
                    logger.info(f"   âœ“ Found mismatch data in {key} ({len(data)} rows)")
                    break

        # ç­–ç•¥2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æœ€è¿‘çš„stepè·å–
        if not mismatch_data:
            logger.warning("   Strategy 1 failed, trying strategy 2...")

            # æŒ‰keyæ’åºï¼Œæ‰¾æœ€è¿‘çš„step
            sorted_keys = sorted([k for k in state.intermediate_data.keys() if k.startswith('step_')],
                                 key=lambda x: int(x.split('_')[1]) if len(x.split('_')) > 1 and x.split('_')[
                                     1].isdigit() else 0,
                                 reverse=True)

            logger.debug(f"   Sorted keys: {sorted_keys}")

            for key in sorted_keys:
                data = state.intermediate_data[key]
                if data and isinstance(data, list) and len(data) > 0:
                    first_row = data[0]
                    if isinstance(first_row, dict) and 'mismatch_combined' in first_row:
                        logger.info(f"   âœ“ Found mismatch data in {key} (strategy 2)")
                        mismatch_data = data
                        mismatch_key = key

                        # ğŸ”§ å¦‚æœæ²¡æœ‰p_valueï¼Œæ·»åŠ é»˜è®¤å€¼
                        if 'p_value' not in first_row:
                            logger.warning(f"   Adding default p_values")
                            for row in mismatch_data:
                                if 'p_value' not in row:
                                    row['p_value'] = 1.0 - min(0.99, row.get('mismatch_combined', 0))

                        break

        # æœ€ç»ˆæ£€æŸ¥
        if not mismatch_data:
            logger.error("   âœ— No mismatch data found!")
            logger.error(f"   Available keys: {list(state.intermediate_data.keys())}")

            # æ‰“å°æ‰€æœ‰æ•°æ®çš„æ ·æœ¬
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    logger.error(
                        f"   {key} sample: {list(data[0].keys()) if isinstance(data[0], dict) else type(data[0])}")

            return {
                'success': False,
                'error': 'No mismatch data with p-values found',
                'data': []
            }

        # æå–p-values
        p_values = []
        for row in mismatch_data:
            pval = row.get('p_value', None)
            if pval is not None:
                p_values.append(float(pval))
            else:
                logger.warning(
                    f"   Row missing p_value: {row.get('region1', 'unknown')}-{row.get('region2', 'unknown')}")
                p_values.append(1.0)

        logger.info(f"   FDR input: {len(p_values)} p-values")
        logger.info(f"   P-value range: [{min(p_values):.4f}, {max(p_values):.4f}]")
        logger.info(f"   P-values < 0.05: {sum(1 for p in p_values if p < 0.05)}")

        # ğŸ¯ æ‰§è¡ŒFDR correction
        try:
            q_values, significant = self.stats.fdr_correction(p_values, alpha)

            # æ•´åˆç»“æœ
            result_data = []
            for i, row in enumerate(mismatch_data):
                result_data.append({
                    **row,
                    'q_value': float(q_values[i]),
                    'fdr_significant': bool(significant[i])
                })

            # ç­›é€‰æ˜¾è‘—çš„
            significant_data = [r for r in result_data if r['fdr_significant']]

            logger.info(f"   âœ… FDR: {len(significant_data)}/{len(result_data)} significant (Î±={alpha})")

            if significant_data:
                top = significant_data[0]
                logger.info(f"   Top: {top['region1']}-{top['region2']}")
                logger.info(f"     Mismatch: {top['mismatch_combined']:.3f}")
                logger.info(f"     Q-value: {top['q_value']:.4f}")
            else:
                logger.warning(f"   No significant pairs after FDR correction")
                logger.warning(f"   Smallest q-value: {min(q_values):.4f}")
                logger.warning(f"   Consider: alpha={alpha} may be too stringent")

            return {
                'success': True,
                'data': significant_data,
                'rows': len(significant_data),
                'test_type': 'fdr',
                'alpha': alpha,
                'n_significant': len(significant_data),
                'n_total': len(result_data),
                'min_q_value': float(min(q_values)),
                'max_q_value': float(max(q_values))
            }

        except Exception as e:
            logger.error(f"   FDR correction failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'data': []
            }

    def _correlation_test(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """Correlation test between modalities"""
        # å®ç°correlation (å¯é€‰,æš‚æ—¶è¿”å›placeholder)
        logger.warning("Correlation test not yet implemented")
        return {'success': False, 'error': 'Not implemented'}

    def _compute_mismatch_matrix(self, params: Dict, state: EnhancedAgentState) -> Dict:
        """
        è®¡ç®—cross-modal mismatchçŸ©é˜µ (å¯¹é½Figure 4æ–¹æ³•)

        ğŸ¯ å…³é”®ä¿®å¤:
        1. å…ˆè®¡ç®—æ‰€æœ‰pairsçš„è·ç¦»çŸ©é˜µ
        2. å…¨å±€Min-Maxå½’ä¸€åŒ–
        3. ç„¶åè®¡ç®—mismatch
        """
        import time
        start_time = time.time()

        # è·å–regions
        regions = state.analysis_state.discovered_entities.get('Region', [])

        if not regions:
            for key, data in state.intermediate_data.items():
                if data and isinstance(data, list) and len(data) > 0:
                    if 'region' in data[0]:
                        regions = list(set([row['region'] for row in data if row.get('region')]))
                        break

        max_regions = params.get('max_regions', 15)
        regions = regions[:max_regions]

        if len(regions) < 2:
            return {'success': False, 'error': 'Need at least 2 regions'}

        n = len(regions)
        logger.info(f"   ğŸš€ Computing mismatch (Figure 4 method) for {n} regions...")

        # ğŸš€ Step 1: æ‰¹é‡è·å–fingerprints
        logger.info(f"   ğŸ“Š Step 1/4: Batch fetching fingerprints...")

        fingerprints = {}
        failed_regions = []

        for region in regions:
            try:
                mol = self.fingerprint.compute_molecular_fingerprint(region)
                morph = self.fingerprint.compute_morphological_fingerprint(region)
                proj = self.fingerprint.compute_projection_fingerprint(region)

                if mol is not None and morph is not None and proj is not None:
                    fingerprints[region] = {
                        'molecular': mol,
                        'morphological': morph,
                        'projection': proj
                    }
                else:
                    failed_regions.append(region)

            except Exception as e:
                logger.warning(f"      Failed {region}: {e}")
                failed_regions.append(region)

        valid_regions = [r for r in regions if r not in failed_regions]
        n_valid = len(valid_regions)

        logger.info(f"      âœ“ Got fingerprints: {len(fingerprints)}/{n}")

        if n_valid < 2:
            return {'success': False, 'error': 'Insufficient valid regions'}

        # ğŸš€ Step 2: æ„å»ºè·ç¦»çŸ©é˜µ (NxN)
        logger.info(f"   ğŸ“ Step 2/4: Building distance matrices...")

        import numpy as np
        from scipy.spatial.distance import cosine, euclidean

        mol_dist_matrix = np.zeros((n_valid, n_valid))
        morph_dist_matrix = np.zeros((n_valid, n_valid))
        proj_dist_matrix = np.zeros((n_valid, n_valid))

        for i, region_a in enumerate(valid_regions):
            for j, region_b in enumerate(valid_regions):
                if i == j:
                    continue

                fp_a = fingerprints[region_a]
                fp_b = fingerprints[region_b]

                # åˆ†å­è·ç¦»: 1 - cosine_similarity
                try:
                    mol_dist_matrix[i, j] = cosine(fp_a['molecular'], fp_b['molecular'])
                except:
                    mol_dist_matrix[i, j] = np.nan

                # å½¢æ€è·ç¦»: Euclidean
                try:
                    morph_a = fp_a['morphological']
                    morph_b = fp_b['morphological']

                    # å¤„ç†NaN
                    valid_mask = ~(np.isnan(morph_a) | np.isnan(morph_b))
                    if valid_mask.sum() > 0:
                        morph_dist_matrix[i, j] = euclidean(
                            morph_a[valid_mask],
                            morph_b[valid_mask]
                        )
                    else:
                        morph_dist_matrix[i, j] = np.nan
                except:
                    morph_dist_matrix[i, j] = np.nan

                # æŠ•å°„è·ç¦»: 1 - cosine_similarity
                try:
                    proj_dist_matrix[i, j] = cosine(fp_a['projection'], fp_b['projection'])
                except:
                    proj_dist_matrix[i, j] = np.nan

        print(f"      âœ“ Distance matrices built")
        # åœ¨ "âœ“ Distance matrices built" åé¢æ·»åŠ 
        print(
            f"      Molecular distance range: [{np.nanmin(mol_dist_matrix):.3f}, {np.nanmax(mol_dist_matrix):.3f}]")
        print(
            f"      Morphology distance range: [{np.nanmin(morph_dist_matrix):.3f}, {np.nanmax(morph_dist_matrix):.3f}]")
        print(
            f"      Projection distance range: [{np.nanmin(proj_dist_matrix):.3f}, {np.nanmax(proj_dist_matrix):.3f}]")

        # ç»Ÿè®¡NaNæ•°é‡
        n_total = mol_dist_matrix.size
        n_mol_nan = np.isnan(mol_dist_matrix).sum()
        n_morph_nan = np.isnan(morph_dist_matrix).sum()
        n_proj_nan = np.isnan(proj_dist_matrix).sum()

        print(
            f"      NaN counts: mol={n_mol_nan}/{n_total}, morph={n_morph_nan}/{n_total}, proj={n_proj_nan}/{n_total}")

        # ğŸš€ Step 3: Min-Maxå½’ä¸€åŒ– (å…¨å±€)
        print(f"   ğŸ”§ Step 3/4: Normalizing distance matrices...")

        def minmax_normalize(matrix):
            """Min-Maxå½’ä¸€åŒ–åˆ°[0,1]"""
            valid = ~np.isnan(matrix)
            if valid.sum() == 0:
                return matrix

            vmin = matrix[valid].min()
            vmax = matrix[valid].max()

            if vmax - vmin < 1e-9:
                return np.zeros_like(matrix)

            normalized = (matrix - vmin) / (vmax - vmin)
            return normalized

        mol_norm = minmax_normalize(mol_dist_matrix)
        morph_norm = minmax_normalize(morph_dist_matrix)
        proj_norm = minmax_normalize(proj_dist_matrix)

        print(f"      âœ“ Normalization complete")
        print(f"      Normalized molecular range: [{np.nanmin(mol_norm):.3f}, {np.nanmax(mol_norm):.3f}]")
        print(f"      Normalized morphology range: [{np.nanmin(morph_norm):.3f}, {np.nanmax(morph_norm):.3f}]")
        print(f"      Normalized projection range: [{np.nanmin(proj_norm):.3f}, {np.nanmax(proj_norm):.3f}]")

        # ğŸš€ Step 4: è®¡ç®—Mismatch (å½’ä¸€åŒ–è·ç¦»çš„å·®å¼‚)
        print(f"   ğŸ§® Step 4/4: Computing mismatches...")

        mismatch_results = []

        from itertools import combinations

        for i, region1 in enumerate(valid_regions):
            for j, region2 in enumerate(valid_regions):
                if i >= j:  # åªè®¡ç®—ä¸Šä¸‰è§’
                    continue

                # Mismatch = |normalized_distance_A - normalized_distance_B|
                mismatch_GM = abs(mol_norm[i, j] - morph_norm[i, j])
                mismatch_GP = abs(mol_norm[i, j] - proj_norm[i, j])
                mismatch_MP = abs(morph_norm[i, j] - proj_norm[i, j])

                mismatch_combined = (mismatch_GM + mismatch_GP + mismatch_MP) / 3

                # ç›¸ä¼¼åº¦ (ç”¨äºæŠ¥å‘Š)
                sim_molecular = 1 - mol_dist_matrix[i, j]
                sim_morphological = 1 - morph_norm[i, j]  # å½’ä¸€åŒ–åçš„
                sim_projection = 1 - proj_dist_matrix[i, j]

                mismatch_results.append({
                    'region1': region1,
                    'region2': region2,
                    'mismatch_GM': float(mismatch_GM),
                    'mismatch_GP': float(mismatch_GP),
                    'mismatch_MP': float(mismatch_MP),
                    'mismatch_combined': float(mismatch_combined),
                    'sim_molecular': float(sim_molecular),
                    'sim_morphological': float(sim_morphological),
                    'sim_projection': float(sim_projection),
                    # è·ç¦»å€¼ (è°ƒè¯•ç”¨)
                    'dist_molecular': float(mol_dist_matrix[i, j]),
                    'dist_morphological': float(morph_dist_matrix[i, j]),
                    'dist_projection': float(proj_dist_matrix[i, j])
                })

        # ç»Ÿè®¡æ£€éªŒ
        all_mismatches = [r['mismatch_combined'] for r in mismatch_results]
        mean_m = np.mean(all_mismatches)
        std_m = np.std(all_mismatches)

        for result in mismatch_results:
            m = result['mismatch_combined']

            if std_m > 0:
                z_score = (m - mean_m) / std_m
                from scipy import stats
                p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
            else:
                z_score = 0
                p_value = 1.0

            result['p_value'] = float(p_value)
            result['z_score'] = float(z_score)
            result['effect_size'] = float(m)
            result['n_permutations'] = 0

        mismatch_results.sort(key=lambda x: x['mismatch_combined'], reverse=True)

        elapsed = time.time() - start_time

        print(f"   âœ… Completed in {elapsed:.1f}s")
        print(f"      Total pairs: {len(mismatch_results)}")

        if mismatch_results:
            top = mismatch_results[0]
            print(f"      Top: {top['region1']}-{top['region2']}")
            print(f"        Mismatch: {top['mismatch_combined']:.3f}")
            print(f"        P-value: {top['p_value']:.4f}")

            # ğŸ” æ˜¾ç¤ºtop 5ç”¨äºéªŒè¯
            print(f"      Top 5 pairs:")
            for i, pair in enumerate(mismatch_results[:5], 1):
                print(f"        {i}. {pair['region1']}-{pair['region2']}: {pair['mismatch_combined']:.3f}")

        return {
            'success': True,
            'data': mismatch_results,
            'rows': len(mismatch_results),
            'analysis_type': 'cross_modal_mismatch',
            'computation_time': elapsed,
            'method': 'figure4_compatible'
        }

    def _compute_cosine_similarity(self, vec1, vec2):
        """
        å¿«é€Ÿè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

        ğŸš€ ä¼˜åŒ–: ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œ
        """
        import numpy as np

        if not vec1 or not vec2:
            return 0.0

        # è½¬æ¢ä¸ºNumPyæ•°ç»„
        v1 = np.array(vec1, dtype=float)
        v2 = np.array(vec2, dtype=float)

        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        if len(v1) != len(v2):
            # Padæˆ–truncate
            max_len = max(len(v1), len(v2))
            if len(v1) < max_len:
                v1 = np.pad(v1, (0, max_len - len(v1)))
            if len(v2) < max_len:
                v2 = np.pad(v2, (0, max_len - len(v2)))

        # ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(v1, v2)
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return float(dot_product / (norm1 * norm2))

    def _interpret_statistical_result(self, test_result: Dict, effect_size: float) -> str:
        """è§£é‡Šç»Ÿè®¡ç»“æœ"""
        p_value = test_result['p_value']

        if p_value < 0.001:
            sig_level = "highly significant (p < 0.001)"
        elif p_value < 0.01:
            sig_level = "very significant (p < 0.01)"
        elif p_value < 0.05:
            sig_level = "significant (p < 0.05)"
        else:
            sig_level = "not significant (p â‰¥ 0.05)"

        if abs(effect_size) > 0.8:
            effect_desc = "large effect size"
        elif abs(effect_size) > 0.5:
            effect_desc = "medium effect size"
        elif abs(effect_size) > 0.2:
            effect_desc = "small effect size"
        else:
            effect_desc = "negligible effect size"

        return f"The difference is {sig_level} with a {effect_desc} (Cohen's d = {effect_size:.2f})"

    def _resolve_parameters(self,
                            step: ReasoningStep,
                            state: EnhancedAgentState,
                            params: Dict) -> Dict:
        """è§£ææ­¥éª¤ä¾èµ–çš„å‚æ•°"""
        resolved = params.copy()

        # æŸ¥æ‰¾ä¾èµ–æ­¥éª¤çš„æ•°æ®
        for dep_num in step.depends_on:
            dep_key = f"step_{dep_num}"
            if dep_key in state.intermediate_data:
                dep_data = state.intermediate_data[dep_key]

                if not dep_data:
                    continue

                # æå–å¸¸ç”¨å­—æ®µ
                # æå–region acronyms
                regions = []
                for row in dep_data:
                    if 'region' in row:
                        regions.append(row['region'])
                    elif 'acronym' in row:
                        regions.append(row['acronym'])

                if regions:
                    resolved['enriched_regions'] = list(set(regions))[:10]
                    resolved['target_regions'] = list(set(regions))[:10]

                # æå–targets
                targets = []
                for row in dep_data:
                    if 'target' in row:
                        targets.append(row['target'])
                    elif 'target_region' in row:
                        targets.append(row['target_region'])

                if targets:
                    resolved['targets'] = list(set(targets))[:10]

        return resolved

    # ==================== Intelligent Replanning ====================

    def _intelligent_replan(self, state: EnhancedAgentState, from_step: int) -> bool:
        """
        æ™ºèƒ½é‡è§„åˆ’

        ä½¿ç”¨:
        - ç»“æ„åŒ–åæ€çš„å»ºè®®
        - æ›¿ä»£å‡è®¾
        - Schemaä¸­çš„æ›¿ä»£è·¯å¾„
        """
        logger.info(f"ğŸ”„ Intelligent replanning from step {from_step}")
        state.replanning_count += 1

        # è·å–æœ€è¿‘çš„ç»“æ„åŒ–åæ€
        if state.structured_reflections:
            last_reflection = state.structured_reflections[-1]

            # ä½¿ç”¨åæ€ä¸­çš„å»ºè®®
            logger.info(f"   Using reflection recommendations:")
            for rec in last_reflection.next_step_recommendations:
                logger.info(f"     â€¢ {rec}")

            # å¦‚æœæœ‰æ›¿ä»£å‡è®¾,å°è¯•ä½¿ç”¨
            if last_reflection.alternative_hypotheses:
                logger.info(f"   Found {len(last_reflection.alternative_hypotheses)} alternative hypotheses")

        # é‡æ–°ç”Ÿæˆè®¡åˆ’ (ä½¿ç”¨ç°æœ‰å®ä½“)
        try:
            query_plans = self.path_planner.generate_plan(
                state.entity_clusters,
                state.question
            )

            # æ›¿æ¢å‰©ä½™æ­¥éª¤
            new_steps = self._llm_refine_plans(query_plans, state)

            # æ›´æ–°plan,ä¿ç•™å·²æ‰§è¡Œçš„
            state.reasoning_plan = state.reasoning_plan[:from_step - 1] + new_steps

            logger.info(f"   âœ… Replanned with {len(new_steps)} new steps")
            return True

        except Exception as e:
            logger.error(f"   âŒ Replanning failed: {e}")
            return False

    # ==================== Answer Synthesis ====================

    def _synthesize_answer(self, state: EnhancedAgentState) -> str:
        """
        åˆæˆæœ€ç»ˆç­”æ¡ˆ (å¢å¼ºç‰ˆ - ç§‘å­¦å™äº‹)
        """
        # å‡†å¤‡è¯æ®æ‘˜è¦
        evidence = []
        for step in state.executed_steps:
            if step.actual_result and step.actual_result.get('success'):
                data_count = len(step.actual_result.get('data', []))
                evidence.append(f"- Step {step.step_number}: {step.purpose} ({data_count} results)")

        evidence_text = "\n".join(evidence)

        # å‡†å¤‡å…³é”®å‘ç°
        key_data = {}
        for step in state.executed_steps:
            if step.actual_result and step.actual_result.get('success'):
                data = step.actual_result.get('data', [])
                if data:
                    key_data[f"step_{step.step_number}"] = data[:5]  # Top 5

        # å‡†å¤‡ç»“æ„åŒ–åæ€æ‘˜è¦
        reflection_summary = []
        for r in state.structured_reflections:
            reflection_summary.append(
                f"Step {r.step_number}: {r.validation_status.value} (confidence: {r.confidence_score:.2f})"
            )

        # ğŸ†• æ£€æµ‹åˆ†æç±»å‹
        analysis_type = self._detect_analysis_type(state)

        # ğŸ†• å‡†å¤‡PRIMARY FOCUSä¿¡æ¯
        primary_focus_info = ""
        if hasattr(state.analysis_state, 'primary_focus') and state.analysis_state.primary_focus:
            focus = state.analysis_state.primary_focus
            supporting = focus.supporting_data
            primary_focus_info = f"""
    **PRIMARY FOCUS IDENTIFIED:**
    - Region: {focus.entity_id}
    - Enrichment: {supporting.get('total_neurons', 'N/A')} neurons across {supporting.get('cluster_count', 'N/A')} clusters
    - This region shows the highest enrichment and was selected for deep characterization
    """

        prompt = f"""Synthesize a comprehensive, publication-quality answer based on the multi-step analysis.

    **CRITICAL: Write as a SCIENTIFIC NARRATIVE, not a data report!**

    **Original Question:** {state.question}

    **Analysis Type Detected:** {analysis_type}

    **Entities Recognized:** {', '.join([e['text'] for e in state.entities[:5]])}

    {primary_focus_info}

    **Reasoning Steps Executed:**
    {chr(10).join([f"{i + 1}. {s.purpose}" for i, s in enumerate(state.executed_steps)])}

    **Evidence Collected:**
    {evidence_text}

    **Key Findings (quantitative data):**
    {json.dumps(key_data, indent=2, default=str)[:3000]}

    **Structured Reflections:**
    {chr(10).join(reflection_summary)}

    **Your Task:**

    Write a comprehensive answer with the following structure:

    ### [Title - Generate an engaging title]

    #### Introduction (1 paragraph)
    - Open with the biological significance
    - State the main finding concisely

    #### Multi-Modal Analysis Results

    **1. Molecular Characterization**
    - Cite SPECIFIC numbers (e.g., "18,474 neurons across 4 clusters")
    - Mention key markers and cell types
    - Use quantitative language

    **2. Spatial Distribution**
    - List regions with enrichment metrics
    - Highlight PRIMARY focus if identified
    - Use percentages and rankings

    **3. Morphological Features** (if available)
    - Report mean Â± SD for axonal/dendritic measurements
    - Compare to baseline if applicable
    - Interpret structural specializations

    **4. Connectivity Patterns** (if available)
    - Describe projection targets with weights
    - Categorize by functional systems (sensory/motor/associative)
    - Mention top 3-5 targets quantitatively

    **5. Target Characterization (CLOSED LOOP)** (if available)
    - Describe cell type composition of projection targets
    - Connect back to molecular findings
    - Emphasize circuit-level integration

    **6. Statistical Validation** (if available)
    - Report p-values and effect sizes
    - Mention significance levels
    - Interpret biological meaning

    #### Integration and Implications
    - Connect molecular â†’ morphological â†’ projection findings
    - Propose functional hypotheses
    - Discuss circuit-level organization

    #### Limitations and Uncertainties
    - Acknowledge data gaps honestly
    - Cite confidence scores from reflections
    - Suggest validation approaches

    **Writing Style:**
    - Use ACTIVE voice ("Our analysis revealed..." not "It was found...")
    - Connect findings CAUSALLY ("Because X, we examined Y, which revealed Z")
    - Emphasize QUANTITATIVE data (numbers, percentages, statistics)
    - Make it VISUAL-READY (structure data for plotting)
    - Be HONEST about uncertainties

    **Avoid:**
    - Lists without narrative flow
    - Vague statements ("some regions", "several")
    - Overconfident claims
    - Jargon without explanation

    Generate a publication-quality narrative now.
    """

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system",
                     "content": "You are a neuroscience writer synthesizing research analysis results into publication-quality narratives."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )

            answer = response.choices[0].message.content.strip()
            state.final_answer = answer

            # ä¼°ç®—ç½®ä¿¡åº¦
            state.confidence_score = self._estimate_confidence(state)

            return answer

        except Exception as e:
            logger.error(f"Synthesis failed: {e}")
            import traceback
            traceback.print_exc()

            # Fallback: ç®€å•æ€»ç»“
            return f"Analysis completed with {len(state.executed_steps)} steps across {len(state.analysis_state.modalities_covered)} modalities. " \
                   f"Identified {len(state.entities)} entities and executed comprehensive multi-modal analysis. " \
                   f"Confidence: {self._estimate_confidence(state):.2f}."

    def _detect_analysis_type(self, state: EnhancedAgentState) -> str:
        """æ£€æµ‹åˆ†æç±»å‹"""
        step_purposes = [s.purpose.lower() for s in state.executed_steps]

        if any('compare' in p or 'versus' in p for p in step_purposes):
            return "Comparative Analysis"
        elif any('mismatch' in p or 'screening' in p for p in step_purposes):
            return "Systematic Screening (Figure 4 type)"
        elif any('primary focus' in p or 'closed loop' in p for p in step_purposes):
            return "Focus-Driven Deep Analysis (Figure 3 type)"
        else:
            return "General Multi-Modal Analysis"

    # ==================== Utilities ====================

    def _step_to_dict(self, step: ReasoningStep) -> Dict:
        """è½¬æ¢æ­¥éª¤ä¸ºå­—å…¸"""
        return {
            'step_number': step.step_number,
            'purpose': step.purpose,
            'action': step.action,
            'rationale': step.rationale,
            'expected_result': step.expected_result,
            'actual_result_summary': {
                'success': step.actual_result.get('success') if step.actual_result else False,
                'row_count': len(step.actual_result.get('data', [])) if step.actual_result else 0
            },
            'reflection': step.reflection,
            'validation_passed': step.validation_passed,
            'execution_time': step.execution_time,
            'modality': step.modality
        }

    def _estimate_confidence(self, state: EnhancedAgentState) -> float:
        """ä¼°ç®—ç½®ä¿¡åº¦"""
        if not state.structured_reflections:
            return 0.5

        # ä½¿ç”¨ç»“æ„åŒ–åæ€çš„ç½®ä¿¡åº¦
        confidences = [r.confidence_score for r in state.structured_reflections]
        avg_confidence = sum(confidences) / len(confidences)

        # è°ƒæ•´å› ç´ 

        # Factor 1: æ­¥éª¤å®Œæˆç‡
        completion_rate = len(state.executed_steps) / len(state.reasoning_plan) \
            if state.reasoning_plan else 0

        # Factor 2: é‡è§„åˆ’æƒ©ç½š
        replan_penalty = 0.95 ** state.replanning_count

        # ç»¼åˆ
        final_confidence = avg_confidence * (0.7 + 0.3 * completion_rate) * replan_penalty

        return min(1.0, max(0.0, final_confidence))

    def _build_error_response(self, question: str, error: str, start_time: float) -> Dict:
        """æ„å»ºé”™è¯¯å“åº”"""
        return {
            'question': question,
            'answer': f"Analysis failed: {error}",
            'error': error,
            'execution_time': time.time() - start_time,
            'success': False,
            'entities_recognized': [],
            'reasoning_plan': [],
            'executed_steps': [],
            'reflections': [],
            'confidence_score': 0.0
        }

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.db.close()


# ==================== Test ====================

def test_v10_agent():
    """æµ‹è¯•V10 agent"""
    import os

    print("\n" + "=" * 80)
    print("AIPOM-CoT V10 PRODUCTION TEST")
    print("=" * 80)

    agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j"),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o"
    )

    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "Tell me about Car3+ neurons",
        "Compare Pvalb and Sst interneurons in MOs",
        "What are the projection targets of the claustrum?"
    ]

    for q in test_questions:
        print(f"\n{'=' * 80}")
        print(f"Q: {q}")
        print('=' * 80)

        result = agent.answer(q, max_iterations=8)

        print(f"\nâœ… Results:")
        print(f"   Entities: {len(result['entities_recognized'])}")
        print(f"   Steps: {result['total_steps']}")
        print(f"   Confidence: {result['confidence_score']:.3f}")
        print(f"   Time: {result['execution_time']:.2f}s")
        print(f"\nğŸ’¡ Answer:\n{result['answer'][:300]}...\n")

    agent.close()


def test_car3_comprehensive():
    """æµ‹è¯•Car3çš„å®Œæ•´åˆ†æ"""

    agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j"),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o"
    )

    # ğŸ¯ å…³é”®: ä½¿ç”¨"comprehensive"è§¦å‘æ·±åº¦åˆ†æ
    # question = "Give me a comprehensive analysis of Car3+ neurons"
    question = "Which brain region pairs show the highest cross-modal mismatch in the top 30 brain regions with most neurons?"
    result = agent.answer(question, max_iterations=12)

    print("\n" + "=" * 80)
    print("FIGURE 3 STORY ARC ANALYSIS")
    print("=" * 80)

    print(f"\nTarget Depth: {result['adaptive_planning']['target_depth']}")
    print(f"Steps Executed: {result['adaptive_planning']['final_depth']}")
    print(f"Modalities: {', '.join(result['adaptive_planning']['modalities_covered'])}")

    print("\n" + "-" * 80)
    print("STEP-BY-STEP NARRATIVE:")
    print("-" * 80)

    for i, step in enumerate(result['executed_steps'], 1):
        print(f"\n{i}. {step['purpose']}")
        print(f"   Modality: {step['modality']}")
        print(f"   Data: {step['actual_result_summary']['row_count']} rows")
        print(f"   Confidence: {step['reflection']}")

    print("\n" + "-" * 80)
    print("ENTITIES DISCOVERED:")
    print("-" * 80)
    for entity_type, count in result['adaptive_planning']['entities_discovered'].items():
        print(f"  â€¢ {entity_type}: {count}")

    print("\n" + "-" * 80)
    print("VALIDATION CHECKLIST:")
    print("-" * 80)

    modalities = result['adaptive_planning']['modalities_covered']
    entities = result['adaptive_planning']['entities_discovered']

    checks = {
        'Has molecular analysis': 'molecular' in modalities,
        'Has morphological analysis': 'morphological' in modalities,
        'Has projection analysis': 'projection' in modalities,
        'Found regions': 'Region' in entities and entities['Region'] > 0,
        'Found projection targets': 'ProjectionTarget' in entities and entities['ProjectionTarget'] > 0,
        'Analyzed target composition': any(
            'target' in s['purpose'].lower() and 'composition' in s['purpose'].lower() for s in
            result['executed_steps'])
    }

    for check, passed in checks.items():
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {check}")

    # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
    completeness = sum(checks.values()) / len(checks) * 100
    print(f"\nğŸ“Š Story Completeness: {completeness:.0f}%")

    if completeness >= 80:
        print("\nğŸ‰ âœ… FIGURE 3 COMPLETE STORY ARC ACHIEVED!")
    else:
        print(f"\nâš ï¸  Story incomplete - missing {100 - completeness:.0f}% of elements")

    print("\n" + "=" * 80)
    print("FINAL ANSWER:")
    print("=" * 80)
    print(result['answer'])

    agent.close()

    return result

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    test_car3_comprehensive()