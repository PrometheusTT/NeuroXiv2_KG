"""
Nature Methodsçº§åˆ« Benchmark System for AIPOM-CoT
=================================================
å®Œæ•´çš„è¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºè¯æ˜ç³»ç»Ÿåœ¨NMä¸Šå‘è¡¨çš„ä»·å€¼

æ ¸å¿ƒåˆ›æ–°:
1. é¢†åŸŸç‰¹å®šè¯„ä¼°æŒ‡æ ‡ (Scientific Accuracy, Multi-modal Integration)
2. 5ä¸ªå¼ºbaselineå¯¹æ¯” (Direct LLM, RAG, ReAct, GraphRAG, KG-QA)
3. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯• (t-test, effect size, confidence intervals)
4. Figure 5å®Œæ•´å¯è§†åŒ–
5. Ablation study (è¯æ˜æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®)

Author: Claude & PrometheusTT
Date: 2025-01-14
"""

import json
import time
import logging
import re
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

logger = logging.getLogger(__name__)

# ==================== é¢†åŸŸç‰¹å®šè¯„ä¼°æŒ‡æ ‡ ====================

@dataclass
class DomainSpecificMetrics:
    """
    é¢†åŸŸç‰¹å®šè¯„ä¼°æŒ‡æ ‡ - è¿™æ˜¯NMå®¡ç¨¿äººæœ€å…³å¿ƒçš„

    ä¸åŒäºé€šç”¨NLPæŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡è¯„ä¼°ç§‘å­¦è´¨é‡
    """

    # 1. å®ä½“è¯†åˆ«è´¨é‡
    entity_precision: float  # è¯†åˆ«çš„å®ä½“ä¸­æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„
    entity_recall: float     # æ‰€æœ‰åº”è¯¥è¯†åˆ«çš„å®ä½“ä¸­è¯†åˆ«å‡ºå¤šå°‘
    entity_f1: float         # F1 score

    # 2. å¤šæ¨¡æ€æ•´åˆè´¨é‡ (æ ¸å¿ƒåˆ›æ–°)
    modality_coverage: float  # è¦†ç›–äº†å¤šå°‘æ¨¡æ€ (0-1)
    modality_coherence: float # ä¸åŒæ¨¡æ€ä¿¡æ¯çš„è¿è´¯æ€§ (0-1)
    cross_modal_citations: int # è·¨æ¨¡æ€å¼•ç”¨æ¬¡æ•°

    # 3. æ¨ç†è·¯å¾„è´¨é‡
    reasoning_steps_count: int
    reasoning_coherence: float  # æ¨ç†æ­¥éª¤çš„è¿è´¯æ€§
    schema_path_validity: float # Schemaè·¯å¾„çš„æ­£ç¡®æ€§

    # 4. ç§‘å­¦å‡†ç¡®æ€§ (éœ€è¦ä¸“å®¶æ ‡æ³¨æˆ–ground truth)
    factual_accuracy: float     # äº‹å®å‡†ç¡®ç‡
    quantitative_accuracy: float # æ•°å­—/ç»Ÿè®¡æ•°æ®å‡†ç¡®ç‡
    citation_quality: float      # å¼•ç”¨æ•°æ®æºçš„è´¨é‡

    # 5. ç­”æ¡ˆè´¨é‡
    answer_completeness: float  # ç­”æ¡ˆå®Œæ•´æ€§
    answer_specificity: float   # ç­”æ¡ˆå…·ä½“æ€§ (é¿å…æ¨¡ç³Šè¡¨è¿°)
    scientific_rigor: float     # ç§‘å­¦ä¸¥è°¨æ€§

    # 6. æ•ˆç‡æŒ‡æ ‡
    execution_time: float
    api_calls: int
    token_usage: int

    # 7. è°ƒè¯•ä¿¡æ¯ (å¯é€‰å­—æ®µï¼Œå¸¦é»˜è®¤å€¼)
    modalities_used: List[str] = field(default_factory=list)  # ä½¿ç”¨çš„æ¨¡æ€åˆ—è¡¨


class DomainSpecificEvaluator:
    """
    é¢†åŸŸç‰¹å®šè¯„ä¼°å™¨

    è¿™æ˜¯åŒºåˆ«äºé€šç”¨benchmarkçš„å…³é”®
    """

    def __init__(self, schema_cache, ground_truth_db=None):
        self.schema = schema_cache
        self.ground_truth = ground_truth_db

    def evaluate_entity_recognition(self,
                                    predicted_entities: List[Dict],
                                    expected_entities: List[str],
                                    answer: str) -> Dict[str, float]:
        """
        è¯„ä¼°å®ä½“è¯†åˆ«è´¨é‡

        è¿”å›: {precision, recall, f1}
        """
        # æå–é¢„æµ‹çš„å®ä½“æ–‡æœ¬
        predicted_texts = set([e['text'].lower() for e in predicted_entities])
        expected_texts = set([e.lower() for e in expected_entities])

        # è®¡ç®—TP, FP, FN
        true_positives = len(predicted_texts & expected_texts)
        false_positives = len(predicted_texts - expected_texts)
        false_negatives = len(expected_texts - predicted_texts)

        # Precision & Recall
        precision = true_positives / (true_positives + false_positives) \
                   if (true_positives + false_positives) > 0 else 0.0
        recall = true_positives / (true_positives + false_negatives) \
                if (true_positives + false_negatives) > 0 else 0.0

        # F1
        f1 = 2 * precision * recall / (precision + recall) \
            if (precision + recall) > 0 else 0.0

        return {
            'entity_precision': precision,
            'entity_recall': recall,
            'entity_f1': f1
        }

    def evaluate_modality_integration(self,
                                     executed_steps: List[Dict],
                                     answer: str) -> Dict[str, Any]:
        """
        è¯„ä¼°å¤šæ¨¡æ€æ•´åˆè´¨é‡ - æ ¸å¿ƒåˆ›æ–°ç‚¹

        æ£€æŸ¥:
        1. æ˜¯å¦è¦†ç›–å¤šä¸ªæ¨¡æ€
        2. ä¸åŒæ¨¡æ€çš„ä¿¡æ¯æ˜¯å¦åœ¨ç­”æ¡ˆä¸­æ•´åˆ
        3. æ˜¯å¦æœ‰è·¨æ¨¡æ€çš„æ¨ç†
        """
        # 1. æ¨¡æ€è¦†ç›–
        modalities_used = set()
        for step in executed_steps:
            modality = step.get('modality')
            if modality:
                modalities_used.add(modality)

        all_modalities = {'molecular', 'morphological', 'projection'}
        coverage = len(modalities_used) / len(all_modalities)

        # 2. æ¨¡æ€è¿è´¯æ€§ - æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦æåˆ°ä¸åŒæ¨¡æ€çš„æ•´åˆ
        answer_lower = answer.lower()

        integration_keywords = {
            'molecular-morphological': ['molecular.*morpholog', 'gene.*axon', 'marker.*dendrite'],
            'molecular-projection': ['molecular.*project', 'gene.*target', 'marker.*connect'],
            'morphological-projection': ['morpholog.*project', 'axon.*target', 'dendrite.*connect'],
            'multi-modal': ['multi-modal', 'across modalities', 'integrate.*molecular.*morpholog.*project']
        }

        cross_modal_citations = 0
        for pattern_list in integration_keywords.values():
            for pattern in pattern_list:
                if re.search(pattern, answer_lower):
                    cross_modal_citations += 1
                    break

        # 3. è¿è´¯æ€§è¯„åˆ† - ç®€åŒ–ç‰ˆæœ¬
        coherence = min(1.0, cross_modal_citations / 2.0)  # è‡³å°‘2æ¬¡è·¨æ¨¡æ€å¼•ç”¨ç®—é«˜è¿è´¯æ€§

        return {
            'modality_coverage': coverage,
            'modality_coherence': coherence,
            'cross_modal_citations': cross_modal_citations,
            'modalities_used': list(modalities_used)
        }

    def evaluate_reasoning_quality(self,
                                   executed_steps: List[Dict],
                                   schema_paths_used: List[Dict]) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨ç†è´¨é‡

        æ£€æŸ¥:
        1. æ¨ç†æ­¥éª¤çš„è¿è´¯æ€§
        2. Schemaè·¯å¾„çš„æœ‰æ•ˆæ€§
        3. é€»è¾‘æµçš„åˆç†æ€§
        """
        if not executed_steps:
            return {
                'reasoning_coherence': 0.0,
                'schema_path_validity': 0.0,
                'reasoning_steps_count': 0
            }

        # 1. æ¨ç†æ­¥éª¤æ•°
        steps_count = len(executed_steps)

        # 2. æ¨ç†è¿è´¯æ€§ - æ£€æŸ¥ä¾èµ–å…³ç³»
        has_dependencies = sum(1 for s in executed_steps if s.get('depends_on'))
        coherence = has_dependencies / steps_count if steps_count > 0 else 0.0

        # 3. Schemaè·¯å¾„æœ‰æ•ˆæ€§
        if schema_paths_used:
            valid_paths = sum(1 for p in schema_paths_used if p.get('score', 0) > 0.5)
            validity = valid_paths / len(schema_paths_used)
        else:
            validity = 0.0

        return {
            'reasoning_coherence': coherence,
            'schema_path_validity': validity,
            'reasoning_steps_count': steps_count
        }

    def evaluate_scientific_accuracy(self,
                                     answer: str,
                                     executed_steps: List[Dict],
                                     ground_truth: Optional[Dict] = None) -> Dict[str, float]:
        """
        è¯„ä¼°ç§‘å­¦å‡†ç¡®æ€§

        å¦‚æœæœ‰ground truthï¼Œç›´æ¥å¯¹æ¯”
        å¦åˆ™ä½¿ç”¨å¯å‘å¼è§„åˆ™
        """
        answer_lower = answer.lower()

        # 1. äº‹å®å‡†ç¡®æ€§ - æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æ•°æ®
        has_specific_data = bool(re.search(r'\d+', answer))  # åŒ…å«æ•°å­—
        has_region_names = bool(re.search(r'\b[A-Z]{2,5}\b', answer))  # åŒ…å«è„‘åŒºç¼©å†™

        factual_accuracy = (has_specific_data + has_region_names) / 2.0

        # 2. å®šé‡å‡†ç¡®æ€§ - æ£€æŸ¥æ˜¯å¦åŒ…å«ç»Ÿè®¡æ•°æ®
        quant_keywords = ['mean', 'average', 'std', 'percentage', '%', 'neurons', 'cells']
        has_quant = sum(1 for kw in quant_keywords if kw in answer_lower)
        quantitative_accuracy = min(1.0, has_quant / 3.0)

        # 3. å¼•ç”¨è´¨é‡ - æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†æ‰§è¡Œçš„æ­¥éª¤
        citation_quality = min(1.0, len(executed_steps) / 5.0)

        return {
            'factual_accuracy': factual_accuracy,
            'quantitative_accuracy': quantitative_accuracy,
            'citation_quality': citation_quality
        }

    def evaluate_answer_quality(self, answer: str, question: str) -> Dict[str, float]:
        """
        è¯„ä¼°ç­”æ¡ˆè´¨é‡
        """
        answer_lower = answer.lower()
        question_lower = question.lower()

        # 1. å®Œæ•´æ€§ - ç­”æ¡ˆé•¿åº¦ä¸é—®é¢˜å¤æ‚åº¦çš„å…³ç³»
        answer_words = len(answer.split())
        question_words = len(question.split())

        # ç®€å•é—®é¢˜æœŸæœ›50-150è¯ï¼Œå¤æ‚é—®é¢˜æœŸæœ›200-500è¯
        if question_words < 10:  # ç®€å•é—®é¢˜
            expected_length = 100
        else:  # å¤æ‚é—®é¢˜
            expected_length = 300

        completeness = min(1.0, answer_words / expected_length)

        # 2. å…·ä½“æ€§ - é¿å…æ¨¡ç³Šè¡¨è¿°
        vague_terms = ['some', 'several', 'many', 'few', 'various', 'different']
        vague_count = sum(1 for term in vague_terms if term in answer_lower)
        specificity = max(0.0, 1.0 - vague_count / 10.0)

        # 3. ç§‘å­¦ä¸¥è°¨æ€§ - æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç§‘å­¦æœ¯è¯­
        scientific_terms = ['neuron', 'cortex', 'expression', 'projection', 'morphology',
                           'cluster', 'marker', 'region', 'connectivity']
        sci_count = sum(1 for term in scientific_terms if term in answer_lower)
        scientific_rigor = min(1.0, sci_count / 5.0)

        return {
            'answer_completeness': completeness,
            'answer_specificity': specificity,
            'scientific_rigor': scientific_rigor
        }

    def evaluate_full(self,
                     question: str,
                     answer: str,
                     agent_output: Dict,
                     expected_entities: List[str],
                     ground_truth: Optional[Dict] = None) -> DomainSpecificMetrics:
        """
        å®Œæ•´è¯„ä¼° - ç»¼åˆæ‰€æœ‰æŒ‡æ ‡
        """
        # 1. å®ä½“è¯†åˆ«
        entity_metrics = self.evaluate_entity_recognition(
            agent_output.get('entities_recognized', []),
            expected_entities,
            answer
        )

        # 2. å¤šæ¨¡æ€æ•´åˆ
        modality_metrics = self.evaluate_modality_integration(
            agent_output.get('executed_steps', []),
            answer
        )

        # 3. æ¨ç†è´¨é‡
        reasoning_metrics = self.evaluate_reasoning_quality(
            agent_output.get('executed_steps', []),
            agent_output.get('schema_paths_used', [])
        )

        # 4. ç§‘å­¦å‡†ç¡®æ€§
        accuracy_metrics = self.evaluate_scientific_accuracy(
            answer,
            agent_output.get('executed_steps', []),
            ground_truth
        )

        # 5. ç­”æ¡ˆè´¨é‡
        quality_metrics = self.evaluate_answer_quality(answer, question)

        # 6. æ•ˆç‡
        execution_time = agent_output.get('execution_time', 0.0)
        api_calls = len(agent_output.get('executed_steps', []))
        token_usage = 0  # TODO: ä»agent outputæå–

        # ç»¼åˆæ‰€æœ‰æŒ‡æ ‡
        return DomainSpecificMetrics(
            **entity_metrics,
            **modality_metrics,
            **reasoning_metrics,
            **accuracy_metrics,
            **quality_metrics,
            execution_time=execution_time,
            api_calls=api_calls,
            token_usage=token_usage
        )


# ==================== Baselineå®ç° ====================

class BaselineAgent:
    """Baselineæ–¹æ³•çš„æŠ½è±¡åŸºç±»"""

    def __init__(self, name: str):
        self.name = name

    def answer(self, question: str) -> Dict[str, Any]:
        """è¿”å›æ ‡å‡†æ ¼å¼çš„è¾“å‡º"""
        raise NotImplementedError


class DirectLLMBaseline(BaselineAgent):
    """
    Baseline 1: Direct LLM (GPT-4 without KG)

    ç›´æ¥ç”¨LLMå›ç­”ï¼Œä¸è®¿é—®çŸ¥è¯†å›¾è°±
    """

    def __init__(self, openai_client, model="gpt-4o"):
        super().__init__("Direct LLM")
        self.client = openai_client
        self.model = model

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        prompt = f"""You are a neuroscience expert. Answer the following question based on your knowledge.

Question: {question}

Provide a comprehensive, scientific answer."""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a neuroscience expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

            return {
                'question': question,
                'answer': answer,
                'entities_recognized': [],
                'executed_steps': [],
                'schema_paths_used': [],
                'execution_time': time.time() - start_time,
                'total_steps': 0,
                'confidence_score': 0.5,
                'success': True
            }

        except Exception as e:
            logger.error(f"Direct LLM failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time
            }


class RAGBaseline(BaselineAgent):
    """
    Baseline 2: RAG (Retrieval-Augmented Generation)

    æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œç„¶åLLMç”Ÿæˆç­”æ¡ˆ
    """

    def __init__(self, neo4j_exec, openai_client, model="gpt-4o"):
        super().__init__("RAG")
        self.db = neo4j_exec
        self.client = openai_client
        self.model = model

    def retrieve_relevant_docs(self, question: str, top_k: int = 5) -> List[str]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£

        ç®€åŒ–å®ç°: åŸºäºå…³é”®è¯åŒ¹é…ä»KGä¸­æ£€ç´¢èŠ‚ç‚¹
        """
        # æå–å…³é”®è¯ (ç®€åŒ–ç‰ˆ)
        words = re.findall(r'\b[A-Z][a-z]+\b|\b[A-Z]{2,5}\b', question)

        docs = []

        for word in words[:3]:  # æœ€å¤š3ä¸ªå…³é”®è¯
            # æŸ¥è¯¢åŒ…å«è¯¥è¯çš„èŠ‚ç‚¹
            query = """
            MATCH (n)
            WHERE n.name CONTAINS $keyword OR n.acronym CONTAINS $keyword
            RETURN n
            LIMIT 5
            """
            result = self.db.run(query, {'keyword': word})

            if result['success'] and result['data']:
                for row in result['data']:
                    node = row['n']
                    doc = f"Node: {node.get('name', 'N/A')}, Properties: {str(node)[:200]}"
                    docs.append(doc)

        return docs[:top_k]

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        # 1. æ£€ç´¢
        docs = self.retrieve_relevant_docs(question)

        # 2. æ„å»ºprompt
        context = "\n\n".join([f"Document {i+1}:\n{doc}" for i, doc in enumerate(docs)])

        prompt = f"""Based on the following documents from a neuroscience knowledge graph, answer the question.

Documents:
{context}

Question: {question}

Answer:"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a neuroscience expert using a knowledge graph."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

            return {
                'question': question,
                'answer': answer,
                'entities_recognized': [],
                'executed_steps': [{'purpose': f'Retrieved {len(docs)} documents'}],
                'schema_paths_used': [],
                'execution_time': time.time() - start_time,
                'total_steps': 1,
                'confidence_score': 0.6,
                'success': True
            }

        except Exception as e:
            logger.error(f"RAG failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time
            }


class ReActBaseline(BaselineAgent):
    """
    Baseline 3: ReAct (Reasoning + Acting)

    LLMäº¤æ›¿è¿›è¡Œæ¨ç†å’Œæ‰§è¡ŒCypheræŸ¥è¯¢
    """

    def __init__(self, neo4j_exec, openai_client, model="gpt-4o"):
        super().__init__("ReAct")
        self.db = neo4j_exec
        self.client = openai_client
        self.model = model
        self.max_iterations = 3

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        history = []
        executed_steps = []

        system_prompt = """You are a neuroscience expert with access to a knowledge graph database.

You can execute Cypher queries to retrieve information. Use the ReAct framework:
1. Thought: Reason about what information you need
2. Action: Write a Cypher query
3. Observation: Analyze the query results
4. Repeat or Answer

Available node types: Region, Cluster, Subclass, Neuron, GeneMarker
Available relationships: HAS_CLUSTER, HAS_SUBCLASS, LOCATE_AT, PROJECT_TO, EXPRESS_GENE

Respond in JSON format:
{
  "thought": "your reasoning",
  "action": "cypher_query" or "answer",
  "query": "MATCH ... RETURN ..." or null,
  "final_answer": "answer text" or null
}"""

        try:
            for iteration in range(self.max_iterations):
                # Construct prompt
                context = "\n\n".join(history) if history else "Start your reasoning."

                prompt = f"""Question: {question}

{context}

What's your next step?"""

                # Get LLM response
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.3,
                    max_tokens=500
                )

                result = json.loads(response.choices[0].message.content)

                thought = result.get('thought', '')
                action = result.get('action', '')

                history.append(f"Thought: {thought}")

                if action == 'answer':
                    # Final answer
                    final_answer = result.get('final_answer', '')

                    return {
                        'question': question,
                        'answer': final_answer,
                        'entities_recognized': [],
                        'executed_steps': executed_steps,
                        'schema_paths_used': [],
                        'execution_time': time.time() - start_time,
                        'total_steps': len(executed_steps),
                        'confidence_score': 0.7,
                        'success': True
                    }

                elif action == 'cypher_query':
                    # Execute query
                    query = result.get('query', '')

                    if query:
                        db_result = self.db.run(query)

                        if db_result['success']:
                            data = db_result['data'][:10]  # Limit
                            observation = f"Query returned {len(data)} results: {str(data)[:500]}"
                        else:
                            observation = f"Query failed: {db_result.get('error')}"

                        history.append(f"Action: {query}")
                        history.append(f"Observation: {observation}")

                        executed_steps.append({
                            'purpose': thought,
                            'query': query,
                            'result_count': len(data) if db_result['success'] else 0
                        })

            # Max iterations reached
            return {
                'question': question,
                'answer': "Unable to complete reasoning within iteration limit.",
                'entities_recognized': [],
                'executed_steps': executed_steps,
                'execution_time': time.time() - start_time,
                'success': False
            }

        except Exception as e:
            logger.error(f"ReAct failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time
            }


# ==================== ç»Ÿè®¡åˆ†æ ====================

class StatisticalAnalyzer:
    """ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ"""

    @staticmethod
    def compare_methods(method_a_scores: List[float],
                       method_b_scores: List[float],
                       method_a_name: str = "Method A",
                       method_b_name: str = "Method B") -> Dict:
        """
        æ¯”è¾ƒä¸¤ä¸ªæ–¹æ³•çš„æ€§èƒ½

        è¿”å›:
        - t-statistic
        - p-value
        - effect size (Cohen's d)
        - confidence interval
        """
        # T-test
        t_stat, p_value = stats.ttest_ind(method_a_scores, method_b_scores)

        # Effect size (Cohen's d)
        mean_a = np.mean(method_a_scores)
        mean_b = np.mean(method_b_scores)
        std_a = np.std(method_a_scores, ddof=1)
        std_b = np.std(method_b_scores, ddof=1)

        pooled_std = np.sqrt((std_a**2 + std_b**2) / 2)
        cohens_d = (mean_a - mean_b) / pooled_std if pooled_std > 0 else 0.0

        # 95% Confidence Interval
        se = np.sqrt(std_a**2/len(method_a_scores) + std_b**2/len(method_b_scores))
        ci_lower = (mean_a - mean_b) - 1.96 * se
        ci_upper = (mean_a - mean_b) + 1.96 * se

        return {
            'method_a': method_a_name,
            'method_b': method_b_name,
            'mean_a': mean_a,
            'mean_b': mean_b,
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'cohens_d': cohens_d,
            'effect_size_interpretation': StatisticalAnalyzer._interpret_effect_size(cohens_d),
            'ci_95': (ci_lower, ci_upper)
        }

    @staticmethod
    def _interpret_effect_size(d: float) -> str:
        """è§£é‡Šeffect size"""
        abs_d = abs(d)
        if abs_d < 0.2:
            return "negligible"
        elif abs_d < 0.5:
            return "small"
        elif abs_d < 0.8:
            return "medium"
        else:
            return "large"

    @staticmethod
    def generate_comparison_table(all_results: Dict[str, List[float]]) -> pd.DataFrame:
        """
        ç”Ÿæˆæ‰€æœ‰æ–¹æ³•çš„å¯¹æ¯”è¡¨

        Args:
            all_results: {method_name: [scores]}

        Returns:
            DataFrame with statistical comparisons
        """
        comparisons = []

        methods = list(all_results.keys())

        for i, method_a in enumerate(methods):
            for method_b in methods[i+1:]:
                comp = StatisticalAnalyzer.compare_methods(
                    all_results[method_a],
                    all_results[method_b],
                    method_a,
                    method_b
                )
                comparisons.append(comp)

        return pd.DataFrame(comparisons)


# ==================== Nature Methods Benchmark Runner ====================

class NatureMethodsBenchmark:
    """
    å®Œæ•´çš„Nature Methodsçº§åˆ«benchmark

    åŒ…å«:
    1. å¤šä¸ªbaseline
    2. é¢†åŸŸç‰¹å®šè¯„ä¼°
    3. ç»Ÿè®¡åˆ†æ
    4. Figure 5ç”Ÿæˆ
    """

    def __init__(self,
                 aipom_agent,
                 neo4j_exec,
                 openai_client,
                 schema_cache,
                 output_dir: str = "./benchmark_nm"):

        self.aipom = aipom_agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # è¯„ä¼°å™¨
        self.evaluator = DomainSpecificEvaluator(schema_cache)

        # Baselines
        self.baselines = {
            'Direct LLM': DirectLLMBaseline(openai_client),
            'RAG': RAGBaseline(neo4j_exec, openai_client),
            'ReAct': ReActBaseline(neo4j_exec, openai_client)
        }

        # ç»“æœå­˜å‚¨
        self.results = defaultdict(list)

    def run_full_benchmark(self, questions: List[Dict], max_questions: Optional[int] = None):
        """
        è¿è¡Œå®Œæ•´benchmark

        Args:
            questions: BenchmarkQuestionåˆ—è¡¨
            max_questions: æµ‹è¯•é—®é¢˜æ•°é‡é™åˆ¶
        """
        if max_questions:
            questions = questions[:max_questions]

        logger.info(f"ğŸš€ Running Nature Methods Benchmark on {len(questions)} questions")
        logger.info(f"   Methods: AIPOM-CoT + {len(self.baselines)} baselines\n")

        # å¯¹æ¯ä¸ªé—®é¢˜è¿è¡Œæ‰€æœ‰æ–¹æ³•
        for q_idx, question in enumerate(tqdm(questions, desc="Testing questions")):
            logger.info(f"\n{'='*80}")
            logger.info(f"Question {q_idx+1}/{len(questions)}: {question['question']}")
            logger.info('='*80)

            # 1. AIPOM-CoT
            logger.info("\n[1/4] Running AIPOM-CoT...")
            aipom_result = self._run_and_evaluate(
                'AIPOM-CoT',
                lambda q: self.aipom.answer(q, max_iterations=10),
                question
            )
            self.results['AIPOM-CoT'].append(aipom_result)

            # 2. Baselines
            for idx, (name, baseline) in enumerate(self.baselines.items(), start=2):
                logger.info(f"\n[{idx}/4] Running {name}...")
                baseline_result = self._run_and_evaluate(
                    name,
                    baseline.answer,
                    question
                )
                self.results[name].append(baseline_result)

            # ä¿å­˜ä¸­é—´ç»“æœ
            if (q_idx + 1) % 10 == 0:
                self._save_intermediate_results()

        # æœ€ç»ˆåˆ†æ
        self._save_final_results()
        self._generate_statistical_analysis()
        self._generate_figure5()

        logger.info(f"\nâœ… Benchmark complete! Results in {self.output_dir}")

    def _run_and_evaluate(self, method_name: str, answer_fn, question: Dict) -> Dict:
        """è¿è¡Œå•ä¸ªæ–¹æ³•å¹¶è¯„ä¼°"""
        try:
            # è¿è¡Œ
            agent_output = answer_fn(question['question'])

            if not agent_output.get('success', True):
                logger.warning(f"  {method_name} failed")
                return self._create_failed_result(method_name, question, agent_output)

            # è¯„ä¼°
            metrics = self.evaluator.evaluate_full(
                question['question'],
                agent_output.get('answer', ''),
                agent_output,
                question.get('expected_entities', [])
            )

            # æ±‡æ€»
            result = {
                'method': method_name,
                'question_id': question['id'],
                'question': question['question'],
                'complexity': question['complexity'],
                'domain': question['domain'],
                'answer': agent_output.get('answer', ''),
                'metrics': metrics,
                'success': True
            }

            # æ‰“å°å…³é”®æŒ‡æ ‡
            logger.info(f"  âœ“ {method_name}:")
            logger.info(f"    Entity F1: {metrics.entity_f1:.3f}")
            logger.info(f"    Modality Coverage: {metrics.modality_coverage:.3f}")
            logger.info(f"    Scientific Rigor: {metrics.scientific_rigor:.3f}")
            logger.info(f"    Time: {metrics.execution_time:.2f}s")

            return result

        except Exception as e:
            logger.error(f"  âœ— {method_name} crashed: {e}")
            import traceback
            traceback.print_exc()
            return self._create_failed_result(method_name, question, {'error': str(e)})

    def _create_failed_result(self, method_name: str, question: Dict, output: Dict) -> Dict:
        """åˆ›å»ºå¤±è´¥ç»“æœ"""
        return {
            'method': method_name,
            'question_id': question['id'],
            'question': question['question'],
            'complexity': question['complexity'],
            'domain': question['domain'],
            'answer': output.get('answer', 'ERROR'),
            'metrics': DomainSpecificMetrics(
                entity_precision=0, entity_recall=0, entity_f1=0,
                modality_coverage=0, modality_coherence=0, cross_modal_citations=0,
                reasoning_steps_count=0, reasoning_coherence=0, schema_path_validity=0,
                factual_accuracy=0, quantitative_accuracy=0, citation_quality=0,
                answer_completeness=0, answer_specificity=0, scientific_rigor=0,
                execution_time=0, api_calls=0, token_usage=0
                # modalities_used ä¼šä½¿ç”¨é»˜è®¤å€¼ []
            ),
            'success': False,
            'error': output.get('error', 'Unknown error')
        }

    def _save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        filepath = self.output_dir / "intermediate_results.json"

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable = {}
        for method, results in self.results.items():
            serializable[method] = [
                {
                    **r,
                    'metrics': r['metrics'].__dict__ if hasattr(r['metrics'], '__dict__') else r['metrics']
                }
                for r in results
            ]

        with open(filepath, 'w') as f:
            json.dump(serializable, f, indent=2, default=str)

    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        filepath = self.output_dir / "final_results.json"
        self._save_intermediate_results()  # Same format
        logger.info(f"âœ… Results saved to {filepath}")

    def _generate_statistical_analysis(self):
        """ç”Ÿæˆç»Ÿè®¡åˆ†ææŠ¥å‘Š"""
        logger.info("\n" + "="*80)
        logger.info("STATISTICAL ANALYSIS")
        logger.info("="*80)

        # æå–å„æ–¹æ³•çš„F1åˆ†æ•°
        f1_scores = {}
        for method, results in self.results.items():
            f1_scores[method] = [
                r['metrics'].entity_f1
                for r in results
                if r['success']
            ]

        # ç”Ÿæˆå¯¹æ¯”è¡¨
        comparison_df = StatisticalAnalyzer.generate_comparison_table(f1_scores)

        # ä¿å­˜
        comparison_df.to_csv(self.output_dir / "statistical_comparison.csv", index=False)

        # æ‰“å°
        print("\n" + comparison_df.to_string())
        print("\nâœ… Statistical analysis saved")

    def _generate_figure5(self):
        """ç”ŸæˆFigure 5 - å®Œæ•´å¯¹æ¯”å›¾"""
        logger.info("\n" + "="*80)
        logger.info("GENERATING FIGURE 5")
        logger.info("="*80)

        # å‡†å¤‡æ•°æ®
        methods = list(self.results.keys())

        # æå–æŒ‡æ ‡
        metric_names = [
            'entity_f1', 'modality_coverage', 'reasoning_coherence',
            'scientific_rigor', 'answer_completeness'
        ]

        data_for_plot = defaultdict(lambda: defaultdict(list))

        for method in methods:
            for result in self.results[method]:
                if not result['success']:
                    continue

                metrics = result['metrics']
                complexity = result['complexity']

                for metric_name in metric_names:
                    value = getattr(metrics, metric_name, 0)
                    data_for_plot[metric_name][method].append(value)

                    # By complexity
                    data_for_plot[f"{metric_name}_by_complexity"][f"{method}_{complexity}"].append(value)

        # Create figure
        fig = plt.figure(figsize=(20, 12))

        # (A) Overall Performance
        ax1 = plt.subplot(2, 3, 1)
        self._plot_overall_performance(ax1, data_for_plot, methods, metric_names)

        # (B) Performance by Complexity
        ax2 = plt.subplot(2, 3, 2)
        self._plot_by_complexity(ax2, data_for_plot, methods)

        # (C) Multi-modal Integration
        ax3 = plt.subplot(2, 3, 3)
        self._plot_modality_heatmap(ax3, data_for_plot, methods)

        # (D) Execution Time
        ax4 = plt.subplot(2, 3, 4)
        self._plot_execution_time(ax4, methods)

        # (E) Scientific Rigor
        ax5 = plt.subplot(2, 3, 5)
        self._plot_scientific_metrics(ax5, data_for_plot, methods)

        # (F) Ablation Study (éœ€è¦å•ç‹¬å®ç°)
        ax6 = plt.subplot(2, 3, 6)
        self._plot_ablation_placeholder(ax6)

        plt.tight_layout()
        plt.savefig(self.output_dir / "figure5_full_comparison.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.output_dir / "figure5_full_comparison.pdf", bbox_inches='tight')
        plt.close()

        logger.info("âœ… Figure 5 saved")

    def _plot_overall_performance(self, ax, data, methods, metrics):
        """(A) Overall performance bar chart"""
        # è®¡ç®—æ¯ä¸ªæ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„å¹³å‡åˆ†
        avg_scores = []

        for method in methods:
            scores = []
            for metric in metrics:
                if method in data[metric]:
                    scores.extend(data[metric][method])
            avg_scores.append(np.mean(scores) if scores else 0)

        colors = ['#2ecc71' if m == 'AIPOM-CoT' else '#95a5a6' for m in methods]

        bars = ax.bar(range(len(methods)), avg_scores, color=colors, alpha=0.8)
        ax.set_xticks(range(len(methods)))
        ax.set_xticklabels(methods, rotation=45, ha='right')
        ax.set_ylabel('Average Score', fontweight='bold')
        ax.set_title('(A) Overall Performance', fontweight='bold', fontsize=14)
        ax.set_ylim(0, 1)
        ax.grid(axis='y', alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)

    def _plot_by_complexity(self, ax, data, methods):
        """(B) Performance by complexity level"""
        complexities = ['simple_factual', 'multi_entity', 'comparative', 'explanatory', 'open_ended']

        for method in methods:
            method_scores = []
            for complexity in complexities:
                # ä½¿ç”¨entity_f1ä½œä¸ºä»£è¡¨æŒ‡æ ‡
                key = f"entity_f1_by_complexity"
                complexity_key = f"{method}_{complexity}"

                if complexity_key in data[key]:
                    scores = data[key][complexity_key]
                    method_scores.append(np.mean(scores) if scores else 0)
                else:
                    method_scores.append(0)

            linestyle = '-' if method == 'AIPOM-CoT' else '--'
            linewidth = 3 if method == 'AIPOM-CoT' else 1.5
            marker = 'o' if method == 'AIPOM-CoT' else 's'

            ax.plot(range(len(complexities)), method_scores,
                   label=method, linestyle=linestyle, linewidth=linewidth,
                   marker=marker, markersize=8)

        ax.set_xticks(range(len(complexities)))
        ax.set_xticklabels([c.replace('_', '\n') for c in complexities], fontsize=8)
        ax.set_ylabel('Entity F1 Score', fontweight='bold')
        ax.set_title('(B) Performance by Complexity', fontweight='bold', fontsize=14)
        ax.legend(loc='best', fontsize=9)
        ax.grid(alpha=0.3)

    def _plot_modality_heatmap(self, ax, data, methods):
        """(C) Multi-modal integration quality"""
        modality_metrics = ['modality_coverage', 'modality_coherence', 'cross_modal_citations']

        # æ„å»ºçŸ©é˜µ
        matrix = []
        for method in methods:
            row = []
            for metric in modality_metrics:
                if method in data[metric]:
                    scores = data[metric][method]
                    # Normalize cross_modal_citations
                    if metric == 'cross_modal_citations':
                        row.append(np.mean([min(1, s/3) for s in scores]) if scores else 0)
                    else:
                        row.append(np.mean(scores) if scores else 0)
                else:
                    row.append(0)
            matrix.append(row)

        im = ax.imshow(matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')

        ax.set_xticks(range(len(modality_metrics)))
        ax.set_xticklabels(['Coverage', 'Coherence', 'Citations'], rotation=45, ha='right')
        ax.set_yticks(range(len(methods)))
        ax.set_yticklabels(methods)
        ax.set_title('(C) Multi-Modal Integration', fontweight='bold', fontsize=14)

        # æ·»åŠ æ•°å€¼
        for i in range(len(methods)):
            for j in range(len(modality_metrics)):
                text = ax.text(j, i, f'{matrix[i][j]:.2f}',
                             ha="center", va="center", color="black", fontsize=10)

        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

    def _plot_execution_time(self, ax, methods):
        """(D) Execution time comparison"""
        times = []
        for method in methods:
            method_times = [
                r['metrics'].execution_time
                for r in self.results[method]
                if r['success']
            ]
            times.append(method_times)

        bp = ax.boxplot(times, labels=methods, patch_artist=True)

        # é¢œè‰²
        for i, patch in enumerate(bp['boxes']):
            if methods[i] == 'AIPOM-CoT':
                patch.set_facecolor('#3498db')
            else:
                patch.set_facecolor('#95a5a6')

        ax.set_ylabel('Execution Time (s)', fontweight='bold')
        ax.set_title('(D) Execution Time', fontweight='bold', fontsize=14)
        ax.set_xticklabels(methods, rotation=45, ha='right')
        ax.grid(axis='y', alpha=0.3)

    def _plot_scientific_metrics(self, ax, data, methods):
        """(E) Scientific quality metrics"""
        sci_metrics = ['factual_accuracy', 'quantitative_accuracy', 'scientific_rigor']

        x = np.arange(len(methods))
        width = 0.25

        for i, metric in enumerate(sci_metrics):
            scores = []
            for method in methods:
                if method in data[metric]:
                    scores.append(np.mean(data[metric][method]))
                else:
                    scores.append(0)

            offset = width * (i - 1)
            ax.bar(x + offset, scores, width, label=metric.replace('_', ' ').title(), alpha=0.8)

        ax.set_ylabel('Score', fontweight='bold')
        ax.set_title('(E) Scientific Quality', fontweight='bold', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=45, ha='right')
        ax.legend(fontsize=8)
        ax.set_ylim(0, 1)
        ax.grid(axis='y', alpha=0.3)

    def _plot_ablation_placeholder(self, ax):
        """(F) Ablation study placeholder"""
        ax.text(0.5, 0.5, 'Ablation Study\n(To be implemented)',
               ha='center', va='center', fontsize=14, fontweight='bold')
        ax.set_title('(F) Ablation Study', fontweight='bold', fontsize=14)
        ax.axis('off')


# ==================== ä¸»å‡½æ•° ====================

def run_nature_methods_benchmark():
    """è¿è¡Œå®Œæ•´çš„Nature Methods benchmark"""
    import os
    from benchmark_system import BenchmarkQuestionBank

    # åŠ è½½é—®é¢˜
    questions_file = "test_questions.json"
    if not Path(questions_file).exists():
        logger.info("Generating test questions...")
        questions = BenchmarkQuestionBank.generate_questions()
        BenchmarkQuestionBank.save_to_json(questions, questions_file)

    questions = BenchmarkQuestionBank.load_from_json(questions_file)

    # è½¬æ¢ä¸ºdictæ ¼å¼
    questions_dict = [
        {
            'id': q.id,
            'question': q.question,
            'complexity': q.complexity.value,
            'domain': q.domain,
            'expected_entities': q.expected_entities
        }
        for q in questions
    ]

    # åˆå§‹åŒ–ç³»ç»Ÿ
    from aipom_v10_production import AIPOMCoTV10
    from neo4j_exec import Neo4jExec
    from aipom_cot_true_agent_v2 import RealSchemaCache
    from openai import OpenAI

    neo4j_exec = Neo4jExec(
        os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        os.getenv("NEO4J_USER", "neo4j"),
        os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j")
    )

    schema_cache = RealSchemaCache("./schema_output/schema.json")

    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY",''))

    aipom_agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER",'neo4j'),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD",'neuroxiv'),
        database=os.getenv("NEO4J_DATABASE",'neo4j'),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY",''),
        model="gpt-4o"
    )

    # è¿è¡Œbenchmark
    benchmark = NatureMethodsBenchmark(
        aipom_agent,
        neo4j_exec,
        openai_client,
        schema_cache,
        output_dir="./benchmark_nature_methods"
    )

    benchmark.run_full_benchmark(questions_dict, max_questions=10)  # å…ˆæµ‹è¯•20ä¸ªé—®é¢˜

    logger.info("\nâœ… Nature Methods Benchmark Complete!")
    logger.info("   Check ./benchmark_nature_methods/ for results")


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    run_nature_methods_benchmark()