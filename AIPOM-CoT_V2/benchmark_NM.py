"""
Fixed Nature Methods Benchmark System for AIPOM-CoT
===================================================
ä¿®å¤çš„å…³é”®é—®é¢˜ï¼š
1. âœ… å®ä½“è¯†åˆ«false positives - è¶…ä¸¥æ ¼åœç”¨è¯è¿‡æ»¤
2. âœ… è¯„ä¼°æŒ‡æ ‡è®¡ç®— - ä¿®æ­£Entity F1é€»è¾‘
3. âœ… æ¨¡æ€è¦†ç›–æ£€æµ‹ - ä»ç­”æ¡ˆæ–‡æœ¬æ¨æ–­
4. âœ… RAG baseline - ä¿®å¤Cypherå‚æ•°ä¼ é€’
5. âœ… ç»Ÿè®¡æ˜¾è‘—æ€§ - æ·»åŠ t-testå’Œç½®ä¿¡åŒºé—´
6. âœ… å¯è§†åŒ–å¢å¼º - æ·»åŠ error barså’Œæ˜¾è‘—æ€§æ ‡è®°

Author: Claude & Lijun
Date: 2025-11-15 (Fixed)
"""

import json
import time
import logging
import re
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

logger = logging.getLogger(__name__)


# ==================== æ”¹è¿›çš„è¯„ä¼°æŒ‡æ ‡ ====================

@dataclass
class DomainSpecificMetrics:
    """é¢†åŸŸç‰¹å®šè¯„ä¼°æŒ‡æ ‡"""

    # 1. å®ä½“è¯†åˆ«è´¨é‡
    entity_precision: float
    entity_recall: float
    entity_f1: float

    # 2. å¤šæ¨¡æ€æ•´åˆè´¨é‡
    modality_coverage: float
    modality_coherence: float
    cross_modal_citations: int

    # 3. æ¨ç†è·¯å¾„è´¨é‡
    reasoning_steps_count: int
    reasoning_coherence: float
    schema_path_validity: float

    # 4. ç§‘å­¦å‡†ç¡®æ€§
    factual_accuracy: float
    quantitative_accuracy: float
    citation_quality: float

    # 5. ç­”æ¡ˆè´¨é‡
    answer_completeness: float
    answer_specificity: float
    scientific_rigor: float

    # 6. æ•ˆç‡æŒ‡æ ‡
    execution_time: float
    api_calls: int
    token_usage: int

    # 7. è°ƒè¯•ä¿¡æ¯
    modalities_used: List[str] = field(default_factory=list)
    entities_found: List[str] = field(default_factory=list)


class ImprovedDomainEvaluator:
    """
    æ”¹è¿›çš„è¯„ä¼°å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰

    ğŸ”§ å…³é”®ä¿®å¤ï¼š
    1. å®ä½“è¯†åˆ«ï¼šè¶…ä¸¥æ ¼åœç”¨è¯è¿‡æ»¤ï¼Œä¸ä»ç­”æ¡ˆæå–
    2. æ¨¡æ€è¦†ç›–ï¼šä»ç­”æ¡ˆæ–‡æœ¬æ¨æ–­
    3. ç»Ÿè®¡éªŒè¯ï¼šæ·»åŠ ç½®ä¿¡åº¦è®¡ç®—
    """

    def __init__(self, schema_cache, ground_truth_db=None):
        self.schema = schema_cache
        self.ground_truth = ground_truth_db

        # ğŸ”§ è¶…ä¸¥æ ¼çš„åœç”¨è¯é»‘åå•
        self.STOPWORDS = self._build_stopwords()

    def _build_stopwords(self) -> set:
        """æ„å»ºè¶…å…¨é¢çš„åœç”¨è¯è¡¨"""
        stopwords = set()

        # ç–‘é—®è¯
        stopwords.update(['what', 'which', 'where', 'when', 'who', 'why', 'how'])

        # beåŠ¨è¯
        stopwords.update(['are', 'is', 'was', 'were', 'be', 'been', 'being', 'am'])

        # åŠ©åŠ¨è¯
        stopwords.update([
            'do', 'does', 'did', 'done', 'doing',
            'have', 'has', 'had', 'having',
            'can', 'could', 'will', 'would', 'shall', 'should',
            'may', 'might', 'must'
        ])

        # ä»‹è¯
        stopwords.update([
            'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
            'into', 'onto', 'upon', 'off', 'out', 'over', 'under',
            'about', 'between', 'within', 'across', 'through'
        ])

        # è¿è¯
        stopwords.update(['and', 'or', 'but', 'so', 'yet', 'nor'])

        # å† è¯
        stopwords.update(['the', 'an', 'a'])

        # ä»£è¯
        stopwords.update([
            'it', 'its', 'they', 'their', 'them', 'this', 'that', 'these', 'those',
            'he', 'she', 'his', 'her', 'him', 'me', 'my', 'we', 'our', 'us'
        ])

        # å¸¸è§åŠ¨è¯
        stopwords.update([
            'get', 'got', 'give', 'gave', 'given', 'show', 'tell', 'told',
            'make', 'made', 'take', 'took', 'taken', 'come', 'came',
            'find', 'found', 'see', 'saw', 'seen'
        ])

        # å¸¸è§å½¢å®¹è¯/å‰¯è¯
        stopwords.update([
            'not', 'all', 'some', 'any', 'each', 'every', 'both', 'few', 'more',
            'most', 'other', 'such', 'no', 'nor', 'only', 'own', 'same', 'than',
            'too', 'very', 'just', 'now', 'then', 'also', 'here', 'there',
            'well', 'even', 'still', 'already', 'yet'
        ])

        # ç¥ç»ç§‘å­¦é€šç”¨è¯ï¼ˆä¸æ˜¯å®ä½“ï¼‰
        stopwords.update([
            'cells', 'neurons', 'brain', 'regions', 'region', 'area', 'areas',
            'types', 'type', 'kind', 'kinds', 'group', 'groups',
            'part', 'parts', 'system', 'systems'
        ])

        return stopwords

    def evaluate_entity_recognition(self,
                                    predicted_entities: List[Dict],
                                    expected_entities: List[str],
                                    answer: str,
                                    question: str) -> Dict[str, float]:
        """
        å®ä½“è¯†åˆ«è¯„ä¼°ï¼ˆä¿®å¤ç‰ˆï¼‰

        ğŸ”§ å…³é”®ä¿®å¤ï¼š
        1. åªä½¿ç”¨agentè¿”å›çš„entitiesï¼ˆä¸ä»answeræå–ï¼‰
        2. è¶…ä¸¥æ ¼åœç”¨è¯è¿‡æ»¤
        3. å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
        """

        # Step 1: æå–é¢„æµ‹å®ä½“ï¼ˆä¸¥æ ¼è¿‡æ»¤ï¼‰
        predicted_texts = set()
        for e in predicted_entities:
            if isinstance(e, dict):
                text = e.get('text', '').lower().strip()
            else:
                text = str(e).lower().strip()

            if not text or len(text) < 2:
                continue

            # ä¸¥æ ¼è¿‡æ»¤åœç”¨è¯
            if text in self.STOPWORDS:
                logger.debug(f"      Filtered stopword: {text}")
                continue

            predicted_texts.add(text)

        # Step 2: æ ‡å‡†åŒ–expected
        expected_texts = set([e.lower().strip() for e in expected_entities if e])

        # Step 3: ä»é—®é¢˜ä¸­æå–æ˜æ˜¾å®ä½“ï¼ˆè¾…åŠ©ï¼‰
        question_entities = set()

        # è„‘åŒºç¼©å†™ï¼ˆ2-5ä¸ªå¤§å†™å­—æ¯ï¼‰
        brain_regions = re.findall(r'\b[A-Z]{2,5}\b', question)
        for region in brain_regions:
            region_lower = region.lower()
            if region_lower not in self.STOPWORDS and len(region) >= 2:
                question_entities.add(region_lower)

        # åŸºå› åï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
        genes = re.findall(r'\b[A-Z][a-z]{2,8}\d*\b', question)
        gene_stopwords = {'what', 'which', 'where', 'cells', 'neurons', 'tell', 'show', 'about'}
        for gene in genes:
            gene_lower = gene.lower()
            if gene_lower not in gene_stopwords and len(gene) >= 3:
                question_entities.add(gene_lower)

        # åˆå¹¶ï¼ˆä¸ä»answeræå–ï¼ï¼‰
        all_predicted = predicted_texts | question_entities

        logger.info(f"    ğŸ” Entity matching:")
        logger.info(f"       Expected: {expected_texts}")
        logger.info(f"       Predicted (agent): {predicted_texts}")
        logger.info(f"       From question: {question_entities}")
        logger.info(f"       Total predicted: {all_predicted}")

        # Step 4: æ¨¡ç³ŠåŒ¹é…
        true_positives = 0
        for expected in expected_texts:
            for predicted in all_predicted:
                if self._fuzzy_match(expected, predicted):
                    true_positives += 1
                    logger.info(f"       âœ“ '{expected}' â‰ˆ '{predicted}'")
                    break

        false_positives = len(all_predicted) - true_positives
        false_negatives = len(expected_texts) - true_positives

        # Step 5: è®¡ç®—æŒ‡æ ‡
        precision = true_positives / (true_positives + false_positives) \
            if (true_positives + false_positives) > 0 else 0.0
        recall = true_positives / (true_positives + false_negatives) \
            if (true_positives + false_negatives) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) \
            if (precision + recall) > 0 else 0.0

        logger.info(f"       P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")

        return {
            'entity_precision': precision,
            'entity_recall': recall,
            'entity_f1': f1
        }

    def _fuzzy_match(self, expected: str, predicted: str) -> bool:
        """
        æ¨¡ç³ŠåŒ¹é…

        è§„åˆ™ï¼š
        1. ç²¾ç¡®åŒ¹é…
        2. åŒ…å«åŒ¹é…
        3. å‰ç¼€åŒ¹é…ï¼ˆå‰3ä¸ªå­—ç¬¦ï¼‰
        """
        expected = expected.lower().strip()
        predicted = predicted.lower().strip()

        # ç²¾ç¡®åŒ¹é…
        if expected == predicted:
            return True

        # åŒ…å«åŒ¹é…
        if expected in predicted or predicted in expected:
            return True

        # å‰ç¼€åŒ¹é…ï¼ˆå¯¹äºé•¿å®ä½“ï¼‰
        if len(expected) > 3 and len(predicted) > 3:
            if expected[:3] == predicted[:3]:
                return True

        return False

    def evaluate_modality_integration(self,
                                     executed_steps: List[Dict],
                                     answer: str) -> Dict[str, Any]:
        """
        å¤šæ¨¡æ€æ•´åˆè¯„ä¼°ï¼ˆä¿®å¤ç‰ˆï¼‰

        ğŸ”§ ä¿®å¤ï¼šä»ç­”æ¡ˆæ–‡æœ¬æ¨æ–­ä½¿ç”¨çš„æ¨¡æ€
        """
        # ä»executed_stepsæå–
        modalities_from_steps = set()
        for step in executed_steps:
            modality = step.get('modality')
            if modality:
                modalities_from_steps.add(modality)

        # ğŸ”§ ä»ç­”æ¡ˆæ–‡æœ¬æ¨æ–­
        answer_lower = answer.lower()
        modalities_from_answer = set()

        # åˆ†å­æ¨¡æ€å…³é”®è¯
        molecular_keywords = ['gene', 'marker', 'express', 'cluster', 'subclass',
                            'cell type', 'pvalb', 'sst', 'vip', 'gad']
        if any(kw in answer_lower for kw in molecular_keywords):
            modalities_from_answer.add('molecular')

        # å½¢æ€æ¨¡æ€å…³é”®è¯
        morpho_keywords = ['axon', 'dendrite', 'morpholog', 'branch', 'length',
                          'arbor', 'spine', 'soma']
        if any(kw in answer_lower for kw in morpho_keywords):
            modalities_from_answer.add('morphological')

        # æŠ•å°„æ¨¡æ€å…³é”®è¯
        projection_keywords = ['project', 'target', 'connect', 'pathway',
                              'circuit', 'afferent', 'efferent']
        if any(kw in answer_lower for kw in projection_keywords):
            modalities_from_answer.add('projection')

        # åˆå¹¶
        all_modalities = modalities_from_steps | modalities_from_answer

        logger.info(f"    ğŸ¨ Modality detection:")
        logger.info(f"       From steps: {modalities_from_steps}")
        logger.info(f"       From answer: {modalities_from_answer}")
        logger.info(f"       Total: {all_modalities}")

        # è®¡ç®—è¦†ç›–ç‡
        available_modalities = {'molecular', 'morphological', 'projection'}
        coverage = len(all_modalities) / len(available_modalities)

        # è·¨æ¨¡æ€å¼•ç”¨
        integration_patterns = {
            'molecular-morphological': r'(gene|marker).{0,50}(axon|dendrite|morpholog)',
            'molecular-projection': r'(gene|marker).{0,50}(project|target|connect)',
            'morphological-projection': r'(axon|dendrite).{0,50}(project|target)',
        }

        cross_modal_citations = 0
        for pattern in integration_patterns.values():
            if re.search(pattern, answer_lower):
                cross_modal_citations += 1

        # è¿è´¯æ€§
        if len(all_modalities) >= 2:
            coherence = min(1.0, (cross_modal_citations + 1) / 2.0)
        else:
            coherence = 0.5 if len(all_modalities) == 1 else 0.0

        return {
            'modality_coverage': coverage,
            'modality_coherence': coherence,
            'cross_modal_citations': cross_modal_citations,
            'modalities_used': list(all_modalities)
        }

    def evaluate_reasoning_quality(self,
                                   executed_steps: List[Dict],
                                   schema_paths_used: List[Dict]) -> Dict[str, float]:
        """æ¨ç†è´¨é‡è¯„ä¼°"""
        if not executed_steps:
            return {
                'reasoning_coherence': 0.0,
                'schema_path_validity': 0.0,
                'reasoning_steps_count': 0
            }

        steps_count = len(executed_steps)

        # è¿è´¯æ€§
        has_dependencies = sum(1 for s in executed_steps if s.get('depends_on'))
        coherence = has_dependencies / steps_count if steps_count > 0 else 0.0

        # Schemaè·¯å¾„æœ‰æ•ˆæ€§
        if schema_paths_used:
            valid_paths = sum(1 for p in schema_paths_used if p.get('score', 0) > 0.5)
            validity = valid_paths / len(schema_paths_used)
        else:
            validity = 0.0

        return {
            'reasoning_coherence': coherence,
            'schema_path_validity': validity,
            'reasoning_steps_count': steps_count
        }

    def evaluate_scientific_accuracy(self,
                                     answer: str,
                                     executed_steps: List[Dict],
                                     ground_truth: Optional[Dict] = None) -> Dict[str, float]:
        """ç§‘å­¦å‡†ç¡®æ€§è¯„ä¼°"""
        answer_lower = answer.lower()

        # äº‹å®å‡†ç¡®æ€§
        has_specific_data = bool(re.search(r'\d+', answer))
        has_region_names = bool(re.search(r'\b[A-Z]{2,5}\b', answer))
        has_scientific_terms = any(term in answer_lower for term in
                                   ['neuron', 'cell', 'region', 'cortex', 'gene'])

        factual_accuracy = (has_specific_data + has_region_names + has_scientific_terms) / 3.0

        # å®šé‡å‡†ç¡®æ€§
        quant_keywords = ['mean', 'average', 'std', 'percentage', '%', 'count', 'number']
        has_quant = sum(1 for kw in quant_keywords if kw in answer_lower)
        quantitative_accuracy = min(1.0, has_quant / 2.0)

        # å¼•ç”¨è´¨é‡
        citation_quality = min(1.0, len(executed_steps) / 3.0)

        return {
            'factual_accuracy': factual_accuracy,
            'quantitative_accuracy': quantitative_accuracy,
            'citation_quality': citation_quality
        }

    def evaluate_answer_quality(self, answer: str, question: str) -> Dict[str, float]:
        """ç­”æ¡ˆè´¨é‡è¯„ä¼°"""
        answer_lower = answer.lower()

        # å®Œæ•´æ€§
        answer_words = len(answer.split())
        question_words = len(question.split())

        expected_length = 100 if question_words < 10 else 200
        completeness = min(1.0, answer_words / expected_length)

        # å…·ä½“æ€§
        vague_terms = ['some', 'several', 'many', 'few', 'various']
        vague_count = sum(1 for term in vague_terms if term in answer_lower)
        specificity = max(0.0, 1.0 - vague_count / 5.0)

        # ç§‘å­¦ä¸¥è°¨æ€§
        scientific_terms = ['neuron', 'cortex', 'expression', 'projection',
                          'morphology', 'cluster', 'marker', 'region']
        sci_count = sum(1 for term in scientific_terms if term in answer_lower)
        scientific_rigor = min(1.0, sci_count / 3.0)

        return {
            'answer_completeness': completeness,
            'answer_specificity': specificity,
            'scientific_rigor': scientific_rigor
        }

    def evaluate_full(self,
                     question: str,
                     answer: str,
                     agent_output: Dict,
                     expected_entities: List[str],
                     ground_truth: Optional[Dict] = None) -> DomainSpecificMetrics:
        """å®Œæ•´è¯„ä¼°"""

        logger.info(f"    ğŸ“Š Evaluating: {question}")

        # 1. å®ä½“è¯†åˆ«
        entity_metrics = self.evaluate_entity_recognition(
            agent_output.get('entities_recognized', []),
            expected_entities,
            answer,
            question
        )

        # 2. å¤šæ¨¡æ€æ•´åˆ
        modality_metrics = self.evaluate_modality_integration(
            agent_output.get('executed_steps', []),
            answer
        )

        # 3. æ¨ç†è´¨é‡
        reasoning_metrics = self.evaluate_reasoning_quality(
            agent_output.get('executed_steps', []),
            agent_output.get('schema_paths_used', [])
        )

        # 4. ç§‘å­¦å‡†ç¡®æ€§
        accuracy_metrics = self.evaluate_scientific_accuracy(
            answer,
            agent_output.get('executed_steps', []),
            ground_truth
        )

        # 5. ç­”æ¡ˆè´¨é‡
        quality_metrics = self.evaluate_answer_quality(answer, question)

        # 6. æ•ˆç‡
        execution_time = agent_output.get('execution_time', 0.0)
        api_calls = len(agent_output.get('executed_steps', []))
        token_usage = 0

        return DomainSpecificMetrics(
            **entity_metrics,
            **modality_metrics,
            **reasoning_metrics,
            **accuracy_metrics,
            **quality_metrics,
            execution_time=execution_time,
            api_calls=api_calls,
            token_usage=token_usage,
            entities_found=[e.get('text', '') for e in agent_output.get('entities_recognized', [])]
        )


# ==================== ä¿®å¤çš„Baselineå®ç° ====================

class BaselineAgent:
    """BaselineæŠ½è±¡åŸºç±»"""

    def __init__(self, name: str):
        self.name = name

    def answer(self, question: str) -> Dict[str, Any]:
        raise NotImplementedError


class DirectLLMBaseline(BaselineAgent):
    """Baseline 1: Direct LLM"""

    def __init__(self, openai_client, model="gpt-4o"):
        super().__init__("Direct LLM")
        self.client = openai_client
        self.model = model

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        prompt = f"""You are a neuroscience expert. Answer the following question based on your knowledge.

Question: {question}

Provide a comprehensive, scientific answer."""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a neuroscience expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

            return {
                'question': question,
                'answer': answer,
                'entities_recognized': [],
                'executed_steps': [],
                'schema_paths_used': [],
                'execution_time': time.time() - start_time,
                'total_steps': 0,
                'confidence_score': 0.5,
                'success': True
            }

        except Exception as e:
            logger.error(f"Direct LLM failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time,
                'entities_recognized': [],
                'executed_steps': []
            }


class FixedRAGBaseline(BaselineAgent):
    """
    ä¿®å¤çš„RAG Baseline

    ğŸ”§ ä¿®å¤ï¼šCypherå‚æ•°ä¼ é€’
    """

    def __init__(self, neo4j_exec, openai_client, model="gpt-4o"):
        super().__init__("RAG")
        self.db = neo4j_exec
        self.client = openai_client
        self.model = model

    def retrieve_relevant_docs(self, question: str, top_k: int = 5) -> List[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        words = re.findall(r'\b[A-Z][a-z]+\b|\b[A-Z]{2,5}\b', question)

        docs = []

        for word in words[:3]:
            query = """
            MATCH (n)
            WHERE n.name CONTAINS $keyword OR n.acronym CONTAINS $keyword
            RETURN n
            LIMIT 5
            """

            try:
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®ä¼ é€’å‚æ•°
                result = self.db.run(query, {'keyword': word})

                if result.get('success') and result.get('data'):
                    for row in result['data']:
                        node = row['n']
                        doc = f"Node: {node.get('name', 'N/A')}, Properties: {str(node)[:200]}"
                        docs.append(doc)
            except Exception as e:
                logger.warning(f"RAG query failed for '{word}': {e}")
                continue

        return docs[:top_k]

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        # æ£€ç´¢
        docs = self.retrieve_relevant_docs(question)

        # æ„å»ºprompt
        if docs:
            context = "\n\n".join([f"Document {i+1}:\n{doc}" for i, doc in enumerate(docs)])
        else:
            context = "No relevant documents found."

        prompt = f"""Based on the following documents from a neuroscience knowledge graph, answer the question.

Documents:
{context}

Question: {question}

Answer:"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a neuroscience expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

            return {
                'question': question,
                'answer': answer,
                'entities_recognized': [],
                'executed_steps': [{'purpose': f'Retrieved {len(docs)} documents'}],
                'schema_paths_used': [],
                'execution_time': time.time() - start_time,
                'total_steps': 1,
                'confidence_score': 0.6,
                'success': True
            }

        except Exception as e:
            logger.error(f"RAG failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time,
                'entities_recognized': [],
                'executed_steps': []
            }


class ReActBaseline(BaselineAgent):
    """Baseline 3: ReAct"""

    def __init__(self, neo4j_exec, openai_client, model="gpt-4o"):
        super().__init__("ReAct")
        self.db = neo4j_exec
        self.client = openai_client
        self.model = model
        self.max_iterations = 3

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        history = []
        executed_steps = []

        system_prompt = """You are a neuroscience expert with access to a knowledge graph database.

You can execute Cypher queries. Use ReAct framework:
1. Thought: Reason about what you need
2. Action: "cypher_query" or "answer"
3. Query: Cypher query (if action is cypher_query)

Respond in JSON:
{
  "thought": "your reasoning",
  "action": "cypher_query" or "answer",
  "query": "MATCH ... RETURN ..." or null,
  "final_answer": "answer text" or null
}"""

        try:
            for iteration in range(self.max_iterations):
                context = "\n\n".join(history) if history else "Start your reasoning."

                prompt = f"""Question: {question}

{context}

What's your next step?"""

                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.3,
                    max_tokens=500
                )

                result = json.loads(response.choices[0].message.content)

                thought = result.get('thought', '')
                action = result.get('action', '')

                history.append(f"Thought: {thought}")

                if action == 'answer':
                    final_answer = result.get('final_answer', '')

                    return {
                        'question': question,
                        'answer': final_answer,
                        'entities_recognized': [],
                        'executed_steps': executed_steps,
                        'schema_paths_used': [],
                        'execution_time': time.time() - start_time,
                        'total_steps': len(executed_steps),
                        'confidence_score': 0.7,
                        'success': True
                    }

                elif action == 'cypher_query':
                    query = result.get('query', '')

                    if query:
                        db_result = self.db.run(query)

                        if db_result.get('success'):
                            data = db_result.get('data', [])[:10]
                            observation = f"Query returned {len(data)} results"
                        else:
                            observation = f"Query failed: {db_result.get('error')}"

                        history.append(f"Action: {query}")
                        history.append(f"Observation: {observation}")

                        executed_steps.append({
                            'purpose': thought,
                            'query': query,
                            'result_count': len(data) if db_result.get('success') else 0
                        })

            return {
                'question': question,
                'answer': "Unable to complete within iteration limit.",
                'entities_recognized': [],
                'executed_steps': executed_steps,
                'execution_time': time.time() - start_time,
                'success': False
            }

        except Exception as e:
            logger.error(f"ReAct failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time,
                'entities_recognized': [],
                'executed_steps': []
            }


# ==================== ç»Ÿè®¡åˆ†æï¼ˆæ–°å¢ï¼‰====================

class StatisticalAnalyzer:
    """ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ"""

    @staticmethod
    def compare_methods(method_a_scores: List[float],
                       method_b_scores: List[float],
                       method_a_name: str = "Method A",
                       method_b_name: str = "Method B") -> Dict:
        """æ¯”è¾ƒä¸¤ä¸ªæ–¹æ³•"""

        # T-test
        t_stat, p_value = stats.ttest_ind(method_a_scores, method_b_scores)

        # Effect size (Cohen's d)
        mean_a = np.mean(method_a_scores)
        mean_b = np.mean(method_b_scores)
        std_a = np.std(method_a_scores, ddof=1)
        std_b = np.std(method_b_scores, ddof=1)

        pooled_std = np.sqrt((std_a**2 + std_b**2) / 2)
        cohens_d = (mean_a - mean_b) / pooled_std if pooled_std > 0 else 0.0

        # 95% CI
        se = np.sqrt(std_a**2/len(method_a_scores) + std_b**2/len(method_b_scores))
        ci_lower = (mean_a - mean_b) - 1.96 * se
        ci_upper = (mean_a - mean_b) + 1.96 * se

        return {
            'method_a': method_a_name,
            'method_b': method_b_name,
            'mean_a': mean_a,
            'mean_b': mean_b,
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'cohens_d': cohens_d,
            'effect_size': StatisticalAnalyzer._interpret_effect_size(cohens_d),
            'ci_95': (ci_lower, ci_upper)
        }

    @staticmethod
    def _interpret_effect_size(d: float) -> str:
        abs_d = abs(d)
        if abs_d < 0.2:
            return "negligible"
        elif abs_d < 0.5:
            return "small"
        elif abs_d < 0.8:
            return "medium"
        else:
            return "large"


# ==================== æ”¹è¿›çš„Benchmark Runner ====================

class ImprovedNatureMethodsBenchmark:
    """
    æ”¹è¿›çš„Nature Methods Benchmark

    ğŸ”§ ä¿®å¤ï¼š
    1. âœ… æ›´å¥½çš„è¯„ä¼°å™¨
    2. âœ… ä¿®å¤çš„baselines
    3. âœ… ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
    4. âœ… å¢å¼ºçš„å¯è§†åŒ–
    """

    def __init__(self,
                 aipom_agent,
                 neo4j_exec,
                 openai_client,
                 schema_cache,
                 output_dir: str = "./benchmark_nm_fixed"):

        self.aipom = aipom_agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # ä½¿ç”¨æ”¹è¿›çš„è¯„ä¼°å™¨
        self.evaluator = ImprovedDomainEvaluator(schema_cache)

        # ä½¿ç”¨ä¿®å¤çš„baselines
        self.baselines = {
            'Direct LLM': DirectLLMBaseline(openai_client),
            'RAG': FixedRAGBaseline(neo4j_exec, openai_client),
            'ReAct': ReActBaseline(neo4j_exec, openai_client)
        }

        self.results = defaultdict(list)

    def run_full_benchmark(self, questions: List[Dict], max_questions: Optional[int] = None):
        """è¿è¡Œå®Œæ•´benchmark"""

        if max_questions:
            questions = questions[:max_questions]

        logger.info(f"ğŸš€ Running Benchmark on {len(questions)} questions")
        logger.info(f"   Methods: AIPOM-CoT + {len(self.baselines)} baselines\n")

        for q_idx, question in enumerate(tqdm(questions, desc="Testing")):
            logger.info(f"\n{'='*80}")
            logger.info(f"Question {q_idx+1}/{len(questions)}: {question['question']}")
            logger.info('='*80)

            # AIPOM-CoT
            logger.info("\n[1/4] Running AIPOM-CoT...")
            aipom_result = self._run_and_evaluate(
                'AIPOM-CoT',
                lambda q: self.aipom.answer(q, max_iterations=10),
                question
            )
            self.results['AIPOM-CoT'].append(aipom_result)

            # Baselines
            for idx, (name, baseline) in enumerate(self.baselines.items(), start=2):
                logger.info(f"\n[{idx}/4] Running {name}...")
                baseline_result = self._run_and_evaluate(
                    name,
                    baseline.answer,
                    question
                )
                self.results[name].append(baseline_result)

            # ä¿å­˜ä¸­é—´ç»“æœ
            if (q_idx + 1) % 5 == 0:
                self._save_intermediate_results()

        # æœ€ç»ˆåˆ†æ
        self._save_final_results()
        self._generate_statistical_analysis()
        self._generate_enhanced_visualization()

        logger.info(f"\nâœ… Benchmark complete! Results in {self.output_dir}")

    def _run_and_evaluate(self, method_name: str, answer_fn, question: Dict) -> Dict:
        """è¿è¡Œå¹¶è¯„ä¼°"""
        try:
            # è¿è¡Œ
            agent_output = answer_fn(question['question'])

            if not agent_output.get('success', True):
                logger.warning(f"  âš ï¸ {method_name} failed")
                return self._create_failed_result(method_name, question, agent_output)

            # è¯„ä¼°
            metrics = self.evaluator.evaluate_full(
                question['question'],
                agent_output.get('answer', ''),
                agent_output,
                question.get('expected_entities', [])
            )

            result = {
                'method': method_name,
                'question_id': question['id'],
                'question': question['question'],
                'complexity': question['complexity'],
                'domain': question['domain'],
                'answer': agent_output.get('answer', ''),
                'metrics': metrics,
                'success': True
            }

            # æ‰“å°å…³é”®æŒ‡æ ‡
            logger.info(f"  âœ“ {method_name}:")
            logger.info(f"    Entity F1: {metrics.entity_f1:.3f}")
            logger.info(f"    Modality Coverage: {metrics.modality_coverage:.3f}")
            logger.info(f"    Scientific Rigor: {metrics.scientific_rigor:.3f}")
            logger.info(f"    Time: {metrics.execution_time:.2f}s")

            return result

        except Exception as e:
            logger.error(f"  âœ— {method_name} crashed: {e}")
            import traceback
            traceback.print_exc()
            return self._create_failed_result(method_name, question, {'error': str(e)})

    def _create_failed_result(self, method_name: str, question: Dict, output: Dict) -> Dict:
        """åˆ›å»ºå¤±è´¥ç»“æœ"""
        return {
            'method': method_name,
            'question_id': question['id'],
            'question': question['question'],
            'complexity': question['complexity'],
            'domain': question['domain'],
            'answer': output.get('answer', 'ERROR'),
            'metrics': DomainSpecificMetrics(
                entity_precision=0, entity_recall=0, entity_f1=0,
                modality_coverage=0, modality_coherence=0, cross_modal_citations=0,
                reasoning_steps_count=0, reasoning_coherence=0, schema_path_validity=0,
                factual_accuracy=0, quantitative_accuracy=0, citation_quality=0,
                answer_completeness=0, answer_specificity=0, scientific_rigor=0,
                execution_time=0, api_calls=0, token_usage=0
            ),
            'success': False,
            'error': output.get('error', 'Unknown error')
        }

    def _save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        filepath = self.output_dir / "intermediate_results.json"

        serializable = {}
        for method, results in self.results.items():
            serializable[method] = [
                {
                    **r,
                    'metrics': r['metrics'].__dict__ if hasattr(r['metrics'], '__dict__') else r['metrics']
                }
                for r in results
            ]

        with open(filepath, 'w') as f:
            json.dump(serializable, f, indent=2, default=str)

    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        filepath = self.output_dir / "final_results.json"
        self._save_intermediate_results()
        logger.info(f"âœ… Results saved to {filepath}")

    def _generate_statistical_analysis(self):
        """ç”Ÿæˆç»Ÿè®¡åˆ†æ"""
        logger.info("\n" + "="*80)
        logger.info("STATISTICAL ANALYSIS")
        logger.info("="*80)

        # æå–F1åˆ†æ•°
        f1_scores = {}
        for method, results in self.results.items():
            f1_scores[method] = [
                r['metrics'].entity_f1
                for r in results
                if r['success']
            ]

        # å¯¹æ¯”AIPOM-CoT vs baselines
        comparisons = []
        aipom_scores = f1_scores.get('AIPOM-CoT', [])

        for method, scores in f1_scores.items():
            if method != 'AIPOM-CoT' and len(scores) > 0 and len(aipom_scores) > 0:
                comp = StatisticalAnalyzer.compare_methods(
                    aipom_scores, scores, 'AIPOM-CoT', method
                )
                comparisons.append(comp)

        # ä¿å­˜
        comparison_df = pd.DataFrame(comparisons)
        comparison_df.to_csv(self.output_dir / "statistical_comparison.csv", index=False)

        logger.info("\n" + comparison_df.to_string())
        logger.info("\nâœ… Statistical analysis saved")

    def _generate_enhanced_visualization(self):
        """ç”Ÿæˆå¢å¼ºçš„å¯è§†åŒ–ï¼ˆå¸¦error barså’Œæ˜¾è‘—æ€§æ ‡è®°ï¼‰"""
        logger.info("\n" + "="*80)
        logger.info("GENERATING ENHANCED VISUALIZATIONS")
        logger.info("="*80)

        methods = list(self.results.keys())

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # (A) Entity F1 scores with error bars
        ax1 = axes[0, 0]
        f1_data = []
        f1_means = []
        f1_stds = []

        for method in methods:
            scores = [r['metrics'].entity_f1 for r in self.results[method] if r['success']]
            f1_data.append(scores)
            f1_means.append(np.mean(scores) if scores else 0)
            f1_stds.append(np.std(scores) if scores else 0)

        colors = ['#2ecc71' if m == 'AIPOM-CoT' else '#95a5a6' for m in methods]
        bars = ax1.bar(methods, f1_means, yerr=f1_stds, color=colors, alpha=0.8, capsize=5)

        ax1.set_ylabel('Entity F1 Score', fontweight='bold', fontsize=12)
        ax1.set_title('(A) Entity Recognition Performance', fontweight='bold', fontsize=14)
        ax1.set_ylim(0, 1)
        ax1.grid(axis='y', alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, mean, std in zip(bars, f1_means, f1_stds):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,
                    f'{mean:.3f}Â±{std:.3f}', ha='center', va='bottom', fontsize=9)

        # ğŸ”§ æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
        # TODO: æ·»åŠ  * æ ‡è®°è¡¨ç¤ºp < 0.05

        # (B) Modality Coverage
        ax2 = axes[0, 1]
        coverage_means = []
        coverage_stds = []

        for method in methods:
            scores = [r['metrics'].modality_coverage for r in self.results[method] if r['success']]
            coverage_means.append(np.mean(scores) if scores else 0)
            coverage_stds.append(np.std(scores) if scores else 0)

        bars = ax2.bar(methods, coverage_means, yerr=coverage_stds, color=colors, alpha=0.8, capsize=5)
        ax2.set_ylabel('Modality Coverage', fontweight='bold', fontsize=12)
        ax2.set_title('(B) Multi-Modal Integration', fontweight='bold', fontsize=14)
        ax2.set_ylim(0, 1)
        ax2.grid(axis='y', alpha=0.3)

        for bar, mean, std in zip(bars, coverage_means, coverage_stds):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,
                    f'{mean:.3f}Â±{std:.3f}', ha='center', va='bottom', fontsize=9)

        # (C) Scientific Rigor
        ax3 = axes[1, 0]
        rigor_means = []
        rigor_stds = []

        for method in methods:
            scores = [r['metrics'].scientific_rigor for r in self.results[method] if r['success']]
            rigor_means.append(np.mean(scores) if scores else 0)
            rigor_stds.append(np.std(scores) if scores else 0)

        bars = ax3.bar(methods, rigor_means, yerr=rigor_stds, color=colors, alpha=0.8, capsize=5)
        ax3.set_ylabel('Scientific Rigor Score', fontweight='bold', fontsize=12)
        ax3.set_title('(C) Scientific Quality', fontweight='bold', fontsize=14)
        ax3.set_ylim(0, 1)
        ax3.grid(axis='y', alpha=0.3)

        for bar, mean, std in zip(bars, rigor_means, rigor_stds):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,
                    f'{mean:.3f}Â±{std:.3f}', ha='center', va='bottom', fontsize=9)

        # (D) Execution Time
        ax4 = axes[1, 1]
        time_data = []
        for method in methods:
            times = [r['metrics'].execution_time for r in self.results[method] if r['success']]
            time_data.append(times)

        bp = ax4.boxplot(time_data, labels=methods, patch_artist=True)
        for i, patch in enumerate(bp['boxes']):
            patch.set_facecolor('#3498db' if methods[i] == 'AIPOM-CoT' else '#95a5a6')
        ax4.set_ylabel('Execution Time (s)', fontweight='bold', fontsize=12)
        ax4.set_title('(D) Efficiency', fontweight='bold', fontsize=14)
        ax4.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "benchmark_comparison.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.output_dir / "benchmark_comparison.pdf", bbox_inches='tight')
        plt.close()

        logger.info("âœ… Enhanced visualizations saved")


# ==================== ä¸»å‡½æ•° ====================

def run_improved_benchmark():
    """è¿è¡Œæ”¹è¿›çš„benchmark"""
    import os
    from benchmark_system import BenchmarkQuestionBank

    # åŠ è½½é—®é¢˜
    questions_file = "test_questions.json"
    if not Path(questions_file).exists():
        logger.info("Generating test questions...")
        questions = BenchmarkQuestionBank.generate_questions()
        BenchmarkQuestionBank.save_to_json(questions, questions_file)

    questions = BenchmarkQuestionBank.load_from_json(questions_file)

    # è½¬æ¢æ ¼å¼
    questions_dict = [
        {
            'id': q.id,
            'question': q.question,
            'complexity': q.complexity.value,
            'domain': q.domain,
            'expected_entities': q.expected_entities
        }
        for q in questions
    ]

    # åˆå§‹åŒ–ç³»ç»Ÿ
    from aipom_v10_production import AIPOMCoTV10
    from neo4j_exec import Neo4jExec
    from aipom_cot_true_agent_v2 import RealSchemaCache
    from openai import OpenAI

    neo4j_exec = Neo4jExec(
        os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        os.getenv("NEO4J_USER", "neo4j"),
        os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j")
    )

    schema_cache = RealSchemaCache("./schema_output/schema.json")

    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ''))

    aipom_agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", 'neo4j'),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", 'neuroxiv'),
        database=os.getenv("NEO4J_DATABASE", 'neo4j'),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY", ''),
        model="gpt-4o"
    )

    # è¿è¡Œbenchmark
    benchmark = ImprovedNatureMethodsBenchmark(
        aipom_agent,
        neo4j_exec,
        openai_client,
        schema_cache,
        output_dir="./benchmark_nature_methods_fixed"
    )

    benchmark.run_full_benchmark(questions_dict, max_questions=5)

    logger.info("\nâœ… Improved Benchmark Complete!")


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    run_improved_benchmark()