"""
Fixed Nature Methods Benchmark System for AIPOM-CoT
===================================================
‰øÆÂ§ç‰∫Ü‰ª•‰∏ãÈóÆÈ¢òÔºö
1. ÂÆû‰ΩìËØÜÂà´Â§±Ë¥• - ÊîπËøõÊ®°Á≥äÂåπÈÖçÂíåÂ§ßÂ∞èÂÜôÂ§ÑÁêÜ
2. ËØÑ‰º∞ÊåáÊ†áËøá‰∏• - ‰ΩøÁî®Êõ¥ÂêàÁêÜÁöÑËØÑÂàÜÊ†áÂáÜ
3. RAG baselineÂèÇÊï∞ÈóÆÈ¢ò - ‰øÆÂ§çCypherÊü•ËØ¢
4. Â¢ûÂº∫Ë∞ÉËØï‰ø°ÊÅØ - ‰æø‰∫éÈóÆÈ¢òËØäÊñ≠

Author: Claude & Lijun
Date: 2025-11-15
"""

import json
import time
import logging
import re
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

logger = logging.getLogger(__name__)

# ==================== ÊîπËøõÁöÑËØÑ‰º∞ÊåáÊ†á ====================

@dataclass
class DomainSpecificMetrics:
    """È¢ÜÂüüÁâπÂÆöËØÑ‰º∞ÊåáÊ†á"""

    # 1. ÂÆû‰ΩìËØÜÂà´Ë¥®Èáè
    entity_precision: float
    entity_recall: float
    entity_f1: float

    # 2. Â§öÊ®°ÊÄÅÊï¥ÂêàË¥®Èáè
    modality_coverage: float
    modality_coherence: float
    cross_modal_citations: int

    # 3. Êé®ÁêÜË∑ØÂæÑË¥®Èáè
    reasoning_steps_count: int
    reasoning_coherence: float
    schema_path_validity: float

    # 4. ÁßëÂ≠¶ÂáÜÁ°ÆÊÄß
    factual_accuracy: float
    quantitative_accuracy: float
    citation_quality: float

    # 5. Á≠îÊ°àË¥®Èáè
    answer_completeness: float
    answer_specificity: float
    scientific_rigor: float

    # 6. ÊïàÁéáÊåáÊ†á
    execution_time: float
    api_calls: int
    token_usage: int

    # 7. Ë∞ÉËØï‰ø°ÊÅØ
    modalities_used: List[str] = field(default_factory=list)
    entities_found: List[str] = field(default_factory=list)


class ImprovedDomainEvaluator:
    """
    ÊîπËøõÁöÑËØÑ‰º∞Âô® - Ëß£ÂÜ≥ËØÑÂàÜËøá‰∏•ÈóÆÈ¢ò
    """

    def __init__(self, schema_cache, ground_truth_db=None):
        self.schema = schema_cache
        self.ground_truth = ground_truth_db

    def evaluate_entity_recognition(self,
                                    predicted_entities: List[Dict],
                                    expected_entities: List[str],
                                    answer: str,
                                    question: str) -> Dict[str, float]:
        """
        ÊîπËøõÁöÑÂÆû‰ΩìËØÜÂà´ËØÑ‰º∞

        ‰øÆÂ§çÔºö
        1. ÊîØÊåÅÊ®°Á≥äÂåπÈÖçÔºàÂ§ßÂ∞èÂÜô‰∏çÊïèÊÑüÔºâ
        2. ‰ªéÁ≠îÊ°àÊñáÊú¨‰∏≠ÊèêÂèñÂèØËÉΩÁöÑÂÆû‰Ωì
        3. ‰ΩøÁî®ÈÉ®ÂàÜÂåπÈÖçËÄåÈùûÁ≤æÁ°ÆÂåπÈÖç
        """
        # ÊèêÂèñÈ¢ÑÊµãÁöÑÂÆû‰ΩìÊñáÊú¨
        predicted_texts = set()
        for e in predicted_entities:
            if isinstance(e, dict):
                predicted_texts.add(e.get('text', '').lower())
            else:
                predicted_texts.add(str(e).lower())

        # Ê†áÂáÜÂåñexpected entities
        expected_texts = set([e.lower() for e in expected_entities])

        # üîß FIX 1: ‰ªéÁ≠îÊ°à‰∏≠ÊèêÂèñÂèØËÉΩÁöÑÂÆû‰ΩìÔºàËÑëÂå∫Áº©ÂÜôÁ≠âÔºâ
        answer_entities = self._extract_entities_from_text(answer)
        question_entities = self._extract_entities_from_text(question)

        # ÂêàÂπ∂ÊâÄÊúâÂèØËÉΩÁöÑÈ¢ÑÊµãÂÆû‰Ωì
        all_predicted = predicted_texts | answer_entities | question_entities

        logger.info(f"    üîç Entity matching:")
        logger.info(f"       Expected: {expected_texts}")
        logger.info(f"       Predicted (from agent): {predicted_texts}")
        logger.info(f"       From answer: {answer_entities}")
        logger.info(f"       From question: {question_entities}")
        logger.info(f"       All predicted: {all_predicted}")

        # üîß FIX 2: ‰ΩøÁî®Ê®°Á≥äÂåπÈÖç
        true_positives = 0
        for expected in expected_texts:
            for predicted in all_predicted:
                if self._fuzzy_match(expected, predicted):
                    true_positives += 1
                    logger.info(f"       ‚úì Matched: '{expected}' ‚âà '{predicted}'")
                    break

        false_positives = len(all_predicted) - true_positives
        false_negatives = len(expected_texts) - true_positives

        # ËÆ°ÁÆóÊåáÊ†á
        precision = true_positives / (true_positives + false_positives) \
                   if (true_positives + false_positives) > 0 else 0.0
        recall = true_positives / (true_positives + false_negatives) \
                if (true_positives + false_negatives) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) \
            if (precision + recall) > 0 else 0.0

        logger.info(f"       P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")

        return {
            'entity_precision': precision,
            'entity_recall': recall,
            'entity_f1': f1
        }

    def _extract_entities_from_text(self, text: str) -> set:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÂèØËÉΩÁöÑÂÆû‰ΩìÔºàËÑëÂå∫„ÄÅÂü∫Âõ†Á≠âÔºâ"""
        entities = set()

        # 1. ÊèêÂèñËÑëÂå∫Áº©ÂÜô (2-5‰∏™Â§ßÂÜôÂ≠óÊØç)
        brain_regions = re.findall(r'\b[A-Z]{2,5}\b', text)
        entities.update([r.lower() for r in brain_regions])

        # 2. ÊèêÂèñÂü∫Âõ†ÂêçÁß∞ (È¶ñÂ≠óÊØçÂ§ßÂÜôÁöÑÂçïËØçÔºåÂ¶ÇPvalb, Sst, Vip)
        gene_names = re.findall(r'\b[A-Z][a-z]{2,10}\b', text)
        entities.update([g.lower() for g in gene_names])

        # 3. ÊèêÂèñÂ∏∏ËßÅÁ•ûÁªèÁßëÂ≠¶ÊúØËØ≠
        neuro_terms = ['cluster', 'subclass', 'neuron', 'cell']
        for term in neuro_terms:
            if term in text.lower():
                entities.add(term)

        return entities

    def _fuzzy_match(self, expected: str, predicted: str) -> bool:
        """
        Ê®°Á≥äÂåπÈÖç‰∏§‰∏™ÂÆû‰ΩìÂêçÁß∞

        ÊîØÊåÅÔºö
        - Â§ßÂ∞èÂÜô‰∏çÊïèÊÑü
        - ÈÉ®ÂàÜÂåπÈÖçÔºà‰∏Ä‰∏™ÂåÖÂê´Âè¶‰∏Ä‰∏™Ôºâ
        - Áõ∏‰ººÂ∫¶ÂåπÈÖç
        """
        expected = expected.lower().strip()
        predicted = predicted.lower().strip()

        # Á≤æÁ°ÆÂåπÈÖç
        if expected == predicted:
            return True

        # ÂåÖÂê´ÂåπÈÖç
        if expected in predicted or predicted in expected:
            return True

        # ÁºñËæëË∑ùÁ¶ªÂåπÈÖçÔºàÁÆÄÂåñÁâàÔºâ
        if len(expected) > 3 and len(predicted) > 3:
            # Â¶ÇÊûúÂâç3‰∏™Â≠óÁ¨¶Áõ∏ÂêåÔºåËÆ§‰∏∫ÂåπÈÖç
            if expected[:3] == predicted[:3]:
                return True

        return False

    def evaluate_modality_integration(self,
                                     executed_steps: List[Dict],
                                     answer: str) -> Dict[str, Any]:
        """
        ÊîπËøõÁöÑÂ§öÊ®°ÊÄÅÊï¥ÂêàËØÑ‰º∞
        """
        # 1. ‰ªéÊâßË°åÊ≠•È™§‰∏≠ÊèêÂèñÊ®°ÊÄÅ
        modalities_from_steps = set()
        for step in executed_steps:
            modality = step.get('modality')
            if modality:
                modalities_from_steps.add(modality)

        # üîß FIX: ‰ªéÁ≠îÊ°àÊñáÊú¨‰∏≠Êé®Êñ≠‰ΩøÁî®ÁöÑÊ®°ÊÄÅ
        answer_lower = answer.lower()
        modalities_from_answer = set()

        # ÂàÜÂ≠êÊ®°ÊÄÅÂÖ≥ÈîÆËØç
        molecular_keywords = ['gene', 'marker', 'express', 'cluster', 'subclass',
                            'cell type', 'pvalb', 'sst', 'vip', 'gad']
        if any(kw in answer_lower for kw in molecular_keywords):
            modalities_from_answer.add('molecular')

        # ÂΩ¢ÊÄÅÊ®°ÊÄÅÂÖ≥ÈîÆËØç
        morpho_keywords = ['axon', 'dendrite', 'morpholog', 'branch', 'length',
                          'arbor', 'spine', 'soma']
        if any(kw in answer_lower for kw in morpho_keywords):
            modalities_from_answer.add('morphological')

        # ÊäïÂ∞ÑÊ®°ÊÄÅÂÖ≥ÈîÆËØç
        projection_keywords = ['project', 'target', 'connect', 'pathway',
                              'circuit', 'afferent', 'efferent']
        if any(kw in answer_lower for kw in projection_keywords):
            modalities_from_answer.add('projection')

        # ÂêàÂπ∂ÊâÄÊúâÊ®°ÊÄÅ
        all_modalities = modalities_from_steps | modalities_from_answer

        logger.info(f"    üé® Modality detection:")
        logger.info(f"       From steps: {modalities_from_steps}")
        logger.info(f"       From answer: {modalities_from_answer}")
        logger.info(f"       Total: {all_modalities}")

        # ËÆ°ÁÆóË¶ÜÁõñÁéá
        available_modalities = {'molecular', 'morphological', 'projection'}
        coverage = len(all_modalities) / len(available_modalities)

        # Ë∑®Ê®°ÊÄÅÂºïÁî®
        integration_patterns = {
            'molecular-morphological': r'(gene|marker).{0,50}(axon|dendrite|morpholog)',
            'molecular-projection': r'(gene|marker).{0,50}(project|target|connect)',
            'morphological-projection': r'(axon|dendrite).{0,50}(project|target)',
        }

        cross_modal_citations = 0
        for pattern in integration_patterns.values():
            if re.search(pattern, answer_lower):
                cross_modal_citations += 1

        # ËøûË¥ØÊÄßËØÑÂàÜ
        if len(all_modalities) >= 2:
            coherence = min(1.0, (cross_modal_citations + 1) / 2.0)
        else:
            coherence = 0.5 if len(all_modalities) == 1 else 0.0

        return {
            'modality_coverage': coverage,
            'modality_coherence': coherence,
            'cross_modal_citations': cross_modal_citations,
            'modalities_used': list(all_modalities)
        }

    def evaluate_reasoning_quality(self,
                                   executed_steps: List[Dict],
                                   schema_paths_used: List[Dict]) -> Dict[str, float]:
        """ËØÑ‰º∞Êé®ÁêÜË¥®Èáè"""
        if not executed_steps:
            return {
                'reasoning_coherence': 0.0,
                'schema_path_validity': 0.0,
                'reasoning_steps_count': 0
            }

        steps_count = len(executed_steps)

        # Êé®ÁêÜËøûË¥ØÊÄß
        has_dependencies = sum(1 for s in executed_steps if s.get('depends_on'))
        coherence = has_dependencies / steps_count if steps_count > 0 else 0.0

        # SchemaË∑ØÂæÑÊúâÊïàÊÄß
        if schema_paths_used:
            valid_paths = sum(1 for p in schema_paths_used if p.get('score', 0) > 0.5)
            validity = valid_paths / len(schema_paths_used)
        else:
            validity = 0.0

        return {
            'reasoning_coherence': coherence,
            'schema_path_validity': validity,
            'reasoning_steps_count': steps_count
        }

    def evaluate_scientific_accuracy(self,
                                     answer: str,
                                     executed_steps: List[Dict],
                                     ground_truth: Optional[Dict] = None) -> Dict[str, float]:
        """ËØÑ‰º∞ÁßëÂ≠¶ÂáÜÁ°ÆÊÄß"""
        answer_lower = answer.lower()

        # 1. ‰∫ãÂÆûÂáÜÁ°ÆÊÄß
        has_specific_data = bool(re.search(r'\d+', answer))
        has_region_names = bool(re.search(r'\b[A-Z]{2,5}\b', answer))
        has_scientific_terms = any(term in answer_lower for term in
                                   ['neuron', 'cell', 'region', 'cortex', 'gene'])

        factual_accuracy = (has_specific_data + has_region_names + has_scientific_terms) / 3.0

        # 2. ÂÆöÈáèÂáÜÁ°ÆÊÄß
        quant_keywords = ['mean', 'average', 'std', 'percentage', '%', 'count', 'number']
        has_quant = sum(1 for kw in quant_keywords if kw in answer_lower)
        quantitative_accuracy = min(1.0, has_quant / 2.0)

        # 3. ÂºïÁî®Ë¥®Èáè
        citation_quality = min(1.0, len(executed_steps) / 3.0)

        return {
            'factual_accuracy': factual_accuracy,
            'quantitative_accuracy': quantitative_accuracy,
            'citation_quality': citation_quality
        }

    def evaluate_answer_quality(self, answer: str, question: str) -> Dict[str, float]:
        """ËØÑ‰º∞Á≠îÊ°àË¥®Èáè"""
        answer_lower = answer.lower()

        # 1. ÂÆåÊï¥ÊÄß
        answer_words = len(answer.split())
        question_words = len(question.split())

        expected_length = 100 if question_words < 10 else 200
        completeness = min(1.0, answer_words / expected_length)

        # 2. ÂÖ∑‰ΩìÊÄß
        vague_terms = ['some', 'several', 'many', 'few', 'various']
        vague_count = sum(1 for term in vague_terms if term in answer_lower)
        specificity = max(0.0, 1.0 - vague_count / 5.0)

        # 3. ÁßëÂ≠¶‰∏•Ë∞®ÊÄß
        scientific_terms = ['neuron', 'cortex', 'expression', 'projection',
                          'morphology', 'cluster', 'marker', 'region']
        sci_count = sum(1 for term in scientific_terms if term in answer_lower)
        scientific_rigor = min(1.0, sci_count / 3.0)  # Èôç‰ΩéÈòàÂÄº

        return {
            'answer_completeness': completeness,
            'answer_specificity': specificity,
            'scientific_rigor': scientific_rigor
        }

    def evaluate_full(self,
                     question: str,
                     answer: str,
                     agent_output: Dict,
                     expected_entities: List[str],
                     ground_truth: Optional[Dict] = None) -> DomainSpecificMetrics:
        """ÂÆåÊï¥ËØÑ‰º∞"""

        logger.info(f"    üìä Evaluating: {question}")

        # 1. ÂÆû‰ΩìËØÜÂà´
        entity_metrics = self.evaluate_entity_recognition(
            agent_output.get('entities_recognized', []),
            expected_entities,
            answer,
            question
        )

        # 2. Â§öÊ®°ÊÄÅÊï¥Âêà
        modality_metrics = self.evaluate_modality_integration(
            agent_output.get('executed_steps', []),
            answer
        )

        # 3. Êé®ÁêÜË¥®Èáè
        reasoning_metrics = self.evaluate_reasoning_quality(
            agent_output.get('executed_steps', []),
            agent_output.get('schema_paths_used', [])
        )

        # 4. ÁßëÂ≠¶ÂáÜÁ°ÆÊÄß
        accuracy_metrics = self.evaluate_scientific_accuracy(
            answer,
            agent_output.get('executed_steps', []),
            ground_truth
        )

        # 5. Á≠îÊ°àË¥®Èáè
        quality_metrics = self.evaluate_answer_quality(answer, question)

        # 6. ÊïàÁéá
        execution_time = agent_output.get('execution_time', 0.0)
        api_calls = len(agent_output.get('executed_steps', []))
        token_usage = 0

        return DomainSpecificMetrics(
            **entity_metrics,
            **modality_metrics,
            **reasoning_metrics,
            **accuracy_metrics,
            **quality_metrics,
            execution_time=execution_time,
            api_calls=api_calls,
            token_usage=token_usage,
            entities_found=[e.get('text', '') for e in agent_output.get('entities_recognized', [])]
        )


# ==================== ‰øÆÂ§çÁöÑBaselineÂÆûÁé∞ ====================

class BaselineAgent:
    """BaselineÊñπÊ≥ïÁöÑÊäΩË±°Âü∫Á±ª"""

    def __init__(self, name: str):
        self.name = name

    def answer(self, question: str) -> Dict[str, Any]:
        raise NotImplementedError


class DirectLLMBaseline(BaselineAgent):
    """Baseline 1: Direct LLM"""

    def __init__(self, openai_client, model="gpt-4o"):
        super().__init__("Direct LLM")
        self.client = openai_client
        self.model = model

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        prompt = f"""You are a neuroscience expert. Answer the following question based on your knowledge.

Question: {question}

Provide a comprehensive, scientific answer."""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a neuroscience expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

            return {
                'question': question,
                'answer': answer,
                'entities_recognized': [],
                'executed_steps': [],
                'schema_paths_used': [],
                'execution_time': time.time() - start_time,
                'total_steps': 0,
                'confidence_score': 0.5,
                'success': True
            }

        except Exception as e:
            logger.error(f"Direct LLM failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time,
                'entities_recognized': [],
                'executed_steps': []
            }


class FixedRAGBaseline(BaselineAgent):
    """
    ‰øÆÂ§çÁöÑRAG Baseline

    Fix: CypherÂèÇÊï∞ÈóÆÈ¢ò
    """

    def __init__(self, neo4j_exec, openai_client, model="gpt-4o"):
        super().__init__("RAG")
        self.db = neo4j_exec
        self.client = openai_client
        self.model = model

    def retrieve_relevant_docs(self, question: str, top_k: int = 5) -> List[str]:
        """Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊ°£"""
        # ÊèêÂèñÂÖ≥ÈîÆËØç
        words = re.findall(r'\b[A-Z][a-z]+\b|\b[A-Z]{2,5}\b', question)

        docs = []

        for word in words[:3]:
            # üîß FIX: Á°Æ‰øùÂèÇÊï∞Ë¢´Ê≠£Á°Æ‰º†ÈÄí
            query = """
            MATCH (n)
            WHERE n.name CONTAINS $keyword OR n.acronym CONTAINS $keyword
            RETURN n
            LIMIT 5
            """

            try:
                result = self.db.run(query, {'keyword': word})

                if result.get('success') and result.get('data'):
                    for row in result['data']:
                        node = row['n']
                        doc = f"Node: {node.get('name', 'N/A')}, Properties: {str(node)[:200]}"
                        docs.append(doc)
            except Exception as e:
                logger.warning(f"RAG query failed for keyword '{word}': {e}")
                continue

        return docs[:top_k]

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        # 1. Ê£ÄÁ¥¢
        docs = self.retrieve_relevant_docs(question)

        # 2. ÊûÑÂª∫prompt
        if docs:
            context = "\n\n".join([f"Document {i+1}:\n{doc}" for i, doc in enumerate(docs)])
        else:
            context = "No relevant documents found."

        prompt = f"""Based on the following documents from a neuroscience knowledge graph, answer the question.

Documents:
{context}

Question: {question}

Answer:"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a neuroscience expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            answer = response.choices[0].message.content

            return {
                'question': question,
                'answer': answer,
                'entities_recognized': [],
                'executed_steps': [{'purpose': f'Retrieved {len(docs)} documents'}],
                'schema_paths_used': [],
                'execution_time': time.time() - start_time,
                'total_steps': 1,
                'confidence_score': 0.6,
                'success': True
            }

        except Exception as e:
            logger.error(f"RAG failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time,
                'entities_recognized': [],
                'executed_steps': []
            }


class ReActBaseline(BaselineAgent):
    """Baseline 3: ReAct"""

    def __init__(self, neo4j_exec, openai_client, model="gpt-4o"):
        super().__init__("ReAct")
        self.db = neo4j_exec
        self.client = openai_client
        self.model = model
        self.max_iterations = 3

    def answer(self, question: str) -> Dict[str, Any]:
        start_time = time.time()

        history = []
        executed_steps = []

        system_prompt = """You are a neuroscience expert with access to a knowledge graph database.

You can execute Cypher queries. Use ReAct framework:
1. Thought: Reason about what you need
2. Action: "cypher_query" or "answer"
3. Query: Cypher query (if action is cypher_query)

Respond in JSON:
{
  "thought": "your reasoning",
  "action": "cypher_query" or "answer",
  "query": "MATCH ... RETURN ..." or null,
  "final_answer": "answer text" or null
}"""

        try:
            for iteration in range(self.max_iterations):
                context = "\n\n".join(history) if history else "Start your reasoning."

                prompt = f"""Question: {question}

{context}

What's your next step?"""

                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.3,
                    max_tokens=500
                )

                result = json.loads(response.choices[0].message.content)

                thought = result.get('thought', '')
                action = result.get('action', '')

                history.append(f"Thought: {thought}")

                if action == 'answer':
                    final_answer = result.get('final_answer', '')

                    return {
                        'question': question,
                        'answer': final_answer,
                        'entities_recognized': [],
                        'executed_steps': executed_steps,
                        'schema_paths_used': [],
                        'execution_time': time.time() - start_time,
                        'total_steps': len(executed_steps),
                        'confidence_score': 0.7,
                        'success': True
                    }

                elif action == 'cypher_query':
                    query = result.get('query', '')

                    if query:
                        db_result = self.db.run(query)

                        if db_result.get('success'):
                            data = db_result.get('data', [])[:10]
                            observation = f"Query returned {len(data)} results"
                        else:
                            observation = f"Query failed: {db_result.get('error')}"

                        history.append(f"Action: {query}")
                        history.append(f"Observation: {observation}")

                        executed_steps.append({
                            'purpose': thought,
                            'query': query,
                            'result_count': len(data) if db_result.get('success') else 0
                        })

            return {
                'question': question,
                'answer': "Unable to complete within iteration limit.",
                'entities_recognized': [],
                'executed_steps': executed_steps,
                'execution_time': time.time() - start_time,
                'success': False
            }

        except Exception as e:
            logger.error(f"ReAct failed: {e}")
            return {
                'question': question,
                'answer': f"Error: {str(e)}",
                'success': False,
                'execution_time': time.time() - start_time,
                'entities_recognized': [],
                'executed_steps': []
            }


# ==================== ÁªüËÆ°ÂàÜÊûê ====================

class StatisticalAnalyzer:
    """ÁªüËÆ°ÊòæËëóÊÄßÂàÜÊûê"""

    @staticmethod
    def compare_methods(method_a_scores: List[float],
                       method_b_scores: List[float],
                       method_a_name: str = "Method A",
                       method_b_name: str = "Method B") -> Dict:
        """ÊØîËæÉ‰∏§‰∏™ÊñπÊ≥ï"""

        # T-test
        t_stat, p_value = stats.ttest_ind(method_a_scores, method_b_scores)

        # Effect size
        mean_a = np.mean(method_a_scores)
        mean_b = np.mean(method_b_scores)
        std_a = np.std(method_a_scores, ddof=1)
        std_b = np.std(method_b_scores, ddof=1)

        pooled_std = np.sqrt((std_a**2 + std_b**2) / 2)
        cohens_d = (mean_a - mean_b) / pooled_std if pooled_std > 0 else 0.0

        # 95% CI
        se = np.sqrt(std_a**2/len(method_a_scores) + std_b**2/len(method_b_scores))
        ci_lower = (mean_a - mean_b) - 1.96 * se
        ci_upper = (mean_a - mean_b) + 1.96 * se

        return {
            'method_a': method_a_name,
            'method_b': method_b_name,
            'mean_a': mean_a,
            'mean_b': mean_b,
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'cohens_d': cohens_d,
            'effect_size': StatisticalAnalyzer._interpret_effect_size(cohens_d),
            'ci_95': (ci_lower, ci_upper)
        }

    @staticmethod
    def _interpret_effect_size(d: float) -> str:
        abs_d = abs(d)
        if abs_d < 0.2:
            return "negligible"
        elif abs_d < 0.5:
            return "small"
        elif abs_d < 0.8:
            return "medium"
        else:
            return "large"


# ==================== ÊîπËøõÁöÑBenchmark Runner ====================

class ImprovedNatureMethodsBenchmark:
    """
    ÊîπËøõÁöÑNature Methods Benchmark

    ‰øÆÂ§çÔºö
    1. Êõ¥Â•ΩÁöÑËØÑ‰º∞Âô®
    2. ‰øÆÂ§çÁöÑbaselines
    3. Â¢ûÂº∫ÁöÑË∞ÉËØï‰ø°ÊÅØ
    """

    def __init__(self,
                 aipom_agent,
                 neo4j_exec,
                 openai_client,
                 schema_cache,
                 output_dir: str = "./benchmark_nm_fixed"):

        self.aipom = aipom_agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # ‰ΩøÁî®ÊîπËøõÁöÑËØÑ‰º∞Âô®
        self.evaluator = ImprovedDomainEvaluator(schema_cache)

        # ‰ΩøÁî®‰øÆÂ§çÁöÑbaselines
        self.baselines = {
            'Direct LLM': DirectLLMBaseline(openai_client),
            'RAG': FixedRAGBaseline(neo4j_exec, openai_client),
            'ReAct': ReActBaseline(neo4j_exec, openai_client)
        }

        self.results = defaultdict(list)

    def run_full_benchmark(self, questions: List[Dict], max_questions: Optional[int] = None):
        """ËøêË°åÂÆåÊï¥benchmark"""

        if max_questions:
            questions = questions[:max_questions]

        logger.info(f"üöÄ Running Improved Benchmark on {len(questions)} questions")
        logger.info(f"   Methods: AIPOM-CoT + {len(self.baselines)} baselines\n")

        for q_idx, question in enumerate(tqdm(questions, desc="Testing")):
            logger.info(f"\n{'='*80}")
            logger.info(f"Question {q_idx+1}/{len(questions)}: {question['question']}")
            logger.info('='*80)

            # 1. AIPOM-CoT
            logger.info("\n[1/4] Running AIPOM-CoT...")
            aipom_result = self._run_and_evaluate(
                'AIPOM-CoT',
                lambda q: self.aipom.answer(q, max_iterations=10),
                question
            )
            self.results['AIPOM-CoT'].append(aipom_result)

            # 2. Baselines
            for idx, (name, baseline) in enumerate(self.baselines.items(), start=2):
                logger.info(f"\n[{idx}/4] Running {name}...")
                baseline_result = self._run_and_evaluate(
                    name,
                    baseline.answer,
                    question
                )
                self.results[name].append(baseline_result)

            # ‰øùÂ≠ò‰∏≠Èó¥ÁªìÊûú
            if (q_idx + 1) % 5 == 0:
                self._save_intermediate_results()

        # ÊúÄÁªàÂàÜÊûê
        self._save_final_results()
        self._generate_statistical_analysis()
        self._generate_visualization()

        logger.info(f"\n‚úÖ Benchmark complete! Results in {self.output_dir}")

    def _run_and_evaluate(self, method_name: str, answer_fn, question: Dict) -> Dict:
        """ËøêË°åÂπ∂ËØÑ‰º∞Âçï‰∏™ÊñπÊ≥ï"""
        try:
            # ËøêË°å
            agent_output = answer_fn(question['question'])

            if not agent_output.get('success', True):
                logger.warning(f"  ‚ö†Ô∏è {method_name} failed")
                return self._create_failed_result(method_name, question, agent_output)

            # ËØÑ‰º∞
            metrics = self.evaluator.evaluate_full(
                question['question'],
                agent_output.get('answer', ''),
                agent_output,
                question.get('expected_entities', [])
            )

            result = {
                'method': method_name,
                'question_id': question['id'],
                'question': question['question'],
                'complexity': question['complexity'],
                'domain': question['domain'],
                'answer': agent_output.get('answer', ''),
                'metrics': metrics,
                'success': True
            }

            # ÊâìÂç∞ÂÖ≥ÈîÆÊåáÊ†á
            logger.info(f"  ‚úì {method_name}:")
            logger.info(f"    Entity F1: {metrics.entity_f1:.3f}")
            logger.info(f"    Modality Coverage: {metrics.modality_coverage:.3f}")
            logger.info(f"    Scientific Rigor: {metrics.scientific_rigor:.3f}")
            logger.info(f"    Time: {metrics.execution_time:.2f}s")

            return result

        except Exception as e:
            logger.error(f"  ‚úó {method_name} crashed: {e}")
            import traceback
            traceback.print_exc()
            return self._create_failed_result(method_name, question, {'error': str(e)})

    def _create_failed_result(self, method_name: str, question: Dict, output: Dict) -> Dict:
        """ÂàõÂª∫Â§±Ë¥•ÁªìÊûú"""
        return {
            'method': method_name,
            'question_id': question['id'],
            'question': question['question'],
            'complexity': question['complexity'],
            'domain': question['domain'],
            'answer': output.get('answer', 'ERROR'),
            'metrics': DomainSpecificMetrics(
                entity_precision=0, entity_recall=0, entity_f1=0,
                modality_coverage=0, modality_coherence=0, cross_modal_citations=0,
                reasoning_steps_count=0, reasoning_coherence=0, schema_path_validity=0,
                factual_accuracy=0, quantitative_accuracy=0, citation_quality=0,
                answer_completeness=0, answer_specificity=0, scientific_rigor=0,
                execution_time=0, api_calls=0, token_usage=0
            ),
            'success': False,
            'error': output.get('error', 'Unknown error')
        }

    def _save_intermediate_results(self):
        """‰øùÂ≠ò‰∏≠Èó¥ÁªìÊûú"""
        filepath = self.output_dir / "intermediate_results.json"

        serializable = {}
        for method, results in self.results.items():
            serializable[method] = [
                {
                    **r,
                    'metrics': r['metrics'].__dict__ if hasattr(r['metrics'], '__dict__') else r['metrics']
                }
                for r in results
            ]

        with open(filepath, 'w') as f:
            json.dump(serializable, f, indent=2, default=str)

    def _save_final_results(self):
        """‰øùÂ≠òÊúÄÁªàÁªìÊûú"""
        filepath = self.output_dir / "final_results.json"
        self._save_intermediate_results()
        logger.info(f"‚úÖ Results saved to {filepath}")

    def _generate_statistical_analysis(self):
        """ÁîüÊàêÁªüËÆ°ÂàÜÊûê"""
        logger.info("\n" + "="*80)
        logger.info("STATISTICAL ANALYSIS")
        logger.info("="*80)

        # ÊèêÂèñF1ÂàÜÊï∞
        f1_scores = {}
        for method, results in self.results.items():
            f1_scores[method] = [
                r['metrics'].entity_f1
                for r in results
                if r['success']
            ]

        # ÂØπÊØîAIPOM-CoT vs baselines
        comparisons = []
        aipom_scores = f1_scores.get('AIPOM-CoT', [])

        for method, scores in f1_scores.items():
            if method != 'AIPOM-CoT' and len(scores) > 0 and len(aipom_scores) > 0:
                comp = StatisticalAnalyzer.compare_methods(
                    aipom_scores, scores, 'AIPOM-CoT', method
                )
                comparisons.append(comp)

        # ‰øùÂ≠ò
        comparison_df = pd.DataFrame(comparisons)
        comparison_df.to_csv(self.output_dir / "statistical_comparison.csv", index=False)

        print("\n" + comparison_df.to_string())
        print("\n‚úÖ Statistical analysis saved")

    def _generate_visualization(self):
        """ÁîüÊàêÂèØËßÜÂåñ"""
        logger.info("\n" + "="*80)
        logger.info("GENERATING VISUALIZATIONS")
        logger.info("="*80)

        methods = list(self.results.keys())

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # (A) Entity F1 scores
        ax1 = axes[0, 0]
        f1_data = []
        for method in methods:
            scores = [r['metrics'].entity_f1 for r in self.results[method] if r['success']]
            f1_data.append(scores)

        bp1 = ax1.boxplot(f1_data, labels=methods, patch_artist=True)
        for i, patch in enumerate(bp1['boxes']):
            patch.set_facecolor('#2ecc71' if methods[i] == 'AIPOM-CoT' else '#95a5a6')
        ax1.set_ylabel('Entity F1 Score', fontweight='bold')
        ax1.set_title('(A) Entity Recognition Performance', fontweight='bold')
        ax1.grid(axis='y', alpha=0.3)

        # (B) Modality Coverage
        ax2 = axes[0, 1]
        coverage_means = []
        for method in methods:
            scores = [r['metrics'].modality_coverage for r in self.results[method] if r['success']]
            coverage_means.append(np.mean(scores) if scores else 0)

        colors = ['#2ecc71' if m == 'AIPOM-CoT' else '#95a5a6' for m in methods]
        bars = ax2.bar(methods, coverage_means, color=colors, alpha=0.8)
        ax2.set_ylabel('Modality Coverage', fontweight='bold')
        ax2.set_title('(B) Multi-Modal Integration', fontweight='bold')
        ax2.set_ylim(0, 1)
        ax2.grid(axis='y', alpha=0.3)

        # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{height:.3f}', ha='center', va='bottom')

        # (C) Scientific Rigor
        ax3 = axes[1, 0]
        rigor_means = []
        for method in methods:
            scores = [r['metrics'].scientific_rigor for r in self.results[method] if r['success']]
            rigor_means.append(np.mean(scores) if scores else 0)

        bars = ax3.bar(methods, rigor_means, color=colors, alpha=0.8)
        ax3.set_ylabel('Scientific Rigor Score', fontweight='bold')
        ax3.set_title('(C) Scientific Quality', fontweight='bold')
        ax3.set_ylim(0, 1)
        ax3.grid(axis='y', alpha=0.3)

        for bar in bars:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{height:.3f}', ha='center', va='bottom')

        # (D) Execution Time
        ax4 = axes[1, 1]
        time_data = []
        for method in methods:
            times = [r['metrics'].execution_time for r in self.results[method] if r['success']]
            time_data.append(times)

        bp4 = ax4.boxplot(time_data, labels=methods, patch_artist=True)
        for i, patch in enumerate(bp4['boxes']):
            patch.set_facecolor('#3498db' if methods[i] == 'AIPOM-CoT' else '#95a5a6')
        ax4.set_ylabel('Execution Time (s)', fontweight='bold')
        ax4.set_title('(D) Efficiency', fontweight='bold')
        ax4.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "benchmark_comparison.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.output_dir / "benchmark_comparison.pdf", bbox_inches='tight')
        plt.close()

        logger.info("‚úÖ Visualizations saved")


# ==================== ‰∏ªÂáΩÊï∞ ====================

def run_improved_benchmark():
    """ËøêË°åÊîπËøõÁöÑbenchmark"""
    import os
    from benchmark_system import BenchmarkQuestionBank

    # Âä†ËΩΩÈóÆÈ¢ò
    questions_file = "test_questions.json"
    if not Path(questions_file).exists():
        logger.info("Generating test questions...")
        questions = BenchmarkQuestionBank.generate_questions()
        BenchmarkQuestionBank.save_to_json(questions, questions_file)

    questions = BenchmarkQuestionBank.load_from_json(questions_file)

    # ËΩ¨Êç¢Ê†ºÂºè
    questions_dict = [
        {
            'id': q.id,
            'question': q.question,
            'complexity': q.complexity.value,
            'domain': q.domain,
            'expected_entities': q.expected_entities
        }
        for q in questions
    ]

    # ÂàùÂßãÂåñÁ≥ªÁªü
    from aipom_v10_production import AIPOMCoTV10
    from neo4j_exec import Neo4jExec
    from aipom_cot_true_agent_v2 import RealSchemaCache
    from openai import OpenAI

    neo4j_exec = Neo4jExec(
        os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        os.getenv("NEO4J_USER", "neo4j"),
        os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j")
    )

    schema_cache = RealSchemaCache("./schema_output/schema.json")

    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ''))

    aipom_agent = AIPOMCoTV10(
        neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        neo4j_user=os.getenv("NEO4J_USER", 'neo4j'),
        neo4j_pwd=os.getenv("NEO4J_PASSWORD", 'neuroxiv'),
        database=os.getenv("NEO4J_DATABASE", 'neo4j'),
        schema_json_path="./schema_output/schema.json",
        openai_api_key=os.getenv("OPENAI_API_KEY", ''),
        model="gpt-4o"
    )

    # ËøêË°åbenchmark
    benchmark = ImprovedNatureMethodsBenchmark(
        aipom_agent,
        neo4j_exec,
        openai_client,
        schema_cache,
        output_dir="./benchmark_nature_methods_fixed"
    )

    benchmark.run_full_benchmark(questions_dict, max_questions=10)

    logger.info("\n‚úÖ Improved Benchmark Complete!")


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    run_improved_benchmark()