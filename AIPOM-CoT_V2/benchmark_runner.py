"""
Benchmark Runner for AIPOM-CoT Evaluation
==========================================
ä¸»æµ‹è¯•è¿è¡Œå™¨ï¼Œåè°ƒæ‰€æœ‰ç»„ä»¶

Author: Claude & PrometheusTT
Date: 2025-01-15
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional
from tqdm import tqdm
from collections import defaultdict

from test_questions import ALL_QUESTIONS, QuestionTier, TestQuestion
from baselines import (
    DirectGPT5Baseline,    # ğŸ†• æ–°å¢
    TemplateKGBaseline, # ğŸ†• æ–°å¢
    RAGBaseline,
    ReActBaseline,
)
from evaluators import ComprehensiveEvaluator, EvaluationMetrics

logger = logging.getLogger(__name__)


# ==================== Result Structures ====================

class BenchmarkResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""

    def __init__(self,
                 question: TestQuestion,
                 method_name: str,
                 agent_output: Dict,
                 metrics: EvaluationMetrics):
        self.question = question
        self.method_name = method_name
        self.agent_output = agent_output
        self.metrics = metrics

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
        return {
            'question_id': self.question.id,
            'tier': self.question.tier.value,
            'question': self.question.question,
            'method': self.method_name,
            'answer': self.agent_output.get('answer', ''),
            'success': self.agent_output.get('success', False),
            'metrics': {
                # D1
                'depth_matching': self.metrics.depth_matching_accuracy,
                'plan_coherence': self.metrics.plan_coherence,
                'modality_coverage': self.metrics.modality_coverage,
                'strategy_selection': self.metrics.strategy_selection_accuracy,
                'closed_loop': 1.0 if self.metrics.closed_loop_achieved else 0.0,

                # D2
                'entity_precision': self.metrics.entity_precision,
                'entity_recall': self.metrics.entity_recall,
                'entity_f1': self.metrics.entity_f1,
                'multi_hop_depth': self.metrics.multi_hop_depth,

                # D4
                'modalities_used': self.metrics.modalities_used,

                # D5
                'factual_accuracy': self.metrics.factual_accuracy,
                'answer_completeness': self.metrics.answer_completeness,
                'scientific_rigor': self.metrics.scientific_rigor,

                # D6
                'execution_time': self.metrics.execution_time,
                'api_calls': self.metrics.api_calls,
                'query_success_rate': self.metrics.query_success_rate,
            }
        }


# ==================== Benchmark Runner ====================

class BenchmarkRunner:
    """
    ä¸»Benchmarkè¿è¡Œå™¨

    åè°ƒï¼š
    - AIPOM-CoT agent
    - 3ä¸ªbaselineæ–¹æ³•
    - è¯„ä¼°å™¨
    - ç»“æœä¿å­˜
    """

    def __init__(self,
                 aipom_agent,
                 neo4j_exec,
                 openai_client,
                 output_dir: str = "./benchmark_results"):

        self.aipom_agent = aipom_agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # ğŸ”§ æ›´æ–°ï¼šä½¿ç”¨GPT-5çš„baselines
        logger.info("Initializing baseline methods (with GPT-5)...")
        self.baselines = {
            'Direct GPT-5': DirectGPT5Baseline(openai_client),  # ğŸ†•
            'Template-KG': TemplateKGBaseline(neo4j_exec, openai_client),
            'RAG': RAGBaseline(neo4j_exec, openai_client),
            'ReAct': ReActBaseline(neo4j_exec, openai_client, max_iterations=5),
        }

        # åˆå§‹åŒ–evaluator
        self.evaluator = ComprehensiveEvaluator()

        # ç»“æœå­˜å‚¨
        self.results = defaultdict(list)

        logger.info("âœ… BenchmarkRunner initialized (v2.1, GPT-5)")

    def run_full_benchmark(self,
                           questions: Optional[List[TestQuestion]] = None,
                           methods: Optional[List[str]] = None,
                           max_questions: Optional[int] = None,
                           save_interval: int = 10) -> Dict:
        """è¿è¡Œå®Œæ•´benchmarkï¼ˆæ›´æ–°ç‰ˆï¼‰"""

        # å‡†å¤‡é—®é¢˜
        if questions is None:
            questions = ALL_QUESTIONS

        if max_questions:
            questions = questions[:max_questions]

        # ğŸ”§ æ›´æ–°ï¼šå‡†å¤‡æ–¹æ³•ï¼ˆæ–°çš„é»˜è®¤åˆ—è¡¨ï¼‰
        if methods is None:
            methods = ['AIPOM-CoT', 'o1-preview', 'Template-KG', 'RAG', 'ReAct']

        logger.info(f"\n{'=' * 80}")
        logger.info(f"ğŸš€ Starting Benchmark (v2.0)")
        logger.info(f"{'=' * 80}")
        logger.info(f"Questions: {len(questions)}")
        logger.info(f"Methods: {methods}")
        logger.info(f"Output: {self.output_dir}")
        logger.info(f"{'=' * 80}\n")

        # è¿è¡Œæµ‹è¯•
        total_tests = len(questions) * len(methods)

        with tqdm(total=total_tests, desc="Benchmark Progress") as pbar:
            for q_idx, question in enumerate(questions, 1):
                logger.info(f"\n{'=' * 80}")
                logger.info(f"Question {q_idx}/{len(questions)}: {question.id}")
                logger.info(
                    f"Complexity: {question.complexity_level.value if hasattr(question, 'complexity_level') else 'N/A'}")
                logger.info(f"Q: {question.question[:80]}...")
                logger.info(f"{'=' * 80}")

                for method_name in methods:
                    logger.info(f"\n[{method_name}] Testing...")

                    result = self._run_single_test(question, method_name)
                    self.results[method_name].append(result)

                    # æ‰“å°å…³é”®æŒ‡æ ‡
                    metrics = result.metrics
                    logger.info(f"  âœ“ Entity F1: {metrics.entity_f1:.3f}")
                    logger.info(f"  âœ“ Depth Match: {metrics.depth_matching_accuracy:.3f}")
                    logger.info(f"  âœ“ Closed Loop: {'Yes' if metrics.closed_loop_achieved else 'No'}")

                    # ğŸ†• æ‰“å°task completionï¼ˆå¦‚æœæœ‰ï¼‰
                    if hasattr(metrics, 'task_completion') and metrics.task_completion != 'unknown':
                        logger.info(f"  âœ“ Task: {metrics.task_completion}")

                    logger.info(f"  âœ“ Time: {metrics.execution_time:.2f}s")

                    pbar.update(1)

                # å®šæœŸä¿å­˜
                if q_idx % save_interval == 0:
                    self._save_intermediate_results()
                    logger.info(f"\nğŸ’¾ Intermediate results saved at Q{q_idx}")

        # æœ€ç»ˆä¿å­˜
        logger.info(f"\n{'=' * 80}")
        logger.info("ğŸ“Š Generating final report...")
        logger.info(f"{'=' * 80}")

        self._save_final_results()
        summary = self._generate_summary()

        logger.info(f"\nâœ… Benchmark Complete!")
        logger.info(f"Results saved to: {self.output_dir}")

        return summary

    def _run_single_test(self,
                         question: TestQuestion,
                         method_name: str) -> BenchmarkResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""

        try:
            # è¿è¡Œagent/baseline
            if method_name == 'AIPOM-CoT':
                agent_output = self._run_aipom(question)
            else:
                agent_output = self._run_baseline(question, method_name)

            # è¯„ä¼°
            question_data = self._question_to_dict(question)
            metrics = self.evaluator.evaluate_full(
                question_data, agent_output, method_name
            )

            return BenchmarkResult(question, method_name, agent_output, metrics)

        except Exception as e:
            logger.error(f"  âœ— {method_name} failed: {e}")
            import traceback
            traceback.print_exc()

            # è¿”å›å¤±è´¥ç»“æœ
            return self._create_failed_result(question, method_name, str(e))

    def _run_aipom(self, question: TestQuestion) -> Dict:
        """è¿è¡ŒAIPOM-CoT"""

        # æ ¹æ®é—®é¢˜å¤æ‚åº¦è®¾ç½®max_iterations
        if question.tier == QuestionTier.SIMPLE:
            max_iter = 4
        elif question.tier == QuestionTier.MEDIUM:
            max_iter = 6
        elif question.tier == QuestionTier.DEEP:
            max_iter = 10
        else:  # SCREENING
            max_iter = 8

        result = self.aipom_agent.answer(
            question.question,
            max_iterations=max_iter
        )

        return result

    def _run_baseline(self, question: TestQuestion, method_name: str) -> Dict:
        """è¿è¡Œbaselineæ–¹æ³•"""

        baseline = self.baselines.get(method_name)
        if not baseline:
            raise ValueError(f"Unknown method: {method_name}")

        # è®¾ç½®timeout
        if question.tier == QuestionTier.SIMPLE:
            timeout = 30
        elif question.tier == QuestionTier.MEDIUM:
            timeout = 60
        else:
            timeout = 120

        result = baseline.answer(question.question, timeout=timeout)

        return result

    def _question_to_dict(self, question: TestQuestion) -> Dict:
        """è½¬æ¢TestQuestionä¸ºdict"""
        return {
            'id': question.id,
            'tier': question.tier.value,
            'question': question.question,
            'expected_entities': question.expected_entities,
            'expected_depth': question.expected_depth,
            'expected_strategy': question.expected_strategy,
            'expected_modalities': question.expected_modalities,
            'expected_closed_loop': question.expected_closed_loop,
            'expected_steps_range': question.expected_steps_range,
            'domain': question.domain,
            'difficulty_score': question.difficulty_score,
        }

    def _create_failed_result(self,
                              question: TestQuestion,
                              method_name: str,
                              error: str) -> BenchmarkResult:
        """åˆ›å»ºå¤±è´¥ç»“æœ"""

        failed_output = {
            'question': question.question,
            'answer': f"ERROR: {error}",
            'entities_recognized': [],
            'executed_steps': [],
            'schema_paths_used': [],
            'execution_time': 0.0,
            'total_steps': 0,
            'confidence_score': 0.0,
            'success': False,
            'method': method_name,
            'error': error,
        }

        # åˆ›å»ºé›¶æŒ‡æ ‡
        metrics = EvaluationMetrics()

        return BenchmarkResult(question, method_name, failed_output, metrics)

    def _save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        filepath = self.output_dir / "intermediate_results.json"

        data = {}
        for method, results in self.results.items():
            data[method] = [r.to_dict() for r in results]

        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)

    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""

        # 1. è¯¦ç»†ç»“æœ
        detailed_file = self.output_dir / "detailed_results.json"
        data = {}
        for method, results in self.results.items():
            data[method] = [r.to_dict() for r in results]

        with open(detailed_file, 'w') as f:
            json.dump(data, f, indent=2)

        logger.info(f"  âœ“ Detailed results: {detailed_file}")

        # 2. èšåˆç»Ÿè®¡
        summary = self._generate_summary()

        summary_file = self.output_dir / "summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)

        logger.info(f"  âœ“ Summary: {summary_file}")

    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆèšåˆç»Ÿè®¡"""

        summary = {}

        for method, results in self.results.items():
            if not results:
                continue

            # æå–æ‰€æœ‰æŒ‡æ ‡
            all_metrics = {
                'depth_matching': [],
                'entity_f1': [],
                'closed_loop': [],
                'modality_coverage': [],
                'factual_accuracy': [],
                'scientific_rigor': [],
                'execution_time': [],
            }

            for result in results:
                m = result.metrics
                all_metrics['depth_matching'].append(m.depth_matching_accuracy)
                all_metrics['entity_f1'].append(m.entity_f1)
                all_metrics['closed_loop'].append(1.0 if m.closed_loop_achieved else 0.0)
                all_metrics['modality_coverage'].append(m.modality_coverage)
                all_metrics['factual_accuracy'].append(m.factual_accuracy)
                all_metrics['scientific_rigor'].append(m.scientific_rigor)
                all_metrics['execution_time'].append(m.execution_time)

            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            import statistics

            summary[method] = {}
            for metric_name, values in all_metrics.items():
                if values:
                    summary[method][metric_name] = {
                        'mean': statistics.mean(values),
                        'std': statistics.stdev(values) if len(values) > 1 else 0.0,
                        'min': min(values),
                        'max': max(values),
                    }

            # æ€»ä½“åˆ†æ•°
            overall_scores = [
                all_metrics['depth_matching'],
                all_metrics['entity_f1'],
                all_metrics['modality_coverage'],
                all_metrics['scientific_rigor'],
            ]

            # è®¡ç®—æ¯ä¸ªé—®é¢˜çš„å¹³å‡åˆ†
            n = len(results)
            overall_per_question = []
            for i in range(n):
                scores = [overall_scores[j][i] for j in range(len(overall_scores))]
                overall_per_question.append(statistics.mean(scores))

            summary[method]['overall'] = {
                'mean': statistics.mean(overall_per_question),
                'std': statistics.stdev(overall_per_question) if len(overall_per_question) > 1 else 0.0,
            }

            # æŒ‰Tierç»Ÿè®¡
            by_tier = defaultdict(list)
            for result in results:
                tier = result.question.tier.value
                by_tier[tier].append(result)

            summary[method]['by_tier'] = {}
            for tier, tier_results in by_tier.items():
                tier_scores = []
                for result in tier_results:
                    m = result.metrics
                    score = statistics.mean([
                        m.depth_matching_accuracy,
                        m.entity_f1,
                        m.modality_coverage,
                        m.scientific_rigor,
                    ])
                    tier_scores.append(score)

                summary[method]['by_tier'][tier] = {
                    'mean': statistics.mean(tier_scores) if tier_scores else 0.0,
                    'count': len(tier_results),
                }

        return summary

    def print_summary(self):
        """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""

        summary = self._generate_summary()

        print("\n" + "=" * 80)
        print("ğŸ“Š BENCHMARK SUMMARY")
        print("=" * 80)

        for method in ['AIPOM-CoT', 'Direct LLM', 'RAG', 'ReAct']:
            if method not in summary:
                continue

            print(f"\n{method}:")
            print("-" * 40)

            overall = summary[method].get('overall', {})
            print(f"Overall Score: {overall.get('mean', 0):.3f} Â± {overall.get('std', 0):.3f}")

            print(f"\nKey Metrics:")
            for metric in ['entity_f1', 'depth_matching', 'closed_loop', 'scientific_rigor']:
                if metric in summary[method]:
                    m = summary[method][metric]
                    print(f"  {metric:20s}: {m['mean']:.3f} Â± {m['std']:.3f}")

            print(f"\nBy Tier:")
            by_tier = summary[method].get('by_tier', {})
            for tier in ['simple', 'medium', 'deep', 'screening']:
                if tier in by_tier:
                    t = by_tier[tier]
                    print(f"  {tier.capitalize():12s}: {t['mean']:.3f} (n={t['count']})")

        print("\n" + "=" * 80)


# ==================== Quick Test Function ====================

def run_quick_test(aipom_agent, neo4j_exec, openai_client, n_questions: int = 10):
    """è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆ10é¢˜ï¼‰"""

    from test_questions import TIER1_SIMPLE, TIER2_MEDIUM, TIER3_DEEP, TIER4_SCREENING

    # é€‰æ‹©ä»£è¡¨æ€§é—®é¢˜
    selected = []
    selected.extend(TIER1_SIMPLE[:2])  # 2ä¸ªç®€å•
    selected.extend(TIER2_MEDIUM[:3])  # 3ä¸ªä¸­ç­‰
    selected.extend(TIER3_DEEP[:3])  # 3ä¸ªæ·±åº¦
    selected.extend(TIER4_SCREENING[:2])  # 2ä¸ªç­›é€‰

    selected = selected[:n_questions]

    runner = BenchmarkRunner(
        aipom_agent,
        neo4j_exec,
        openai_client,
        output_dir="./benchmark_results_quick"
    )

    summary = runner.run_full_benchmark(
        questions=selected,
        methods=['AIPOM-CoT', 'Direct LLM', 'RAG', 'ReAct'],
        save_interval=5
    )

    runner.print_summary()

    return summary


# ==================== Test ====================

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    print("BenchmarkRunner loaded successfully!")