"""
Evaluation System for AIPOM-CoT Benchmark (v3.0 - Fair Evaluation)
===================================================================
åŒ…å«å®Œæ•´çš„è¯„ä¼°ä½“ç³»ï¼š
- åˆ†å±‚è¯„ä¼°ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ vs ç³»ç»ŸæŒ‡æ ‡ï¼‰
- æ–¹æ³•ç‰¹å®šæƒé‡
- ç”Ÿç‰©å­¦ä»»åŠ¡è¯„ä¼°
- å…¬å¹³çš„Overallåˆ†æ•°è®¡ç®—

Changes in v3.0 (å…¬å¹³æ€§ä¿®å¤):
- âœ… åˆ†å±‚è¯„ä¼°ï¼šåŒºåˆ†æ ¸å¿ƒèƒ½åŠ›å’Œç³»ç»Ÿèƒ½åŠ›
- âœ… æ–¹æ³•ç‰¹å®šæƒé‡ï¼šLLMå’ŒAgentä½¿ç”¨ä¸åŒè¯„ä¼°æ ‡å‡†
- âœ… None-ableæŒ‡æ ‡ï¼šä¸å¼ºåˆ¶æ‰€æœ‰æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šè¯„åˆ†
- âœ… ç”Ÿç‰©å­¦ä»»åŠ¡è¯„ä¼°ï¼šæ˜ç¡®çš„æˆåŠŸæ ‡å‡†

Author: Claude & PrometheusTT
Date: 2025-01-15
Version: 3.0
"""

import numpy as np
import logging
import re
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


# ==================== ğŸ”§ æ–°å¢ï¼šè¯„ä¼°é…ç½® ====================

EVALUATION_CONFIG = {
    # æ ¸å¿ƒæŒ‡æ ‡ï¼šæ‰€æœ‰æ–¹æ³•éƒ½å¿…é¡»è¯„ä¼°
    'core_metrics': {
        'entity_f1': {
            'weight': 1.0,
            'methods': 'all',
            'description': 'Accuracy of entity recognition'
        },
        'factual_accuracy': {
            'weight': 1.0,
            'methods': 'all',
            'description': 'Factual correctness of answer'
        },
        'answer_completeness': {
            'weight': 1.0,
            'methods': 'all',
            'description': 'Completeness of answer'
        },
        'scientific_rigor': {
            'weight': 1.0,
            'methods': 'all',
            'description': 'Scientific rigor and quantitative support'
        },
    },

    # ç³»ç»Ÿèƒ½åŠ›æŒ‡æ ‡ï¼šåªè¯„ä¼°æœ‰è¯¥èƒ½åŠ›çš„æ–¹æ³•
    'system_metrics': {
        'depth_matching': {
            'weight': 1.0,
            'methods': ['AIPOM-CoT', 'ReAct', 'Template-KG'],  # æœ‰planning/æ­¥éª¤çš„æ–¹æ³•
            'description': 'Adaptive depth matching'
        },
        'plan_coherence': {
            'weight': 1.0,
            'methods': ['AIPOM-CoT', 'ReAct'],  # åªæœ‰åŠ¨æ€planningçš„
            'description': 'Coherence of execution plan'
        },
        'closed_loop': {
            'weight': 1.0,
            'methods': ['AIPOM-CoT'],  # åªæœ‰AIPOMè®¾è®¡äº†é—­ç¯
            'description': 'Closed-loop circuit analysis'
        },
        'modality_coverage': {
            'weight': 1.0,
            'methods': ['AIPOM-CoT', 'Template-KG', 'RAG', 'ReAct'],  # æœ‰KGè®¿é—®çš„
            'description': 'Multi-modal data coverage'
        },
    },

    # ğŸ”§ æ–¹æ³•ç‰¹å®šæƒé‡ï¼ˆç”¨äºè®¡ç®—Overallåˆ†æ•°ï¼‰
    'method_weights': {
        'AIPOM-CoT': {
            # å…¨é¢è¯„ä¼°
            'entity_f1': 0.15,
            'factual_accuracy': 0.15,
            'answer_completeness': 0.12,
            'scientific_rigor': 0.13,
            'depth_matching': 0.15,
            'plan_coherence': 0.10,
            'closed_loop': 0.10,
            'modality_coverage': 0.10,
        },
        'Direct GPT-4o': {
            # é‡ç‚¹è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼ˆæ— planningæŒ‡æ ‡ï¼‰
            'entity_f1': 0.20,
            'factual_accuracy': 0.30,
            'answer_completeness': 0.25,
            'scientific_rigor': 0.25,
        },
        'Template-KG': {
            # æœ‰KGè®¿é—®å’Œå›ºå®šæ­¥éª¤
            'entity_f1': 0.20,
            'factual_accuracy': 0.20,
            'answer_completeness': 0.15,
            'scientific_rigor': 0.15,
            'depth_matching': 0.15,  # è¯„ä¼°æ­¥éª¤åŒ¹é…
            'modality_coverage': 0.15,
        },
        'RAG': {
            # é‡ç‚¹è¯„ä¼°æ£€ç´¢å’Œç­”æ¡ˆè´¨é‡
            'entity_f1': 0.20,
            'factual_accuracy': 0.25,
            'answer_completeness': 0.20,
            'scientific_rigor': 0.20,
            'modality_coverage': 0.15,
        },
        'ReAct': {
            # è¯„ä¼°æ¨ç†å’Œplanning
            'entity_f1': 0.15,
            'factual_accuracy': 0.20,
            'answer_completeness': 0.15,
            'scientific_rigor': 0.15,
            'depth_matching': 0.15,
            'plan_coherence': 0.10,
            'modality_coverage': 0.10,
        },
    },
}


# ==================== Data Structures ====================

@dataclass
class EvaluationMetrics:
    """
    è¯„ä¼°æŒ‡æ ‡ï¼ˆæ›´æ–°ç‰ˆ - æ”¯æŒNoneå€¼ï¼‰

    Noneå€¼è¡¨ç¤ºè¯¥æŒ‡æ ‡ä¸é€‚ç”¨äºå½“å‰æ–¹æ³•
    """

    # D1: Adaptive Planning (ç³»ç»Ÿèƒ½åŠ›)
    depth_matching_accuracy: Optional[float] = None
    plan_coherence: Optional[float] = None
    strategy_selection_accuracy: Optional[float] = None

    # D2: Entity Recognition (æ ¸å¿ƒèƒ½åŠ›)
    entity_precision: float = 0.0
    entity_recall: float = 0.0
    entity_f1: float = 0.0

    # D3: Multi-hop Reasoning (æ ¸å¿ƒèƒ½åŠ›)
    multi_hop_depth: int = 0
    query_success_rate: float = 0.0

    # D4: Multi-Modal Integration (ç³»ç»Ÿèƒ½åŠ›)
    modality_coverage: Optional[float] = None
    modalities_used: List[str] = field(default_factory=list)
    closed_loop_achieved: Optional[bool] = None

    # D5: Answer Quality (æ ¸å¿ƒèƒ½åŠ›)
    factual_accuracy: float = 0.0
    answer_completeness: float = 0.0
    scientific_rigor: float = 0.0

    # D6: Efficiency (æ‰€æœ‰æ–¹æ³•)
    execution_time: float = 0.0
    api_calls: int = 0

    # ğŸ”§ æ–°å¢ï¼šæ–¹æ³•ç‰¹å®šOverallåˆ†æ•°
    overall_score: Optional[float] = None

    # ğŸ”§ æ–°å¢ï¼šä»»åŠ¡å®Œæˆåº¦
    task_completion: Optional[str] = None  # 'completed', 'partial', 'failed', None


# ==================== D1: Adaptive Planning Evaluator ====================

class AdaptivePlanningEvaluator:
    """
    è‡ªé€‚åº”è§„åˆ’è¯„ä¼°å™¨

    ğŸ”§ ä¿®å¤ï¼šåªå¯¹æœ‰planningèƒ½åŠ›çš„æ–¹æ³•è¯„ä¼°
    """

    def __init__(self):
        self.depth_map = {
            'shallow': 2,
            'medium': 4,
            'deep': 6,
        }

    def evaluate(self,
                question_data: Dict,
                agent_output: Dict,
                method_name: str) -> Dict[str, float]:
        """
        è¯„ä¼°adaptive planning

        ğŸ”§ ä¿®å¤ï¼šå¯¹äºæ— planningçš„æ–¹æ³•ï¼Œè¿”å›Noneè€Œé0
        """

        metrics = {}

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¯„ä¼°planning
        should_evaluate_planning = method_name in EVALUATION_CONFIG['system_metrics']['plan_coherence']['methods']

        # D1.1: Depth Matching
        if method_name in EVALUATION_CONFIG['system_metrics']['depth_matching']['methods']:
            metrics['depth_matching'] = self._evaluate_depth_matching(
                question_data, agent_output
            )
        else:
            metrics['depth_matching'] = None

        # D1.2: Plan Coherence
        if should_evaluate_planning:
            metrics['plan_coherence'] = self._evaluate_plan_coherence(
                agent_output
            )
        else:
            metrics['plan_coherence'] = None

        # D1.3: Strategy Selection
        if should_evaluate_planning:
            metrics['strategy_selection'] = self._evaluate_strategy_selection(
                question_data, agent_output
            )
        else:
            metrics['strategy_selection'] = None

        # D1.4: Modality Coverage (æœ‰KGè®¿é—®çš„æ–¹æ³•éƒ½è¯„ä¼°)
        if method_name in EVALUATION_CONFIG['system_metrics']['modality_coverage']['methods']:
            metrics['modality_coverage'] = self._evaluate_modality_coverage(
                question_data, agent_output
            )
        else:
            metrics['modality_coverage'] = None

        # D1.5: Closed-Loop (åªæœ‰AIPOMè¯„ä¼°)
        if method_name in EVALUATION_CONFIG['system_metrics']['closed_loop']['methods']:
            metrics['closed_loop'] = self._evaluate_closed_loop(
                question_data, agent_output
            )
        else:
            metrics['closed_loop'] = None

        return metrics

    def _evaluate_depth_matching(self, question_data: Dict, agent_output: Dict) -> float:
        """è¯„ä¼°æ·±åº¦åŒ¹é…"""

        expected_depth = question_data.get('expected_depth', 'medium')
        expected_steps = self.depth_map.get(expected_depth, 4)

        executed_steps = agent_output.get('executed_steps', [])
        actual_steps = len(executed_steps)

        if actual_steps == 0:
            return 0.0

        # è®¡ç®—åŒ¹é…åº¦ï¼ˆå…è®¸Â±2æ­¥çš„è¯¯å·®ï¼‰
        diff = abs(actual_steps - expected_steps)

        if diff == 0:
            score = 1.0
        elif diff == 1:
            score = 0.9
        elif diff == 2:
            score = 0.75
        elif diff == 3:
            score = 0.5
        else:
            score = max(0.0, 1.0 - (diff - 3) * 0.15)

        return score

    def _evaluate_plan_coherence(self, agent_output: Dict) -> float:
        """è¯„ä¼°è®¡åˆ’è¿è´¯æ€§"""

        executed_steps = agent_output.get('executed_steps', [])

        if len(executed_steps) < 2:
            return 1.0 if len(executed_steps) == 1 else 0.0

        coherence_score = 0.0

        # 1. æ­¥éª¤é—´é€»è¾‘è¿è´¯æ€§ (40%)
        logical_coherence = self._check_logical_flow(executed_steps)
        coherence_score += logical_coherence * 0.4

        # 2. æ¨¡æ€å¤šæ ·æ€§ (30%)
        modality_diversity = self._check_modality_diversity(executed_steps)
        coherence_score += modality_diversity * 0.3

        # 3. æ— é‡å¤æŸ¥è¯¢ (30%)
        no_duplication = self._check_no_duplication(executed_steps)
        coherence_score += no_duplication * 0.3

        return coherence_score

    def _check_logical_flow(self, steps: List[Dict]) -> float:
        """æ£€æŸ¥é€»è¾‘æµ"""

        if len(steps) < 2:
            return 1.0

        # æ£€æŸ¥æ˜¯å¦æœ‰åˆç†çš„progression
        # molecular â†’ morphological â†’ projectionæ˜¯å¥½çš„æµç¨‹
        modality_order = {
            'molecular': 1,
            'morphological': 2,
            'projection': 3,
            'statistical': 4,
        }

        scores = []
        for i in range(len(steps) - 1):
            mod1 = steps[i].get('modality')
            mod2 = steps[i+1].get('modality')

            if mod1 and mod2:
                order1 = modality_order.get(mod1, 0)
                order2 = modality_order.get(mod2, 0)

                # å…è®¸å¹³çº§æˆ–é€’è¿›
                if order2 >= order1:
                    scores.append(1.0)
                elif order2 == order1 - 1:  # å…è®¸å°å¹…å›é€€
                    scores.append(0.8)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.7)  # æœªçŸ¥æ¨¡æ€

        return np.mean(scores) if scores else 0.5

    def _check_modality_diversity(self, steps: List[Dict]) -> float:
        """æ£€æŸ¥æ¨¡æ€å¤šæ ·æ€§"""

        modalities = set(s.get('modality') for s in steps if s.get('modality'))

        num_modalities = len(modalities)

        if num_modalities >= 3:
            return 1.0
        elif num_modalities == 2:
            return 0.7
        elif num_modalities == 1:
            return 0.4
        else:
            return 0.0

    def _check_no_duplication(self, steps: List[Dict]) -> float:
        """æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æŸ¥è¯¢"""

        purposes = [s.get('purpose', '') for s in steps]

        if len(purposes) == 0:
            return 1.0

        unique_purposes = len(set(purposes))
        total_purposes = len(purposes)

        return unique_purposes / total_purposes

    def _evaluate_strategy_selection(self, question_data: Dict, agent_output: Dict) -> float:
        """è¯„ä¼°ç­–ç•¥é€‰æ‹©"""

        expected_strategy = question_data.get('expected_strategy', 'adaptive')

        # ä»agent_outputæ¨æ–­å®é™…ç­–ç•¥
        steps = agent_output.get('executed_steps', [])

        if not steps:
            return 0.0

        # å¯å‘å¼åˆ¤æ–­ç­–ç•¥
        modalities = set(s.get('modality') for s in steps if s.get('modality'))

        if len(modalities) >= 3:
            inferred_strategy = 'focus_driven'
        elif len(steps) > 5:
            inferred_strategy = 'comparative'
        else:
            inferred_strategy = 'adaptive'

        # åŒ¹é…åº¦
        if inferred_strategy == expected_strategy:
            return 1.0
        else:
            return 0.6  # éƒ¨åˆ†åŒ¹é…

    def _evaluate_modality_coverage(self, question_data: Dict, agent_output: Dict) -> float:
        """è¯„ä¼°æ¨¡æ€è¦†ç›–"""

        expected_modalities = set(question_data.get('expected_modalities', []))

        if not expected_modalities:
            return 1.0

        executed_steps = agent_output.get('executed_steps', [])
        covered_modalities = set(s.get('modality') for s in executed_steps if s.get('modality'))

        if not covered_modalities:
            return 0.0

        intersection = expected_modalities & covered_modalities

        recall = len(intersection) / len(expected_modalities)
        precision = len(intersection) / len(covered_modalities) if covered_modalities else 0

        # F1 score
        if recall + precision == 0:
            return 0.0

        f1 = 2 * (precision * recall) / (precision + recall)

        return f1

    def _evaluate_closed_loop(self, question_data: Dict, agent_output: Dict) -> float:
        """è¯„ä¼°é—­ç¯å®Œæˆåº¦"""

        expected_closed_loop = question_data.get('expected_closed_loop', False)

        if not expected_closed_loop:
            return 1.0  # ä¸éœ€è¦é—­ç¯

        # æ£€æŸ¥æ˜¯å¦å®Œæˆé—­ç¯
        executed_steps = agent_output.get('executed_steps', [])

        # é—­ç¯éœ€è¦ï¼šprojectionæ­¥éª¤ + target compositionæ­¥éª¤
        has_projection = False
        has_target_composition = False

        for step in executed_steps:
            purpose = step.get('purpose', '').lower()
            modality = step.get('modality', '')

            if 'projection' in purpose or modality == 'projection':
                has_projection = True

            if ('target' in purpose and 'composition' in purpose) or \
               ('target' in purpose and modality == 'molecular'):
                has_target_composition = True

        if has_projection and has_target_composition:
            return 1.0
        elif has_projection:
            return 0.5  # éƒ¨åˆ†å®Œæˆ
        else:
            return 0.0


# ==================== D2: Entity Recognition Evaluator ====================

class EntityRecognitionEvaluator:
    """å®ä½“è¯†åˆ«è¯„ä¼°å™¨ï¼ˆæ‰€æœ‰æ–¹æ³•éƒ½è¯„ä¼°ï¼‰"""

    def evaluate(self, question_data: Dict, agent_output: Dict) -> Dict[str, float]:
        """è¯„ä¼°å®ä½“è¯†åˆ«"""

        expected_entities = set(question_data.get('expected_entities', []))
        recognized_entities = agent_output.get('entities_recognized', [])

        if not expected_entities:
            # æ²¡æœ‰é¢„æœŸå®ä½“ï¼Œæ£€æŸ¥æ˜¯å¦è¯†åˆ«äº†ä»»ä½•å®ä½“
            if recognized_entities:
                return {
                    'entity_precision': 0.5,
                    'entity_recall': 0.5,
                    'entity_f1': 0.5,
                }
            else:
                return {
                    'entity_precision': 1.0,
                    'entity_recall': 1.0,
                    'entity_f1': 1.0,
                }

        # æå–è¯†åˆ«çš„å®ä½“æ–‡æœ¬
        recognized_texts = set()
        for entity in recognized_entities:
            if isinstance(entity, dict):
                text = entity.get('text', '')
            else:
                text = str(entity)

            recognized_texts.add(text.lower().strip())

        # æ ‡å‡†åŒ–é¢„æœŸå®ä½“
        expected_texts = set(e.lower().strip() for e in expected_entities)

        # è®¡ç®—precision, recall, F1
        if not recognized_texts:
            precision = 0.0
            recall = 0.0
            f1 = 0.0
        else:
            true_positives = len(expected_texts & recognized_texts)

            precision = true_positives / len(recognized_texts) if recognized_texts else 0.0
            recall = true_positives / len(expected_texts) if expected_texts else 0.0

            if precision + recall == 0:
                f1 = 0.0
            else:
                f1 = 2 * (precision * recall) / (precision + recall)

        return {
            'entity_precision': precision,
            'entity_recall': recall,
            'entity_f1': f1,
        }


# ==================== D5: Answer Quality Evaluator ====================

class AnswerQualityEvaluator:
    """ç­”æ¡ˆè´¨é‡è¯„ä¼°å™¨ï¼ˆæ‰€æœ‰æ–¹æ³•éƒ½è¯„ä¼°ï¼‰"""

    def evaluate(self, question_data: Dict, agent_output: Dict) -> Dict[str, float]:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""

        answer = agent_output.get('answer', '')

        if not answer or len(answer) < 20:
            return {
                'factual_accuracy': 0.0,
                'answer_completeness': 0.0,
                'scientific_rigor': 0.0,
            }

        metrics = {}

        # D5.1: Factual Accuracy (åŸºäºå…³é”®å®ä½“å’Œæ•°æ®çš„å­˜åœ¨)
        metrics['factual_accuracy'] = self._evaluate_factual_accuracy(
            question_data, answer, agent_output
        )

        # D5.2: Answer Completeness
        metrics['answer_completeness'] = self._evaluate_completeness(
            question_data, answer
        )

        # D5.3: Scientific Rigor
        metrics['scientific_rigor'] = self._evaluate_scientific_rigor(
            answer
        )

        return metrics

    def _evaluate_factual_accuracy(self, question_data: Dict, answer: str, agent_output: Dict) -> float:
        """è¯„ä¼°äº‹å®å‡†ç¡®æ€§"""

        score = 0.0

        # 1. æ£€æŸ¥é¢„æœŸå®ä½“æ˜¯å¦åœ¨ç­”æ¡ˆä¸­ (40%)
        expected_entities = question_data.get('expected_entities', [])
        if expected_entities:
            mentioned = sum(1 for entity in expected_entities if entity.lower() in answer.lower())
            entity_score = mentioned / len(expected_entities)
            score += entity_score * 0.4
        else:
            score += 0.4

        # 2. æ£€æŸ¥æ˜¯å¦æœ‰å®šé‡æ•°æ® (30%)
        has_numbers = bool(re.search(r'\d+', answer))
        has_specific_data = bool(re.search(r'\d+[,\d]*\s*(neurons?|cells?|clusters?|%)', answer, re.IGNORECASE))

        if has_specific_data:
            score += 0.3
        elif has_numbers:
            score += 0.15

        # 3. æ£€æŸ¥æ˜¯å¦æˆåŠŸæ‰§è¡Œ (30%)
        if agent_output.get('success', False):
            steps = agent_output.get('executed_steps', [])
            if steps:
                successful_steps = sum(1 for s in steps if s.get('success', True))
                success_rate = successful_steps / len(steps)
                score += success_rate * 0.3
            else:
                score += 0.3

        return min(score, 1.0)

    def _evaluate_completeness(self, question_data: Dict, answer: str) -> float:
        """è¯„ä¼°ç­”æ¡ˆå®Œæ•´æ€§"""

        score = 0.0

        # 1. ç­”æ¡ˆé•¿åº¦ (20%)
        word_count = len(answer.split())
        if word_count >= 100:
            score += 0.2
        elif word_count >= 50:
            score += 0.15
        elif word_count >= 20:
            score += 0.1

        # 2. è¦†ç›–é¢„æœŸæ¨¡æ€ (40%)
        expected_modalities = set(question_data.get('expected_modalities', []))
        if expected_modalities:
            answer_lower = answer.lower()

            modality_keywords = {
                'molecular': ['marker', 'gene', 'express', 'cluster', 'cell type'],
                'morphological': ['morphology', 'axon', 'dendrite', 'branch', 'length'],
                'projection': ['project', 'target', 'connect', 'pathway'],
            }

            covered = 0
            for modality in expected_modalities:
                keywords = modality_keywords.get(modality, [])
                if any(kw in answer_lower for kw in keywords):
                    covered += 1

            modality_score = covered / len(expected_modalities)
            score += modality_score * 0.4
        else:
            score += 0.4

        # 3. ç»“æ„åŒ–ç¨‹åº¦ (20%)
        has_list = bool(re.search(r'(\d+\.|â€¢|-)(\s+\w+)', answer))
        has_sections = answer.count('\n') >= 2

        if has_list and has_sections:
            score += 0.2
        elif has_list or has_sections:
            score += 0.1

        # 4. æ— æ˜æ˜¾é”™è¯¯æ ‡è®° (20%)
        error_markers = ['error', 'failed', 'unable', 'cannot', 'no data', 'not found']
        has_errors = any(marker in answer.lower() for marker in error_markers)

        if not has_errors:
            score += 0.2

        return min(score, 1.0)

    def _evaluate_scientific_rigor(self, answer: str) -> float:
        """è¯„ä¼°ç§‘å­¦ä¸¥è°¨æ€§"""

        score = 0.0

        # 1. å®šé‡æ•°æ® (40%)
        numbers = re.findall(r'\d+[,\d]*', answer)
        num_count = len(numbers)

        if num_count >= 5:
            score += 0.4
        elif num_count >= 3:
            score += 0.3
        elif num_count >= 1:
            score += 0.2

        # 2. ç§‘å­¦æœ¯è¯­ (30%)
        scientific_terms = [
            'neuron', 'cluster', 'marker', 'express', 'project',
            'morphology', 'axon', 'dendrite', 'synapse', 'circuit',
            'region', 'cortex', 'connectivity', 'distribution'
        ]

        answer_lower = answer.lower()
        term_count = sum(1 for term in scientific_terms if term in answer_lower)

        if term_count >= 8:
            score += 0.3
        elif term_count >= 5:
            score += 0.2
        elif term_count >= 3:
            score += 0.1

        # 3. å¼•ç”¨KGæ•°æ® (30%)
        kg_citations = [
            'according to', 'based on', 'data shows', 'found',
            'identified', 'observed', 'recorded', 'measured'
        ]

        has_citation = any(cite in answer_lower for cite in kg_citations)

        if has_citation:
            score += 0.3

        return min(score, 1.0)


# ==================== ğŸ”§ æ–°å¢ï¼šBiological Task Evaluator ====================

class BiologicalTaskEvaluator:
    """
    ç”Ÿç‰©å­¦ä»»åŠ¡è¯„ä¼°å™¨

    è¯„ä¼°ä»»åŠ¡å®Œæˆåº¦ï¼š'completed', 'partial', 'failed'
    """

    def __init__(self):
        pass

    def evaluate_task_completion(self,
                                 question_data: Dict,
                                 agent_output: Dict) -> str:
        """
        è¯„ä¼°ä»»åŠ¡å®Œæˆåº¦

        Returns:
            'completed' | 'partial' | 'failed'
        """

        # è·å–successå’Œpartial criteria
        success_criteria = question_data.get('success_criteria', {})
        partial_criteria = question_data.get('partial_criteria', {})

        if not success_criteria:
            # å¦‚æœæ²¡æœ‰å®šä¹‰criteriaï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°
            return self._default_evaluation(question_data, agent_output)

        # æ£€æŸ¥success criteria
        success_checks = self._check_criteria(
            success_criteria,
            agent_output,
            question_data
        )

        if all(success_checks.values()):
            return 'completed'

        # æ£€æŸ¥partial criteria
        if partial_criteria:
            partial_checks = self._check_criteria(
                partial_criteria,
                agent_output,
                question_data
            )

            if all(partial_checks.values()):
                return 'partial'

        return 'failed'

    def _check_criteria(self,
                       criteria: Dict,
                       agent_output: Dict,
                       question_data: Dict) -> Dict[str, bool]:
        """æ£€æŸ¥æ ‡å‡†æ˜¯å¦æ»¡è¶³"""

        checks = {}

        answer = agent_output.get('answer', '')
        steps = agent_output.get('executed_steps', [])

        for criterion, requirement in criteria.items():

            if criterion == 'modalities_covered':
                # æ£€æŸ¥æ¨¡æ€è¦†ç›–
                checks[criterion] = self._check_modalities(requirement, steps)

            elif criterion == 'min_steps':
                # æ£€æŸ¥æœ€å°æ­¥æ•°
                checks[criterion] = len(steps) >= requirement

            elif criterion == 'closed_loop_required':
                # æ£€æŸ¥é—­ç¯
                if requirement:
                    checks[criterion] = self._check_closed_loop(steps)
                else:
                    checks[criterion] = True

            elif criterion == 'systematic_analysis':
                # æ£€æŸ¥ç³»ç»Ÿåˆ†æ
                if requirement:
                    checks[criterion] = self._check_systematic(steps, answer)
                else:
                    checks[criterion] = True

            elif criterion == 'min_regions_compared':
                # æ£€æŸ¥æ¯”è¾ƒçš„è„‘åŒºæ•°é‡
                checks[criterion] = self._check_regions_compared(answer, requirement)

            elif criterion == 'statistical_testing':
                # æ£€æŸ¥ç»Ÿè®¡æ£€éªŒ
                if requirement:
                    checks[criterion] = self._check_statistical_test(steps, answer)
                else:
                    checks[criterion] = True

            elif criterion == 'regions_identified':
                # æ£€æŸ¥æ˜¯å¦è¯†åˆ«äº†è„‘åŒº
                checks[criterion] = self._check_regions_identified(answer, requirement)

            elif criterion == 'quantitative_data':
                # æ£€æŸ¥æ˜¯å¦æœ‰å®šé‡æ•°æ®
                checks[criterion] = self._check_quantitative_data(answer, requirement)

            elif criterion == 'factual_correct':
                # æ£€æŸ¥äº‹å®æ­£ç¡®æ€§
                checks[criterion] = agent_output.get('success', False)

            else:
                # æœªçŸ¥criterionï¼Œé»˜è®¤é€šè¿‡
                checks[criterion] = True

        return checks

    def _check_modalities(self, required_modalities: List[str], steps: List[Dict]) -> bool:
        """æ£€æŸ¥æ¨¡æ€è¦†ç›–"""
        modalities_used = set()

        for step in steps:
            modality = step.get('modality')
            if modality:
                modalities_used.add(modality)

        return set(required_modalities).issubset(modalities_used)

    def _check_closed_loop(self, steps: List[Dict]) -> bool:
        """æ£€æŸ¥æ˜¯å¦å®Œæˆé—­ç¯"""
        has_projection = False
        has_target_composition = False

        for step in steps:
            purpose = step.get('purpose', '').lower()
            modality = step.get('modality', '')

            if 'projection' in purpose or modality == 'projection':
                has_projection = True

            if ('target' in purpose and 'composition' in purpose) or \
               ('target' in purpose and modality == 'molecular'):
                has_target_composition = True

        return has_projection and has_target_composition

    def _check_systematic(self, steps: List[Dict], answer: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†ç³»ç»Ÿåˆ†æ"""
        # æ£€æŸ¥æ­¥éª¤ä¸­æ˜¯å¦æœ‰comparative/systematicå…³é”®è¯
        for step in steps:
            purpose = step.get('purpose', '').lower()
            if any(kw in purpose for kw in ['compare', 'systematic', 'all', 'multiple', 'screen']):
                return True

        # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦æœ‰systematicåˆ†æçš„è¿¹è±¡
        answer_lower = answer.lower()
        if any(kw in answer_lower for kw in ['compared', 'across regions', 'systematic', 'all regions']):
            return True

        return False

    def _check_regions_compared(self, answer: str, min_count: int) -> bool:
        """æ£€æŸ¥æ¯”è¾ƒäº†å¤šå°‘ä¸ªè„‘åŒº"""
        regions = re.findall(r'\b[A-Z]{2,5}\b', answer)

        unique_regions = set(regions)

        stopwords = {'DNA', 'RNA', 'ATP', 'GABA', 'LLM', 'ALL', 'THE'}
        unique_regions -= stopwords

        return len(unique_regions) >= min_count

    def _check_statistical_test(self, steps: List[Dict], answer: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†ç»Ÿè®¡æ£€éªŒ"""
        for step in steps:
            step_type = step.get('step_type', '')
            purpose = step.get('purpose', '').lower()

            if step_type == 'statistical' or \
               any(kw in purpose for kw in ['statistic', 'test', 'fdr', 'p-value', 'significance']):
                return True

        answer_lower = answer.lower()
        stat_terms = ['p-value', 'p value', 'statistical', 'significance', 'fdr', 't-test', 'anova']

        return any(term in answer_lower for term in stat_terms)

    def _check_regions_identified(self, answer: str, required: bool) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¯†åˆ«äº†è„‘åŒº"""
        if not required:
            return True

        regions = re.findall(r'\b[A-Z]{2,5}\b', answer)
        return len(regions) > 0

    def _check_quantitative_data(self, answer: str, required: bool) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å®šé‡æ•°æ®"""
        if not required:
            return True

        has_numbers = bool(re.search(r'\d+', answer))
        return has_numbers

    def _default_evaluation(self, question_data: Dict, agent_output: Dict) -> str:
        """é»˜è®¤è¯„ä¼°æ–¹æ³•ï¼ˆå½“æ²¡æœ‰å®šä¹‰criteriaæ—¶ï¼‰"""

        expected_depth = question_data.get('expected_depth', 'medium')
        steps = agent_output.get('executed_steps', [])
        answer = agent_output.get('answer', '')

        if not agent_output.get('success', False):
            return 'failed'

        if len(answer) < 50:
            return 'failed'

        # ç®€å•çš„å¯å‘å¼
        if expected_depth == 'shallow':
            return 'completed' if len(steps) >= 1 else 'failed'

        elif expected_depth == 'medium':
            if len(steps) >= 2:
                return 'completed'
            elif len(steps) >= 1:
                return 'partial'
            else:
                return 'failed'

        else:  # deep
            if len(steps) >= 4:
                return 'completed'
            elif len(steps) >= 2:
                return 'partial'
            else:
                return 'failed'


# ==================== Comprehensive Evaluator (Updated) ====================

class ComprehensiveEvaluator:
    """
    ç»¼åˆè¯„ä¼°å™¨ï¼ˆv3.0 - å…¬å¹³çš„åˆ†å±‚è¯„ä¼°ï¼‰

    ğŸ”§ å…³é”®æ”¹è¿›ï¼š
    - åˆ†å±‚è¯„ä¼°ï¼šåŒºåˆ†æ ¸å¿ƒèƒ½åŠ›å’Œç³»ç»Ÿèƒ½åŠ›
    - None-ableæŒ‡æ ‡ï¼šä¸å¼ºåˆ¶æ‰€æœ‰æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šè¯„åˆ†
    - æ–¹æ³•ç‰¹å®šæƒé‡ï¼šè®¡ç®—Overallåˆ†æ•°
    """

    def __init__(self):
        self.planning_eval = AdaptivePlanningEvaluator()
        self.entity_eval = EntityRecognitionEvaluator()
        self.answer_eval = AnswerQualityEvaluator()
        self.task_eval = BiologicalTaskEvaluator()

        self.config = EVALUATION_CONFIG

    def evaluate_full(self,
                     question_data: Dict,
                     agent_output: Dict,
                     method_name: str) -> EvaluationMetrics:
        """
        å®Œæ•´è¯„ä¼°ï¼ˆv3.0 - å…¬å¹³ç‰ˆï¼‰

        ğŸ”§ ä¿®å¤ï¼š
        - åªè¯„ä¼°é€‚ç”¨çš„æŒ‡æ ‡
        - ä½¿ç”¨æ–¹æ³•ç‰¹å®šæƒé‡è®¡ç®—Overall
        """

        metrics = EvaluationMetrics()

        # D1: Adaptive Planning (ç³»ç»Ÿèƒ½åŠ› - åˆ†å±‚è¯„ä¼°)
        planning_metrics = self.planning_eval.evaluate(
            question_data, agent_output, method_name
        )

        metrics.depth_matching_accuracy = planning_metrics.get('depth_matching')
        metrics.plan_coherence = planning_metrics.get('plan_coherence')
        metrics.strategy_selection_accuracy = planning_metrics.get('strategy_selection')
        metrics.modality_coverage = planning_metrics.get('modality_coverage')

        closed_loop_score = planning_metrics.get('closed_loop')
        if closed_loop_score is not None:
            metrics.closed_loop_achieved = closed_loop_score >= 0.9
        else:
            metrics.closed_loop_achieved = None

        # D2: Entity Recognition (æ ¸å¿ƒèƒ½åŠ› - æ‰€æœ‰æ–¹æ³•)
        entity_metrics = self.entity_eval.evaluate(question_data, agent_output)
        metrics.entity_precision = entity_metrics['entity_precision']
        metrics.entity_recall = entity_metrics['entity_recall']
        metrics.entity_f1 = entity_metrics['entity_f1']

        # D3: Multi-hop (æ‰€æœ‰æœ‰KGè®¿é—®çš„æ–¹æ³•)
        steps = agent_output.get('executed_steps', [])
        metrics.multi_hop_depth = len(steps)

        if steps:
            successful = sum(1 for s in steps if s.get('success', True))
            metrics.query_success_rate = successful / len(steps)
        else:
            metrics.query_success_rate = 1.0

        # D4: Multi-Modal (å·²åœ¨planningä¸­è¯„ä¼°)
        modalities = set(s.get('modality') for s in steps if s.get('modality'))
        metrics.modalities_used = list(modalities)

        # D5: Answer Quality (æ ¸å¿ƒèƒ½åŠ› - æ‰€æœ‰æ–¹æ³•)
        answer_metrics = self.answer_eval.evaluate(question_data, agent_output)
        metrics.factual_accuracy = answer_metrics['factual_accuracy']
        metrics.answer_completeness = answer_metrics['answer_completeness']
        metrics.scientific_rigor = answer_metrics['scientific_rigor']

        # D6: Efficiency (æ‰€æœ‰æ–¹æ³•)
        metrics.execution_time = agent_output.get('execution_time', 0.0)
        metrics.api_calls = len(steps)

        # ğŸ”§ Task Completion (å¦‚æœæœ‰å®šä¹‰)
        if question_data.get('task_type'):
            metrics.task_completion = self.task_eval.evaluate_task_completion(
                question_data, agent_output
            )

        # ğŸ”§ è®¡ç®—æ–¹æ³•ç‰¹å®šçš„Overallåˆ†æ•°
        metrics.overall_score = self._calculate_weighted_overall(metrics, method_name)

        return metrics

    def _calculate_weighted_overall(self, metrics: EvaluationMetrics, method_name: str) -> float:
        """
        ğŸ”§ è®¡ç®—æ–¹æ³•ç‰¹å®šçš„åŠ æƒOverallåˆ†æ•°

        å…³é”®ï¼šåªå¯¹non-Noneçš„æŒ‡æ ‡åŠ æƒ
        """

        weights = self.config['method_weights'].get(method_name, {})

        if not weights:
            # Fallbackï¼šæ ¸å¿ƒæŒ‡æ ‡ç®€å•å¹³å‡
            core_scores = [
                metrics.entity_f1,
                metrics.factual_accuracy,
                metrics.answer_completeness,
                metrics.scientific_rigor,
            ]
            valid_scores = [s for s in core_scores if s is not None]
            return sum(valid_scores) / len(valid_scores) if valid_scores else 0.0

        # åŠ æƒå¹³å‡ï¼ˆåªå¯¹non-Noneçš„æŒ‡æ ‡ï¼‰
        weighted_sum = 0.0
        total_weight = 0.0

        metric_values = {
            'entity_f1': metrics.entity_f1,
            'factual_accuracy': metrics.factual_accuracy,
            'answer_completeness': metrics.answer_completeness,
            'scientific_rigor': metrics.scientific_rigor,
            'depth_matching': metrics.depth_matching_accuracy,
            'plan_coherence': metrics.plan_coherence,
            'modality_coverage': metrics.modality_coverage,
            'closed_loop': 1.0 if metrics.closed_loop_achieved else (0.0 if metrics.closed_loop_achieved is not None else None),
        }

        for metric_name, weight in weights.items():
            value = metric_values.get(metric_name)

            if value is not None:  # ğŸ”§ åªè®¡ç®—non-Noneçš„æŒ‡æ ‡
                weighted_sum += value * weight
                total_weight += weight

        return weighted_sum / total_weight if total_weight > 0 else 0.0


# ==================== Export ====================

__all__ = [
    'EvaluationMetrics',
    'AdaptivePlanningEvaluator',
    'EntityRecognitionEvaluator',
    'AnswerQualityEvaluator',
    'BiologicalTaskEvaluator',
    'ComprehensiveEvaluator',
    'EVALUATION_CONFIG',
]


# ==================== Test ====================

if __name__ == "__main__":
    print("\n" + "="*80)
    print("âœ… Enhanced evaluators.py v3.0 (Fair Evaluation) loaded successfully!")
    print("="*80)

    print("\nğŸ“Š Evaluation Configuration:")
    print("\nCore Metrics (all methods):")
    for metric, config in EVALUATION_CONFIG['core_metrics'].items():
        print(f"  - {metric}: {config['description']}")

    print("\nSystem Metrics (method-specific):")
    for metric, config in EVALUATION_CONFIG['system_metrics'].items():
        print(f"  - {metric}: {config['description']}")
        print(f"    â†’ Applicable to: {', '.join(config['methods'])}")

    print("\nğŸ”§ Method-Specific Weights:")
    for method, weights in EVALUATION_CONFIG['method_weights'].items():
        print(f"\n{method}:")
        total = sum(weights.values())
        for metric, weight in sorted(weights.items(), key=lambda x: -x[1]):
            print(f"  - {metric:25s}: {weight:.2f} ({weight/total*100:.1f}%)")

    print("\n" + "="*80)