"""
Evaluators for AIPOM-CoT Benchmark
===================================
å®ç°6ä¸ªç»´åº¦çš„è¯„ä¼°å™¨

Author: Claude & PrometheusTT
Date: 2025-01-15
"""

import re
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import statistics

logger = logging.getLogger(__name__)


# ==================== Data Structures ====================

@dataclass
class EvaluationMetrics:
    """å®Œæ•´è¯„ä¼°æŒ‡æ ‡"""

    # D1: Adaptive Planning
    depth_matching_accuracy: float = 0.0
    plan_coherence: float = 0.0
    modality_coverage: float = 0.0
    strategy_selection_accuracy: float = 0.0
    closed_loop_achieved: bool = False

    # D2: KG Reasoning
    entity_precision: float = 0.0
    entity_recall: float = 0.0
    entity_f1: float = 0.0
    multi_hop_depth: int = 0
    multi_hop_success: bool = True

    # D3: Reflection (AIPOM-CoT only)
    replanning_triggered: int = 0
    confidence_calibration_error: float = 0.0

    # D4: Multi-Modal Integration
    modalities_used: List[str] = field(default_factory=list)
    cross_modal_citations: int = 0

    # D5: Answer Quality
    factual_accuracy: float = 0.0
    answer_completeness: float = 0.0
    scientific_rigor: float = 0.0

    # D6: Efficiency
    execution_time: float = 0.0
    api_calls: int = 0
    query_success_rate: float = 0.0

    task_completion: str = 'unknown'  # 'completed', 'partial', 'failed', 'unknown'


# ==================== Evaluator Base Class ====================

class BaseEvaluator:
    """è¯„ä¼°å™¨åŸºç±»"""

    def __init__(self):
        self.stopwords = self._build_stopwords()

    def _build_stopwords(self) -> set:
        """æ„å»ºåœç”¨è¯è¡¨"""
        stopwords = set([
            # ç–‘é—®è¯
            'what', 'which', 'where', 'when', 'who', 'why', 'how',
            # beåŠ¨è¯
            'are', 'is', 'was', 'were', 'be', 'been', 'being', 'am',
            # åŠ©åŠ¨è¯
            'do', 'does', 'did', 'have', 'has', 'had',
            'can', 'could', 'will', 'would', 'shall', 'should',
            # ä»‹è¯
            'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
            # å† è¯
            'the', 'an', 'a',
            # ä»£è¯
            'it', 'its', 'they', 'their', 'this', 'that',
            # å¸¸è§åŠ¨è¯
            'get', 'give', 'show', 'tell', 'make', 'take',
            # ç¥ç»ç§‘å­¦é€šç”¨è¯ï¼ˆä¸æ˜¯å®ä½“ï¼‰
            'cells', 'neurons', 'brain', 'regions', 'region', 'areas', 'area',
        ])
        return stopwords


# ==================== D1: Adaptive Planning Evaluator ====================

class AdaptivePlanningEvaluator(BaseEvaluator):
    """è¯„ä¼°è‡ªé€‚åº”è§„åˆ’èƒ½åŠ›"""

    def evaluate(self,
                 question_data: Dict,
                 agent_output: Dict,
                 method_name: str) -> Dict[str, float]:
        """
        è¯„ä¼°è‡ªé€‚åº”è§„åˆ’

        Args:
            question_data: æµ‹è¯•é—®é¢˜æ•°æ®
            agent_output: Agentè¾“å‡º
            method_name: æ–¹æ³•åç§°

        Returns:
            è¯„ä¼°æŒ‡æ ‡dict
        """

        metrics = {}

        # D1.1: Depth Matching Accuracy
        metrics['depth_matching'] = self._evaluate_depth_matching(
            question_data, agent_output, method_name
        )

        # D1.2: Plan Coherence
        metrics['plan_coherence'] = self._evaluate_plan_coherence(
            agent_output, method_name
        )

        # D1.3: Modality Coverage
        metrics['modality_coverage'] = self._evaluate_modality_coverage(
            question_data, agent_output
        )

        # D1.4: Strategy Selection (AIPOM-CoT only)
        if method_name == 'AIPOM-CoT':
            metrics['strategy_selection'] = self._evaluate_strategy_selection(
                question_data, agent_output
            )
        else:
            metrics['strategy_selection'] = 0.0

        # D1.5: Closed-Loop Achievement
        metrics['closed_loop'] = self._evaluate_closed_loop(
            question_data, agent_output
        )

        return metrics

    def _evaluate_depth_matching(self,
                                 question_data: Dict,
                                 agent_output: Dict,
                                 method_name: str) -> float:
        """è¯„ä¼°æ·±åº¦åŒ¹é…"""

        expected_depth = question_data.get('expected_depth', 'medium')
        executed_steps = agent_output.get('total_steps', 0)

        # Baselineæ–¹æ³•æ·±åº¦å›ºå®š
        if method_name == 'Direct LLM':
            return 1.0 if expected_depth == 'shallow' else 0.0

        if method_name == 'RAG':
            return 1.0 if expected_depth in ['shallow', 'medium'] else 0.0

        if method_name == 'ReAct':
            # ReActå›ºå®š3æ­¥
            if expected_depth == 'medium' and 2 <= executed_steps <= 4:
                return 1.0
            else:
                return 0.3

        # AIPOM-CoT
        depth_map = {
            'shallow': (1, 2),
            'medium': (3, 4),
            'deep': (5, 7),
        }

        expected_range = depth_map.get(expected_depth, (3, 4))
        min_steps, max_steps = expected_range

        # åœ¨èŒƒå›´å†… â†’ 1.0
        if min_steps <= executed_steps <= max_steps:
            return 1.0

        # åœ¨èŒƒå›´å¤–ï¼Œè®¡ç®—åç¦»ç¨‹åº¦
        if executed_steps < min_steps:
            deviation = min_steps - executed_steps
        else:
            deviation = executed_steps - max_steps

        # æ¯åç¦»1æ­¥ï¼Œæ‰£0.2åˆ†
        score = max(0.0, 1.0 - deviation * 0.2)

        return score

    def _evaluate_plan_coherence(self, agent_output: Dict, method_name: str) -> float:
        """è¯„ä¼°è®¡åˆ’è¿è´¯æ€§"""

        steps = agent_output.get('executed_steps', [])

        if not steps:
            return 0.0

        if method_name in ['Direct LLM']:
            return 0.0  # æ— è®¡åˆ’

        if method_name == 'RAG':
            return 0.3  # å•æ­¥æ£€ç´¢ï¼Œè¿è´¯æ€§ä½

        # ReActå’ŒAIPOM-CoTæ£€æŸ¥stepä¹‹é—´çš„ä¾èµ–
        has_dependencies = 0
        for i, step in enumerate(steps):
            if i > 0:
                # æ£€æŸ¥purposeæ˜¯å¦æåˆ°å‰ä¸€æ­¥
                purpose = step.get('purpose', '').lower()
                prev_purpose = steps[i - 1].get('purpose', '').lower()

                # ç®€å•å¯å‘å¼ï¼šæ˜¯å¦æåˆ°"target", "focus", "primary"ç­‰
                if any(kw in purpose for kw in ['target', 'focus', 'primary', 'discovered', 'identified']):
                    has_dependencies += 1

        if len(steps) <= 1:
            return 0.5

        coherence = has_dependencies / (len(steps) - 1)

        return coherence

    def _evaluate_modality_coverage(self, question_data: Dict, agent_output: Dict) -> float:
        """è¯„ä¼°æ¨¡æ€è¦†ç›–"""

        expected_modalities = set(question_data.get('expected_modalities', []))

        # ä»stepsæå–å®é™…ä½¿ç”¨çš„æ¨¡æ€
        steps = agent_output.get('executed_steps', [])
        used_modalities = set()

        for step in steps:
            modality = step.get('modality')
            if modality:
                used_modalities.add(modality)

        # ä»ç­”æ¡ˆæ¨æ–­æ¨¡æ€
        answer = agent_output.get('answer', '').lower()

        molecular_kw = ['gene', 'marker', 'express', 'cluster', 'subclass', 'cell type']
        if any(kw in answer for kw in molecular_kw):
            used_modalities.add('molecular')

        morpho_kw = ['axon', 'dendrite', 'morpholog', 'branch', 'length', 'arbor']
        if any(kw in answer for kw in morpho_kw):
            used_modalities.add('morphological')

        projection_kw = ['project', 'target', 'connect', 'pathway', 'circuit']
        if any(kw in answer for kw in projection_kw):
            used_modalities.add('projection')

        if not expected_modalities:
            return 1.0

        coverage = len(used_modalities & expected_modalities) / len(expected_modalities)

        return coverage

    def _evaluate_strategy_selection(self, question_data: Dict, agent_output: Dict) -> float:
        """è¯„ä¼°ç­–ç•¥é€‰æ‹©ï¼ˆAIPOM-CoT onlyï¼‰"""

        expected_strategy = question_data.get('expected_strategy', 'adaptive')

        # ä»agent_outputæå–å®é™…ç­–ç•¥
        actual_strategy = agent_output.get('adaptive_planning', {}).get('selected_planner', 'unknown')

        if actual_strategy == expected_strategy:
            return 1.0

        # éƒ¨åˆ†åŒ¹é…
        if expected_strategy == 'focus_driven' and actual_strategy == 'adaptive':
            return 0.5

        return 0.0

    def _evaluate_closed_loop(self, question_data: Dict, agent_output: Dict) -> float:
        """è¯„ä¼°é—­ç¯å®Œæˆ"""

        expected_closed_loop = question_data.get('expected_closed_loop', False)

        if not expected_closed_loop:
            # ä¸éœ€è¦é—­ç¯
            return 1.0

        # æ£€æŸ¥æ˜¯å¦æœ‰target compositionæ­¥éª¤
        steps = agent_output.get('executed_steps', [])

        has_projection = False
        has_target_composition = False

        for step in steps:
            purpose = step.get('purpose', '').lower()
            modality = step.get('modality', '')

            if 'projection' in purpose or modality == 'projection':
                has_projection = True

            if 'target' in purpose and 'composition' in purpose:
                has_target_composition = True

            if 'target' in purpose and modality == 'molecular':
                has_target_composition = True

        if has_projection and has_target_composition:
            return 1.0
        elif has_projection:
            return 0.5  # æœ‰projectionä½†æ²¡é—­ç¯
        else:
            return 0.0


# ==================== D2: Entity Recognition Evaluator ====================

class EntityRecognitionEvaluator(BaseEvaluator):
    """è¯„ä¼°å®ä½“è¯†åˆ«"""

    def evaluate(self,
                 question_data: Dict,
                 agent_output: Dict) -> Dict[str, float]:
        """è¯„ä¼°å®ä½“è¯†åˆ«F1"""

        expected_entities = set([
            e.lower().strip()
            for e in question_data.get('expected_entities', [])
            if e
        ])

        predicted_entities = set()

        # ä»agent_outputæå–
        for e in agent_output.get('entities_recognized', []):
            if isinstance(e, dict):
                text = e.get('text', '').lower().strip()
            else:
                text = str(e).lower().strip()

            if text and len(text) >= 2 and text not in self.stopwords:
                predicted_entities.add(text)

        # ä»é—®é¢˜ä¸­æå–æ˜æ˜¾å®ä½“ï¼ˆè¾…åŠ©ï¼‰
        question = question_data.get('question', '')
        question_entities = self._extract_from_question(question)

        predicted_entities |= question_entities

        # è®¡ç®—F1
        if not expected_entities:
            # æ²¡æœ‰expected entitiesï¼Œè®¤ä¸ºé€šè¿‡
            return {'entity_precision': 1.0, 'entity_recall': 1.0, 'entity_f1': 1.0}

        # æ¨¡ç³ŠåŒ¹é…
        true_positives = 0
        for expected in expected_entities:
            for predicted in predicted_entities:
                if self._fuzzy_match(expected, predicted):
                    true_positives += 1
                    break

        false_positives = len(predicted_entities) - true_positives
        false_negatives = len(expected_entities) - true_positives

        precision = true_positives / (true_positives + false_positives) if (
                                                                                       true_positives + false_positives) > 0 else 0.0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

        return {
            'entity_precision': precision,
            'entity_recall': recall,
            'entity_f1': f1,
        }

    def _extract_from_question(self, question: str) -> set:
        """ä»é—®é¢˜æå–æ˜æ˜¾å®ä½“"""
        entities = set()

        # è„‘åŒºç¼©å†™
        regions = re.findall(r'\b[A-Z]{2,5}\b', question)
        for r in regions:
            if len(r) >= 2 and r.lower() not in self.stopwords:
                entities.add(r.lower())

        # åŸºå› å
        genes = re.findall(r'\b[A-Z][a-z]{2,8}\d*\+?\b', question)
        gene_stopwords = {'What', 'Which', 'Where', 'Tell', 'Give', 'Show', 'Find'}
        for g in genes:
            if g not in gene_stopwords:
                entities.add(g.rstrip('+').lower())

        return entities

    def _fuzzy_match(self, expected: str, predicted: str) -> bool:
        """æ¨¡ç³ŠåŒ¹é…"""
        expected = expected.lower().strip()
        predicted = predicted.lower().strip()

        if expected == predicted:
            return True

        if expected in predicted or predicted in expected:
            return True

        if len(expected) > 3 and len(predicted) > 3:
            if expected[:3] == predicted[:3]:
                return True

        return False


# ==================== D5: Answer Quality Evaluator ====================

class AnswerQualityEvaluator(BaseEvaluator):
    """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""

    def evaluate(self,
                 question_data: Dict,
                 agent_output: Dict) -> Dict[str, float]:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""

        answer = agent_output.get('answer', '')
        question = question_data.get('question', '')

        metrics = {}

        # D5.1: Factual Accuracy
        metrics['factual_accuracy'] = self._evaluate_factual_accuracy(answer)

        # D5.2: Answer Completeness
        metrics['answer_completeness'] = self._evaluate_completeness(
            answer, question, question_data
        )

        # D5.3: Scientific Rigor
        metrics['scientific_rigor'] = self._evaluate_scientific_rigor(answer)

        return metrics

    def _evaluate_factual_accuracy(self, answer: str) -> float:
        """è¯„ä¼°äº‹å®å‡†ç¡®æ€§"""
        answer_lower = answer.lower()

        score = 0.0

        # æœ‰å…·ä½“æ•°æ®
        if re.search(r'\d+', answer):
            score += 0.3

        # æœ‰è„‘åŒºåç§°
        if re.search(r'\b[A-Z]{2,5}\b', answer):
            score += 0.3

        # æœ‰ç§‘å­¦æœ¯è¯­
        sci_terms = ['neuron', 'cell', 'region', 'cortex', 'gene', 'marker',
                     'cluster', 'projection', 'axon', 'dendrite']
        if any(term in answer_lower for term in sci_terms):
            score += 0.2

        # æœ‰å®šé‡æè¿°
        quant_terms = ['average', 'mean', 'number', 'count', 'percentage', '%']
        if any(term in answer_lower for term in quant_terms):
            score += 0.2

        return min(1.0, score)

    def _evaluate_completeness(self, answer: str, question: str, question_data: Dict) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""

        answer_words = len(answer.split())
        question_words = len(question.split())

        # é¢„æœŸé•¿åº¦
        expected_depth = question_data.get('expected_depth', 'medium')

        if expected_depth == 'shallow':
            expected_length = 50
        elif expected_depth == 'medium':
            expected_length = 150
        else:  # deep
            expected_length = 300

        # é•¿åº¦é€‚ä¸­æ€§
        length_score = min(1.0, answer_words / expected_length)

        # æ£€æŸ¥æ˜¯å¦å›ç­”äº†é—®é¢˜çš„å„ä¸ªæ–¹é¢
        expected_modalities = question_data.get('expected_modalities', [])

        coverage = 0.0
        for modality in expected_modalities:
            if modality == 'molecular':
                if any(kw in answer.lower() for kw in ['gene', 'marker', 'cluster', 'cell type']):
                    coverage += 1
            elif modality == 'morphological':
                if any(kw in answer.lower() for kw in ['axon', 'dendrite', 'morpholog', 'branch']):
                    coverage += 1
            elif modality == 'projection':
                if any(kw in answer.lower() for kw in ['project', 'target', 'connect']):
                    coverage += 1

        if expected_modalities:
            modality_score = coverage / len(expected_modalities)
        else:
            modality_score = 1.0

        completeness = (length_score + modality_score) / 2

        return completeness

    def _evaluate_scientific_rigor(self, answer: str) -> float:
        """è¯„ä¼°ç§‘å­¦ä¸¥è°¨æ€§"""
        answer_lower = answer.lower()

        score = 0.0

        # æœ‰ç§‘å­¦æœ¯è¯­
        sci_terms = ['neuron', 'cortex', 'expression', 'projection',
                     'morphology', 'cluster', 'marker', 'region', 'circuit']
        sci_count = sum(1 for term in sci_terms if term in answer_lower)
        score += min(0.4, sci_count * 0.1)

        # æœ‰å®šé‡æ•°æ®
        has_numbers = bool(re.search(r'\d+', answer))
        if has_numbers:
            score += 0.3

        # é¿å…æ¨¡ç³Šè¯
        vague_terms = ['some', 'several', 'many', 'few', 'various', 'might', 'maybe']
        vague_count = sum(1 for term in vague_terms if term in answer_lower)
        score += max(0.0, 0.3 - vague_count * 0.1)

        return min(1.0, score)


# ==================== Comprehensive Evaluator ====================

class ComprehensiveEvaluator:
    """ç»¼åˆè¯„ä¼°å™¨"""

    def __init__(self):
        self.planning_eval = AdaptivePlanningEvaluator()
        self.entity_eval = EntityRecognitionEvaluator()
        self.answer_eval = AnswerQualityEvaluator()
        self.task_eval = BiologicalTaskEvaluator()

    def evaluate_full(self,
                      question_data: Dict,
                      agent_output: Dict,
                      method_name: str) -> EvaluationMetrics:
        """å®Œæ•´è¯„ä¼°ï¼ˆæ›´æ–°ç‰ˆï¼‰"""

        metrics = EvaluationMetrics()

        # D1: Adaptive Planning
        planning_metrics = self.planning_eval.evaluate(
            question_data, agent_output, method_name
        )
        metrics.depth_matching_accuracy = planning_metrics.get('depth_matching', 0.0)
        metrics.plan_coherence = planning_metrics.get('plan_coherence', 0.0)
        metrics.modality_coverage = planning_metrics.get('modality_coverage', 0.0)
        metrics.strategy_selection_accuracy = planning_metrics.get('strategy_selection', 0.0)
        metrics.closed_loop_achieved = planning_metrics.get('closed_loop', 0.0) >= 0.9

        # D2: Entity Recognition
        entity_metrics = self.entity_eval.evaluate(question_data, agent_output)
        metrics.entity_precision = entity_metrics['entity_precision']
        metrics.entity_recall = entity_metrics['entity_recall']
        metrics.entity_f1 = entity_metrics['entity_f1']

        # ğŸ†• Task Completionï¼ˆå¦‚æœæœ‰å®šä¹‰ï¼‰
        if question_data.get('task_type'):
            task_completion = self.task_eval.evaluate_task_completion(question_data, agent_output)
            # å­˜å‚¨åœ¨metricsä¸­ï¼ˆéœ€è¦æ·»åŠ å­—æ®µï¼‰
            if not hasattr(metrics, 'task_completion'):
                metrics.task_completion = task_completion

        # D4: Multi-Modal
        steps = agent_output.get('executed_steps', [])
        modalities = set(s.get('modality') for s in steps if s.get('modality'))
        metrics.modalities_used = list(modalities)

        # D5: Answer Quality
        answer_metrics = self.answer_eval.evaluate(question_data, agent_output)
        metrics.factual_accuracy = answer_metrics['factual_accuracy']
        metrics.answer_completeness = answer_metrics['answer_completeness']
        metrics.scientific_rigor = answer_metrics['scientific_rigor']

        # D6: Efficiency
        metrics.execution_time = agent_output.get('execution_time', 0.0)
        metrics.api_calls = len(steps)
        metrics.multi_hop_depth = len(steps)

        # Query success rate
        if steps:
            successful = sum(1 for s in steps if s.get('success', True))
            metrics.query_success_rate = successful / len(steps)
        else:
            metrics.query_success_rate = 1.0

        return metrics


class BiologicalTaskEvaluator:
    """
    ç”Ÿç‰©å­¦ä»»åŠ¡è¯„ä¼°å™¨

    è¯„ä¼°ä»»åŠ¡å®Œæˆåº¦ï¼š'completed', 'partial', 'failed'
    """

    def __init__(self):
        self.stopwords = self._build_stopwords()

    def _build_stopwords(self) -> set:
        """æ„å»ºåœç”¨è¯è¡¨"""
        return set([
            'what', 'which', 'where', 'when', 'who', 'why', 'how',
            'are', 'is', 'was', 'were', 'be', 'been', 'being',
            'do', 'does', 'did', 'have', 'has', 'had',
            'the', 'an', 'a', 'this', 'that',
        ])

    def evaluate_task_completion(self,
                                 question_data: Dict,
                                 agent_output: Dict) -> str:
        """
        è¯„ä¼°ä»»åŠ¡å®Œæˆåº¦

        Returns:
            'completed' | 'partial' | 'failed'
        """

        # è·å–successå’Œpartial criteria
        success_criteria = question_data.get('success_criteria', {})
        partial_criteria = question_data.get('partial_criteria', {})

        if not success_criteria:
            # å¦‚æœæ²¡æœ‰å®šä¹‰criteriaï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°
            return self._default_evaluation(question_data, agent_output)

        # æ£€æŸ¥success criteria
        success_checks = self._check_criteria(
            success_criteria,
            agent_output,
            question_data
        )

        if all(success_checks.values()):
            return 'completed'

        # æ£€æŸ¥partial criteria
        if partial_criteria:
            partial_checks = self._check_criteria(
                partial_criteria,
                agent_output,
                question_data
            )

            if all(partial_checks.values()):
                return 'partial'

        return 'failed'

    def _check_criteria(self,
                        criteria: Dict,
                        agent_output: Dict,
                        question_data: Dict) -> Dict[str, bool]:
        """æ£€æŸ¥æ ‡å‡†æ˜¯å¦æ»¡è¶³"""

        checks = {}

        answer = agent_output.get('answer', '')
        steps = agent_output.get('executed_steps', [])

        for criterion, requirement in criteria.items():

            if criterion == 'modalities_covered':
                # æ£€æŸ¥æ¨¡æ€è¦†ç›–
                checks[criterion] = self._check_modalities(requirement, steps)

            elif criterion == 'min_steps':
                # æ£€æŸ¥æœ€å°æ­¥æ•°
                checks[criterion] = len(steps) >= requirement

            elif criterion == 'closed_loop_required':
                # æ£€æŸ¥é—­ç¯
                if requirement:
                    checks[criterion] = self._check_closed_loop(steps)
                else:
                    checks[criterion] = True

            elif criterion == 'systematic_analysis':
                # æ£€æŸ¥ç³»ç»Ÿåˆ†æ
                if requirement:
                    checks[criterion] = self._check_systematic(steps, answer)
                else:
                    checks[criterion] = True

            elif criterion == 'min_regions_compared':
                # æ£€æŸ¥æ¯”è¾ƒçš„è„‘åŒºæ•°é‡
                checks[criterion] = self._check_regions_compared(answer, requirement)

            elif criterion == 'statistical_testing':
                # æ£€æŸ¥ç»Ÿè®¡æ£€éªŒ
                if requirement:
                    checks[criterion] = self._check_statistical_test(steps, answer)
                else:
                    checks[criterion] = True

            else:
                # æœªçŸ¥criterionï¼Œé»˜è®¤é€šè¿‡
                checks[criterion] = True

        return checks

    def _check_modalities(self, required_modalities: List[str], steps: List[Dict]) -> bool:
        """æ£€æŸ¥æ¨¡æ€è¦†ç›–"""
        modalities_used = set()

        for step in steps:
            modality = step.get('modality')
            if modality:
                modalities_used.add(modality)

        return set(required_modalities).issubset(modalities_used)

    def _check_closed_loop(self, steps: List[Dict]) -> bool:
        """æ£€æŸ¥æ˜¯å¦å®Œæˆé—­ç¯"""
        has_projection = False
        has_target_composition = False

        for step in steps:
            purpose = step.get('purpose', '').lower()
            modality = step.get('modality', '')

            if 'projection' in purpose or modality == 'projection':
                has_projection = True

            if ('target' in purpose and 'composition' in purpose) or \
                    ('target' in purpose and modality == 'molecular'):
                has_target_composition = True

        return has_projection and has_target_composition

    def _check_systematic(self, steps: List[Dict], answer: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†ç³»ç»Ÿåˆ†æ"""
        # æ£€æŸ¥æ­¥éª¤ä¸­æ˜¯å¦æœ‰comparative/systematicå…³é”®è¯
        for step in steps:
            purpose = step.get('purpose', '').lower()
            if any(kw in purpose for kw in ['compare', 'systematic', 'all', 'multiple', 'screen']):
                return True

        # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦æœ‰systematicåˆ†æçš„è¿¹è±¡
        answer_lower = answer.lower()
        if any(kw in answer_lower for kw in ['compared', 'across regions', 'systematic', 'all regions']):
            return True

        return False

    def _check_regions_compared(self, answer: str, min_count: int) -> bool:
        """æ£€æŸ¥æ¯”è¾ƒäº†å¤šå°‘ä¸ªè„‘åŒº"""
        import re

        # æå–è„‘åŒºç¼©å†™
        regions = re.findall(r'\b[A-Z]{2,5}\b', answer)

        # å»é‡
        unique_regions = set(regions)

        # è¿‡æ»¤æ‰å¸¸è§éè„‘åŒºè¯
        stopwords = {'DNA', 'RNA', 'ATP', 'GABA', 'LLM', 'ALL'}
        unique_regions -= stopwords

        return len(unique_regions) >= min_count

    def _check_statistical_test(self, steps: List[Dict], answer: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†ç»Ÿè®¡æ£€éªŒ"""
        # æ£€æŸ¥æ­¥éª¤ä¸­æ˜¯å¦æœ‰statisticalç±»å‹
        for step in steps:
            step_type = step.get('step_type', '')
            purpose = step.get('purpose', '').lower()

            if step_type == 'statistical' or \
                    any(kw in purpose for kw in ['statistic', 'test', 'fdr', 'p-value', 'significance']):
                return True

        # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦æåˆ°ç»Ÿè®¡æœ¯è¯­
        answer_lower = answer.lower()
        stat_terms = ['p-value', 'p value', 'statistical', 'significance', 'fdr', 't-test', 'anova']

        return any(term in answer_lower for term in stat_terms)

    def _default_evaluation(self, question_data: Dict, agent_output: Dict) -> str:
        """é»˜è®¤è¯„ä¼°æ–¹æ³•ï¼ˆå½“æ²¡æœ‰å®šä¹‰criteriaæ—¶ï¼‰"""

        # åŸºäºexpected_depthè¯„ä¼°
        expected_depth = question_data.get('expected_depth', 'medium')
        steps = agent_output.get('executed_steps', [])
        answer = agent_output.get('answer', '')

        if not agent_output.get('success', False):
            return 'failed'

        if len(answer) < 50:
            return 'failed'

        # ç®€å•çš„å¯å‘å¼
        if expected_depth == 'shallow':
            return 'completed' if len(steps) >= 1 else 'failed'

        elif expected_depth == 'medium':
            if len(steps) >= 2:
                return 'completed'
            elif len(steps) >= 1:
                return 'partial'
            else:
                return 'failed'

        else:  # deep
            if len(steps) >= 4:
                return 'completed'
            elif len(steps) >= 2:
                return 'partial'
            else:
                return 'failed'
# ==================== Test ====================

if __name__ == "__main__":
    print("Evaluators loaded successfully!")
    print("\nAvailable evaluators:")
    print("1. AdaptivePlanningEvaluator - D1 metrics")
    print("2. EntityRecognitionEvaluator - D2 metrics")
    print("3. AnswerQualityEvaluator - D5 metrics")
    print("4. ComprehensiveEvaluator - All metrics")