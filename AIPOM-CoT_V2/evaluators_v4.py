"""
Evaluation System for AIPOM-CoT Benchmark (v4.0 - Nature Methods)
==================================================================
å…¨é¢è¯„ä¼°ç³»ç»Ÿï¼Œè¯æ˜AIPOM-CoTçš„å‘è¡¨ä»·å€¼

New in v4.0:
- âœ… Planning Quality Evaluation (è§„åˆ’èƒ½åŠ›)
- âœ… Reasoning Capability Evaluation (æ¨ç†èƒ½åŠ›)
- âœ… Chain-of-Thought Quality (CoTè´¨é‡)
- âœ… Reflection Capability (åæ€èƒ½åŠ›)
- âœ… Natural Language Understanding (NLU)
- âœ… Biological Task Performance (ç”Ÿç‰©å­¦ä»»åŠ¡)

Author: Claude & PrometheusTT
Date: 2025-01-15
Version: 4.0 (Nature Methods Submission)
"""

import numpy as np
import logging
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import Counter

logger = logging.getLogger(__name__)

# ==================== Configuration ====================

EVALUATION_CONFIG = {
    # ğŸ”¬ Nature Methodsæ ¸å¿ƒèƒ½åŠ›ç»´åº¦
    'nm_core_dimensions': {
        'planning_quality': {
            'weight': 1.0,
            'methods': ['AIPOM-CoT', 'ReAct'],
            'description': 'Quality of adaptive planning'
        },
        'reasoning_capability': {
            'weight': 1.0,
            'methods': 'all',
            'description': 'Multi-hop reasoning and logical consistency'
        },
        'cot_quality': {
            'weight': 1.0,
            'methods': ['AIPOM-CoT', 'ReAct'],
            'description': 'Quality of chain-of-thought generation'
        },
        'reflection_capability': {
            'weight': 1.0,
            'methods': ['AIPOM-CoT', 'ReAct'],
            'description': 'Self-correction and error detection'
        },
        'nlu_capability': {
            'weight': 1.0,
            'methods': 'all',
            'description': 'Natural language understanding'
        },
    },

    # åŸæœ‰æ ¸å¿ƒæŒ‡æ ‡ï¼ˆä¿ç•™ï¼‰
    'core_metrics': {
        'entity_f1': {'weight': 1.0, 'methods': 'all'},
        'factual_accuracy': {'weight': 1.0, 'methods': 'all'},
        'answer_completeness': {'weight': 1.0, 'methods': 'all'},
        'scientific_rigor': {'weight': 1.0, 'methods': 'all'},
    },

    # ç³»ç»Ÿèƒ½åŠ›æŒ‡æ ‡ï¼ˆä¿ç•™ï¼‰
    'system_metrics': {
        'reasoning_depth': {'weight': 1.0, 'methods': 'all'},
        'modality_coverage': {'weight': 1.0, 'methods': ['AIPOM-CoT', 'Template-KG', 'RAG', 'ReAct']},
        'closed_loop': {'weight': 1.0, 'methods': ['AIPOM-CoT']},
    },

    # ğŸ”§ Nature Methodsæƒé‡é…ç½®
    'nm_method_weights': {
        'AIPOM-CoT': {
            # NMæ ¸å¿ƒèƒ½åŠ› (50%)
            'planning_quality': 0.10,
            'reasoning_capability': 0.10,
            'cot_quality': 0.10,
            'reflection_capability': 0.10,
            'nlu_capability': 0.10,
            # ä¼ ç»ŸæŒ‡æ ‡ (50%)
            'entity_f1': 0.10,
            'factual_accuracy': 0.10,
            'scientific_rigor': 0.10,
            'modality_coverage': 0.10,
            'closed_loop': 0.10,
        },
        'Direct GPT-4o': {
            'reasoning_capability': 0.25,
            'nlu_capability': 0.25,
            'entity_f1': 0.15,
            'factual_accuracy': 0.20,
            'scientific_rigor': 0.15,
        },
        'Template-KG': {
            'reasoning_capability': 0.20,
            'nlu_capability': 0.15,
            'entity_f1': 0.20,
            'factual_accuracy': 0.20,
            'modality_coverage': 0.15,
            'scientific_rigor': 0.10,
        },
        'RAG': {
            'reasoning_capability': 0.20,
            'nlu_capability': 0.20,
            'entity_f1': 0.20,
            'factual_accuracy': 0.20,
            'scientific_rigor': 0.20,
        },
        'ReAct': {
            'planning_quality': 0.15,
            'reasoning_capability': 0.15,
            'cot_quality': 0.10,
            'reflection_capability': 0.10,
            'nlu_capability': 0.10,
            'entity_f1': 0.12,
            'factual_accuracy': 0.13,
            'modality_coverage': 0.15,
        },
    },
}


# ==================== Data Structures ====================

@dataclass
class NMEvaluationMetrics:
    """Nature Methodsè¯„ä¼°æŒ‡æ ‡ (v4.0)"""

    # ğŸ”¬ NMæ ¸å¿ƒèƒ½åŠ›
    planning_quality: Optional[float] = None
    planning_coherence: Optional[float] = None
    planning_optimality: Optional[float] = None
    planning_adaptability: Optional[float] = None

    reasoning_capability: Optional[float] = None
    logical_consistency: Optional[float] = None
    evidence_integration: Optional[float] = None
    multi_hop_depth_score: Optional[float] = None

    cot_quality: Optional[float] = None
    cot_clarity: Optional[float] = None
    cot_completeness: Optional[float] = None
    intermediate_steps_quality: Optional[float] = None

    reflection_capability: Optional[float] = None
    error_detection: Optional[float] = None
    self_correction: Optional[float] = None
    iterative_refinement: Optional[float] = None

    nlu_capability: Optional[float] = None
    query_understanding: Optional[float] = None
    intent_recognition: Optional[float] = None
    ambiguity_resolution: Optional[float] = None

    # ä¼ ç»Ÿæ ¸å¿ƒæŒ‡æ ‡
    entity_precision: float = 0.0
    entity_recall: float = 0.0
    entity_f1: float = 0.0
    factual_accuracy: float = 0.0
    answer_completeness: float = 0.0
    scientific_rigor: float = 0.0

    # ç³»ç»Ÿèƒ½åŠ›
    reasoning_depth: Optional[float] = None
    modality_coverage: Optional[float] = None
    closed_loop_achieved: Optional[bool] = None
    modalities_used: List[str] = field(default_factory=list)

    # æ•ˆç‡
    execution_time: float = 0.0
    api_calls: int = 0
    query_success_rate: float = 0.0

    # ç”Ÿç‰©å­¦ä»»åŠ¡
    task_completion: Optional[str] = None
    biological_insight_score: Optional[float] = None

    # Overall
    overall_score: Optional[float] = None
    nm_capability_score: Optional[float] = None  # NMæ ¸å¿ƒèƒ½åŠ›æ€»åˆ†


# ==================== ğŸ”¬ Planning Quality Evaluator ====================

class PlanningQualityEvaluator:
    """
    è§„åˆ’èƒ½åŠ›è¯„ä¼°å™¨

    è¯„ä¼°ç»´åº¦ï¼š
    1. Planning Coherence - è®¡åˆ’è¿è´¯æ€§
    2. Planning Optimality - è®¡åˆ’æœ€ä¼˜æ€§
    3. Planning Adaptability - è®¡åˆ’é€‚åº”æ€§
    """

    def __init__(self):
        pass

    def evaluate(self, question_data: Dict, agent_output: Dict, method_name: str) -> Dict[str, float]:
        """è¯„ä¼°è§„åˆ’è´¨é‡"""

        # åªè¯„ä¼°æœ‰planningèƒ½åŠ›çš„æ–¹æ³•
        if method_name not in ['AIPOM-CoT', 'ReAct']:
            return {
                'planning_quality': None,
                'planning_coherence': None,
                'planning_optimality': None,
                'planning_adaptability': None,
            }

        executed_steps = agent_output.get('executed_steps', [])

        if len(executed_steps) < 2:
            return {
                'planning_quality': 0.5 if len(executed_steps) == 1 else 0.0,
                'planning_coherence': 0.5 if len(executed_steps) == 1 else 0.0,
                'planning_optimality': 0.5 if len(executed_steps) == 1 else 0.0,
                'planning_adaptability': 0.5 if len(executed_steps) == 1 else 0.0,
            }

        # 1. Planning Coherence
        coherence = self._evaluate_coherence(executed_steps)

        # 2. Planning Optimality
        optimality = self._evaluate_optimality(executed_steps, question_data)

        # 3. Planning Adaptability
        adaptability = self._evaluate_adaptability(executed_steps, question_data)

        # Overall planning quality
        planning_quality = np.mean([coherence, optimality, adaptability])

        return {
            'planning_quality': planning_quality,
            'planning_coherence': coherence,
            'planning_optimality': optimality,
            'planning_adaptability': adaptability,
        }

    def _evaluate_coherence(self, steps: List[Dict]) -> float:
        """
        è¯„ä¼°è®¡åˆ’è¿è´¯æ€§

        æ£€æŸ¥ï¼š
        - æ­¥éª¤é—´é€»è¾‘æµ
        - æ¨¡æ€æ¸è¿›æ€§
        - æ— é‡å¤æŸ¥è¯¢
        """

        if len(steps) < 2:
            return 1.0 if len(steps) == 1 else 0.0

        score = 0.0

        # 1. é€»è¾‘æµ (40%)
        modality_order = {'molecular': 1, 'morphological': 2, 'projection': 3, 'statistical': 4}

        flow_scores = []
        for i in range(len(steps) - 1):
            mod1 = steps[i].get('modality')
            mod2 = steps[i + 1].get('modality')

            if mod1 and mod2:
                order1 = modality_order.get(mod1, 2)
                order2 = modality_order.get(mod2, 2)

                if order2 >= order1:  # å…è®¸åŒçº§æˆ–é€’è¿›
                    flow_scores.append(1.0)
                elif order2 == order1 - 1:  # å°å¹…å›é€€
                    flow_scores.append(0.7)
                else:
                    flow_scores.append(0.4)
            else:
                flow_scores.append(0.6)

        score += (np.mean(flow_scores) if flow_scores else 0.5) * 0.4

        # 2. ç›®æ ‡å¯¼å‘æ€§ (30%)
        # æ£€æŸ¥æ¯ä¸ªæ­¥éª¤çš„purposeæ˜¯å¦æ˜ç¡®ä¸”ç›¸å…³
        purposes = [s.get('purpose', '') for s in steps]

        keywords = ['identify', 'find', 'analyze', 'compare', 'characterize', 'profile',
                    'discover', 'validate', 'quantify', 'retrieve']

        purpose_quality = sum(1 for p in purposes if any(kw in p.lower() for kw in keywords))
        score += (purpose_quality / len(purposes)) * 0.3

        # 3. æ— é‡å¤ (30%)
        unique_purposes = len(set(purposes))
        duplication_score = unique_purposes / len(purposes)
        score += duplication_score * 0.3

        return min(score, 1.0)

    def _evaluate_optimality(self, steps: List[Dict], question_data: Dict) -> float:
        """
        è¯„ä¼°è®¡åˆ’æœ€ä¼˜æ€§

        æ£€æŸ¥ï¼š
        - æ­¥æ•°æ˜¯å¦åœ¨åˆç†èŒƒå›´
        - æ˜¯å¦è¦†ç›–å¿…è¦æ¨¡æ€
        - æ˜¯å¦é¿å…ä¸å¿…è¦æ­¥éª¤
        """

        expected_range = question_data.get('expected_steps_range', (1, 10))
        expected_modalities = set(question_data.get('expected_modalities', []))

        actual_steps = len(steps)
        actual_modalities = set(s.get('modality') for s in steps if s.get('modality'))

        score = 0.0

        # 1. æ­¥æ•°åŒ¹é…åº¦ (40%)
        min_steps, max_steps = expected_range

        if min_steps <= actual_steps <= max_steps:
            step_score = 1.0
        elif actual_steps < min_steps:
            # ä¸è¶³
            step_score = max(0.3, actual_steps / min_steps)
        else:
            # è¿‡å¤š
            excess = actual_steps - max_steps
            step_score = max(0.3, 1.0 - (excess / max_steps) * 0.5)

        score += step_score * 0.4

        # 2. æ¨¡æ€è¦†ç›– (40%)
        if expected_modalities:
            covered = expected_modalities & actual_modalities
            modality_score = len(covered) / len(expected_modalities)
        else:
            modality_score = 0.8  # é»˜è®¤åˆ†

        score += modality_score * 0.4

        # 3. æ•ˆç‡ (20%) - æˆåŠŸç‡
        successful_steps = sum(1 for s in steps if s.get('success', True))
        efficiency = successful_steps / len(steps) if steps else 0

        score += efficiency * 0.2

        return min(score, 1.0)

    def _evaluate_adaptability(self, steps: List[Dict], question_data: Dict) -> float:
        """
        è¯„ä¼°è®¡åˆ’é€‚åº”æ€§

        æ£€æŸ¥ï¼š
        - æ¨¡æ€å¤šæ ·æ€§
        - ç­–ç•¥çµæ´»æ€§
        - é—®é¢˜å“åº”æ€§
        """

        score = 0.0

        # 1. æ¨¡æ€å¤šæ ·æ€§ (40%)
        modalities = set(s.get('modality') for s in steps if s.get('modality'))

        if len(modalities) >= 3:
            diversity_score = 1.0
        elif len(modalities) == 2:
            diversity_score = 0.7
        elif len(modalities) == 1:
            diversity_score = 0.4
        else:
            diversity_score = 0.0

        score += diversity_score * 0.4

        # 2. ç­–ç•¥å˜åŒ– (30%)
        # æ£€æŸ¥æ˜¯å¦æ ¹æ®ä¸­é—´ç»“æœè°ƒæ•´ç­–ç•¥
        purposes = [s.get('purpose', '').lower() for s in steps]

        # å¯»æ‰¾ç­–ç•¥è½¬æ¢çš„è¯æ®
        transitions = 0
        prev_type = None

        for purpose in purposes:
            if 'compare' in purpose or 'versus' in purpose:
                curr_type = 'comparative'
            elif 'all' in purpose or 'screen' in purpose:
                curr_type = 'screening'
            elif 'profile' in purpose or 'characterize' in purpose:
                curr_type = 'profiling'
            else:
                curr_type = 'retrieval'

            if prev_type and curr_type != prev_type:
                transitions += 1

            prev_type = curr_type

        adaptation_score = min(1.0, transitions / max(1, len(steps) - 1))
        score += adaptation_score * 0.3

        # 3. å¤æ‚åº¦åŒ¹é… (30%)
        expected_depth = question_data.get('expected_depth', 'medium')

        depth_map = {'shallow': 1, 'medium': 3, 'deep': 5}
        expected_min_steps = depth_map.get(expected_depth, 3)

        if len(steps) >= expected_min_steps:
            complexity_score = 1.0
        else:
            complexity_score = len(steps) / expected_min_steps

        score += complexity_score * 0.3

        return min(score, 1.0)


# ==================== ğŸ”¬ Reasoning Capability Evaluator ====================

class ReasoningCapabilityEvaluator:
    """
    æ¨ç†èƒ½åŠ›è¯„ä¼°å™¨

    è¯„ä¼°ç»´åº¦ï¼š
    1. Logical Consistency - é€»è¾‘ä¸€è‡´æ€§
    2. Evidence Integration - è¯æ®æ•´åˆ
    3. Multi-hop Depth - å¤šè·³æ¨ç†æ·±åº¦
    """

    def __init__(self):
        pass

    def evaluate(self, question_data: Dict, agent_output: Dict, method_name: str) -> Dict[str, float]:
        """è¯„ä¼°æ¨ç†èƒ½åŠ›"""

        answer = agent_output.get('answer', '')
        executed_steps = agent_output.get('executed_steps', [])

        # 1. Logical Consistency
        consistency = self._evaluate_logical_consistency(answer, executed_steps)

        # 2. Evidence Integration
        integration = self._evaluate_evidence_integration(answer, executed_steps)

        # 3. Multi-hop Depth Score
        depth_score = self._evaluate_depth_score(executed_steps)

        # Overall reasoning capability
        reasoning_capability = np.mean([consistency, integration, depth_score])

        return {
            'reasoning_capability': reasoning_capability,
            'logical_consistency': consistency,
            'evidence_integration': integration,
            'multi_hop_depth_score': depth_score,
        }

    def _evaluate_logical_consistency(self, answer: str, steps: List[Dict]) -> float:
        """
        è¯„ä¼°é€»è¾‘ä¸€è‡´æ€§

        æ£€æŸ¥ï¼š
        - ç­”æ¡ˆä¸æ­¥éª¤çš„ä¸€è‡´æ€§
        - æ— çŸ›ç›¾é™ˆè¿°
        - å› æœå…³ç³»åˆç†
        """

        if not answer or len(answer) < 20:
            return 0.0

        score = 0.0

        # 1. ç­”æ¡ˆå¼•ç”¨äº†æ­¥éª¤ä¸­çš„æ•°æ® (40%)
        # æå–æ­¥éª¤ä¸­çš„å…³é”®å®ä½“å’Œæ•°æ®
        step_entities = set()
        step_numbers = set()

        for step in steps:
            purpose = step.get('purpose', '')

            # æå–å®ä½“
            entities = re.findall(r'\b[A-Z][a-z]{2,8}\b', purpose)
            step_entities.update(entities)

            # æå–æ•°å­—
            numbers = re.findall(r'\d+', purpose)
            step_numbers.update(numbers)

        # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦å¼•ç”¨
        answer_lower = answer.lower()

        mentioned_entities = sum(1 for e in step_entities if e.lower() in answer_lower)
        entity_citation = mentioned_entities / len(step_entities) if step_entities else 0.5

        score += entity_citation * 0.4

        # 2. æ— çŸ›ç›¾æ ‡è®° (30%)
        contradiction_markers = ['however', 'but', 'although', 'nevertheless', 'on the other hand']

        # é€‚åº¦çš„è½¬æŠ˜æ˜¯å¥½çš„ï¼Œè¿‡å¤šå¯èƒ½è¡¨ç¤ºçŸ›ç›¾
        contradictions = sum(1 for marker in contradiction_markers if marker in answer_lower)

        if contradictions == 0:
            contradiction_score = 0.8  # å®Œå…¨æ— è½¬æŠ˜å¯èƒ½å¤ªç®€å•
        elif contradictions <= 2:
            contradiction_score = 1.0  # é€‚åº¦è½¬æŠ˜
        else:
            contradiction_score = max(0.3, 1.0 - (contradictions - 2) * 0.15)

        score += contradiction_score * 0.3

        # 3. ç»“æ„åŒ–æ¨ç† (30%)
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¸…æ™°çš„æ¨ç†ç»“æ„
        reasoning_markers = [
            'therefore', 'thus', 'hence', 'consequently', 'as a result',
            'because', 'since', 'due to', 'given that',
            'first', 'second', 'third', 'finally',
            'in addition', 'moreover', 'furthermore'
        ]

        reasoning_count = sum(1 for marker in reasoning_markers if marker in answer_lower)

        if reasoning_count >= 3:
            structure_score = 1.0
        elif reasoning_count >= 1:
            structure_score = 0.6 + reasoning_count * 0.13
        else:
            structure_score = 0.4

        score += structure_score * 0.3

        return min(score, 1.0)

    def _evaluate_evidence_integration(self, answer: str, steps: List[Dict]) -> float:
        """
        è¯„ä¼°è¯æ®æ•´åˆ

        æ£€æŸ¥ï¼š
        - å¤šæ­¥éª¤æ•°æ®æ•´åˆ
        - å®šé‡è¯æ®ä½¿ç”¨
        - è·¨æ¨¡æ€è¯æ®ç»¼åˆ
        """

        if not answer or len(steps) == 0:
            return 0.0

        score = 0.0

        # 1. å¤šæ­¥éª¤æ•´åˆ (40%)
        # å¦‚æœæœ‰å¤šä¸ªæ­¥éª¤ï¼Œç­”æ¡ˆåº”è¯¥æ•´åˆå¤šä¸ªæ­¥éª¤çš„ä¿¡æ¯
        if len(steps) >= 2:
            # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æåˆ°å¤šä¸ªæ¨¡æ€æˆ–å¤šä¸ªæ–¹é¢
            modalities_mentioned = 0

            if any(kw in answer.lower() for kw in ['marker', 'express', 'cluster', 'gene']):
                modalities_mentioned += 1
            if any(kw in answer.lower() for kw in ['morphology', 'axon', 'dendrite', 'branch']):
                modalities_mentioned += 1
            if any(kw in answer.lower() for kw in ['project', 'target', 'connect', 'pathway']):
                modalities_mentioned += 1

            integration_score = min(1.0, modalities_mentioned / 2)
        else:
            integration_score = 0.5

        score += integration_score * 0.4

        # 2. å®šé‡è¯æ® (30%)
        numbers = re.findall(r'\d+[,\d]*', answer)

        if len(numbers) >= 5:
            quantitative_score = 1.0
        elif len(numbers) >= 3:
            quantitative_score = 0.8
        elif len(numbers) >= 1:
            quantitative_score = 0.5
        else:
            quantitative_score = 0.2

        score += quantitative_score * 0.3

        # 3. è·¨æ¨¡æ€ç»¼åˆ (30%)
        modalities_used = set(s.get('modality') for s in steps if s.get('modality'))

        if len(modalities_used) >= 3:
            cross_modal_score = 1.0
        elif len(modalities_used) == 2:
            cross_modal_score = 0.7
        elif len(modalities_used) == 1:
            cross_modal_score = 0.4
        else:
            cross_modal_score = 0.0

        score += cross_modal_score * 0.3

        return min(score, 1.0)

    def _evaluate_depth_score(self, steps: List[Dict]) -> float:
        """
        è¯„ä¼°å¤šè·³æ¨ç†æ·±åº¦åˆ†æ•°

        å½’ä¸€åŒ–æ­¥æ•°åˆ°0-1åˆ†æ•°
        """

        num_steps = len(steps)

        if num_steps == 0:
            return 0.0
        elif num_steps == 1:
            return 0.3
        elif num_steps == 2:
            return 0.5
        elif num_steps == 3:
            return 0.65
        elif num_steps == 4:
            return 0.75
        elif num_steps == 5:
            return 0.85
        elif num_steps >= 6:
            return min(1.0, 0.85 + (num_steps - 5) * 0.03)

        return 0.0


# ==================== ğŸ”¬ CoT Quality Evaluator ====================

class CoTQualityEvaluator:
    """
    Chain-of-Thoughtè´¨é‡è¯„ä¼°å™¨

    è¯„ä¼°ç»´åº¦ï¼š
    1. CoT Clarity - æ¨ç†é“¾æ¸…æ™°åº¦
    2. CoT Completeness - æ¨ç†é“¾å®Œæ•´æ€§
    3. Intermediate Steps Quality - ä¸­é—´æ­¥éª¤è´¨é‡
    """

    def __init__(self):
        pass

    def evaluate(self, question_data: Dict, agent_output: Dict, method_name: str) -> Dict[str, float]:
        """è¯„ä¼°CoTè´¨é‡"""

        # åªè¯„ä¼°æœ‰CoTçš„æ–¹æ³•
        if method_name not in ['AIPOM-CoT', 'ReAct']:
            return {
                'cot_quality': None,
                'cot_clarity': None,
                'cot_completeness': None,
                'intermediate_steps_quality': None,
            }

        executed_steps = agent_output.get('executed_steps', [])
        answer = agent_output.get('answer', '')

        # 1. CoT Clarity
        clarity = self._evaluate_clarity(executed_steps)

        # 2. CoT Completeness
        completeness = self._evaluate_completeness(executed_steps, question_data)

        # 3. Intermediate Steps Quality
        steps_quality = self._evaluate_steps_quality(executed_steps)

        # Overall CoT quality
        cot_quality = np.mean([clarity, completeness, steps_quality])

        return {
            'cot_quality': cot_quality,
            'cot_clarity': clarity,
            'cot_completeness': completeness,
            'intermediate_steps_quality': steps_quality,
        }

    def _evaluate_clarity(self, steps: List[Dict]) -> float:
        """
        è¯„ä¼°æ¨ç†é“¾æ¸…æ™°åº¦

        æ£€æŸ¥ï¼š
        - æ¯æ­¥ç›®æ ‡æ˜ç¡®
        - æ­¥éª¤æè¿°æ¸…æ™°
        - æ— æ­§ä¹‰
        """

        if not steps:
            return 0.0

        score = 0.0

        # 1. ç›®æ ‡æ˜ç¡®æ€§ (40%)
        clear_purposes = 0

        for step in steps:
            purpose = step.get('purpose', '').lower()

            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„åŠ¨è¯
            action_verbs = ['identify', 'find', 'retrieve', 'analyze', 'compare',
                            'characterize', 'profile', 'discover', 'validate']

            if any(verb in purpose for verb in action_verbs):
                clear_purposes += 1

        score += (clear_purposes / len(steps)) * 0.4

        # 2. æè¿°è¯¦ç»†åº¦ (30%)
        avg_length = np.mean([len(s.get('purpose', '')) for s in steps])

        if avg_length >= 50:
            detail_score = 1.0
        elif avg_length >= 30:
            detail_score = 0.8
        elif avg_length >= 15:
            detail_score = 0.6
        else:
            detail_score = 0.3

        score += detail_score * 0.3

        # 3. ç»“æ„ä¸€è‡´æ€§ (30%)
        # æ£€æŸ¥æ‰€æœ‰æ­¥éª¤æ˜¯å¦æœ‰ä¸€è‡´çš„æ ¼å¼
        has_modality = sum(1 for s in steps if s.get('modality'))
        has_purpose = sum(1 for s in steps if s.get('purpose'))

        consistency_score = (has_modality + has_purpose) / (2 * len(steps))
        score += consistency_score * 0.3

        return min(score, 1.0)

    def _evaluate_completeness(self, steps: List[Dict], question_data: Dict) -> float:
        """
        è¯„ä¼°æ¨ç†é“¾å®Œæ•´æ€§

        æ£€æŸ¥ï¼š
        - è¦†ç›–é—®é¢˜æ‰€éœ€æ¨¡æ€
        - æ­¥éª¤è¿è´¯æ— è·³è·ƒ
        - è¾¾åˆ°é¢„æœŸæ·±åº¦
        """

        if not steps:
            return 0.0

        score = 0.0

        # 1. æ¨¡æ€è¦†ç›– (40%)
        expected_modalities = set(question_data.get('expected_modalities', []))
        actual_modalities = set(s.get('modality') for s in steps if s.get('modality'))

        if expected_modalities:
            coverage = len(expected_modalities & actual_modalities) / len(expected_modalities)
        else:
            coverage = 0.7  # é»˜è®¤

        score += coverage * 0.4

        # 2. æ­¥éª¤è¿è´¯æ€§ (30%)
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é€»è¾‘è·³è·ƒ
        gap_count = 0

        for i in range(len(steps) - 1):
            mod1 = steps[i].get('modality', '')
            mod2 = steps[i + 1].get('modality', '')

            # ä»molecularç›´æ¥è·³åˆ°projectionï¼ˆè·³è¿‡morphologicalï¼‰å¯èƒ½æ˜¯è·³è·ƒ
            if mod1 == 'molecular' and mod2 == 'projection':
                gap_count += 1

        if len(steps) > 1:
            coherence = 1.0 - (gap_count / (len(steps) - 1))
        else:
            coherence = 1.0

        score += coherence * 0.3

        # 3. æ·±åº¦å……è¶³æ€§ (30%)
        expected_range = question_data.get('expected_steps_range', (1, 10))
        min_steps = expected_range[0]

        if len(steps) >= min_steps:
            depth_score = 1.0
        else:
            depth_score = len(steps) / min_steps

        score += depth_score * 0.3

        return min(score, 1.0)

    def _evaluate_steps_quality(self, steps: List[Dict]) -> float:
        """
        è¯„ä¼°ä¸­é—´æ­¥éª¤è´¨é‡

        æ£€æŸ¥ï¼š
        - æ­¥éª¤æˆåŠŸç‡
        - æ­¥éª¤ä¿¡æ¯é‡
        - æ­¥éª¤ä»·å€¼
        """

        if not steps:
            return 0.0

        score = 0.0

        # 1. æˆåŠŸç‡ (40%)
        successful = sum(1 for s in steps if s.get('success', True))
        success_rate = successful / len(steps)

        score += success_rate * 0.4

        # 2. ä¿¡æ¯é‡ (30%)
        # æ£€æŸ¥æ­¥éª¤æ˜¯å¦äº§ç”Ÿäº†æœ‰ä»·å€¼çš„ä¿¡æ¯
        purposes = [s.get('purpose', '') for s in steps]
        avg_informativeness = np.mean([len(p.split()) for p in purposes])

        if avg_informativeness >= 8:
            info_score = 1.0
        elif avg_informativeness >= 5:
            info_score = 0.7
        elif avg_informativeness >= 3:
            info_score = 0.5
        else:
            info_score = 0.3

        score += info_score * 0.3

        # 3. æ­¥éª¤ä»·å€¼ (30%)
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æˆ–æ— ç”¨æ­¥éª¤
        unique_purposes = len(set(purposes))
        value_score = unique_purposes / len(purposes)

        score += value_score * 0.3

        return min(score, 1.0)


# ==================== ğŸ”¬ Reflection Capability Evaluator ====================

class ReflectionCapabilityEvaluator:
    """
    åæ€èƒ½åŠ›è¯„ä¼°å™¨

    è¯„ä¼°ç»´åº¦ï¼š
    1. Error Detection - é”™è¯¯æ£€æµ‹
    2. Self-Correction - è‡ªæˆ‘çº æ­£
    3. Iterative Refinement - è¿­ä»£ä¼˜åŒ–
    """

    def __init__(self):
        pass

    def evaluate(self, question_data: Dict, agent_output: Dict, method_name: str) -> Dict[str, float]:
        """è¯„ä¼°åæ€èƒ½åŠ›"""

        # åªè¯„ä¼°æœ‰reflectionèƒ½åŠ›çš„æ–¹æ³•
        if method_name not in ['AIPOM-CoT', 'ReAct']:
            return {
                'reflection_capability': None,
                'error_detection': None,
                'self_correction': None,
                'iterative_refinement': None,
            }

        executed_steps = agent_output.get('executed_steps', [])
        answer = agent_output.get('answer', '')

        # 1. Error Detection
        detection = self._evaluate_error_detection(executed_steps)

        # 2. Self-Correction
        correction = self._evaluate_self_correction(executed_steps, answer)

        # 3. Iterative Refinement
        refinement = self._evaluate_iterative_refinement(executed_steps)

        # Overall reflection capability
        reflection_capability = np.mean([detection, correction, refinement])

        return {
            'reflection_capability': reflection_capability,
            'error_detection': detection,
            'self_correction': correction,
            'iterative_refinement': refinement,
        }

    def _evaluate_error_detection(self, steps: List[Dict]) -> float:
        """
        è¯„ä¼°é”™è¯¯æ£€æµ‹èƒ½åŠ›

        æ£€æŸ¥ï¼š
        - å¤±è´¥æ­¥éª¤çš„è¯†åˆ«
        - é—®é¢˜è¯Šæ–­
        - æ›¿ä»£æ–¹æ¡ˆ
        """

        if not steps:
            return 0.0

        score = 0.0

        # 1. å¤±è´¥è¯†åˆ« (40%)
        failed_steps = [s for s in steps if not s.get('success', True)]

        if len(failed_steps) == 0:
            # æ— å¤±è´¥ - å¯èƒ½æ˜¯å¥½çš„ï¼Œä¹Ÿå¯èƒ½ç¼ºä¹æŒ‘æˆ˜
            detection_score = 0.7
        else:
            # æœ‰å¤±è´¥ä½†ç»§ç»­æ‰§è¡Œ - è¯´æ˜æ£€æµ‹åˆ°äº†
            detection_score = 1.0

        score += detection_score * 0.4

        # 2. é—®é¢˜è¯Šæ–­ (30%)
        # æ£€æŸ¥åç»­æ­¥éª¤æ˜¯å¦è°ƒæ•´äº†ç­–ç•¥
        if len(steps) >= 3:
            modalities = [s.get('modality') for s in steps]

            # æ£€æŸ¥æ˜¯å¦æœ‰ç­–ç•¥å˜åŒ–
            changes = 0
            for i in range(len(modalities) - 1):
                if modalities[i] != modalities[i + 1]:
                    changes += 1

            diagnosis_score = min(1.0, changes / (len(steps) - 1) * 2)
        else:
            diagnosis_score = 0.5

        score += diagnosis_score * 0.3

        # 3. æ¢å¤èƒ½åŠ› (30%)
        # æ£€æŸ¥å¤±è´¥åæ˜¯å¦æœ‰æˆåŠŸæ­¥éª¤
        if failed_steps and len(steps) > len(failed_steps):
            # æœ‰å¤±è´¥ï¼Œä½†æ•´ä½“å®Œæˆäº†
            recovery_score = 1.0
        elif not failed_steps:
            recovery_score = 0.8
        else:
            recovery_score = 0.3

        score += recovery_score * 0.3

        return min(score, 1.0)

    def _evaluate_self_correction(self, steps: List[Dict], answer: str) -> float:
        """
        è¯„ä¼°è‡ªæˆ‘çº æ­£èƒ½åŠ›

        æ£€æŸ¥ï¼š
        - ç­”æ¡ˆä¸­æ‰¿è®¤ä¸ç¡®å®šæ€§
        - æä¾›æ›¿ä»£è§£é‡Š
        - è°¨æ…æªè¾
        """

        if not answer:
            return 0.0

        score = 0.0

        answer_lower = answer.lower()

        # 1. ä¸ç¡®å®šæ€§è¡¨è¾¾ (40%)
        uncertainty_markers = [
            'may', 'might', 'could', 'possibly', 'likely',
            'suggests', 'indicates', 'appears', 'seems',
            'approximately', 'around', 'about'
        ]

        uncertainty_count = sum(1 for marker in uncertainty_markers if marker in answer_lower)

        if 2 <= uncertainty_count <= 5:
            uncertainty_score = 1.0  # é€‚åº¦çš„ä¸ç¡®å®šæ€§
        elif uncertainty_count == 1 or uncertainty_count == 6:
            uncertainty_score = 0.7
        elif uncertainty_count == 0:
            uncertainty_score = 0.4  # è¿‡äºç¡®å®šå¯èƒ½ä¸å¥½
        else:
            uncertainty_score = 0.5  # è¿‡å¤šä¸ç¡®å®šæ€§

        score += uncertainty_score * 0.4

        # 2. æ›¿ä»£è§£é‡Š (30%)
        alternative_markers = [
            'alternatively', 'another', 'also', 'additionally',
            'or', 'either', 'different'
        ]

        alternative_count = sum(1 for marker in alternative_markers if marker in answer_lower)

        if alternative_count >= 2:
            alternative_score = 1.0
        elif alternative_count == 1:
            alternative_score = 0.7
        else:
            alternative_score = 0.4

        score += alternative_score * 0.3

        # 3. è°¨æ…æªè¾ (30%)
        # æ£€æŸ¥æ˜¯å¦é¿å…ç»å¯¹åŒ–é™ˆè¿°
        absolute_markers = ['always', 'never', 'all', 'none', 'every', 'must', 'definitely']

        absolute_count = sum(1 for marker in absolute_markers if marker in answer_lower)

        if absolute_count == 0:
            caution_score = 1.0
        elif absolute_count <= 2:
            caution_score = 0.6
        else:
            caution_score = 0.3

        score += caution_score * 0.3

        return min(score, 1.0)

    def _evaluate_iterative_refinement(self, steps: List[Dict]) -> float:
        """
        è¯„ä¼°è¿­ä»£ä¼˜åŒ–èƒ½åŠ›

        æ£€æŸ¥ï¼š
        - æ­¥éª¤æ¸è¿›æ€§
        - ç­–ç•¥è°ƒæ•´
        - ç›®æ ‡èšç„¦
        """

        if len(steps) < 2:
            return 0.5 if len(steps) == 1 else 0.0

        score = 0.0

        # 1. æ¸è¿›æ€§ (40%)
        # æ£€æŸ¥æ­¥éª¤æ˜¯å¦é€æ­¥æ·±å…¥
        modalities = [s.get('modality') for s in steps if s.get('modality')]

        if len(modalities) >= 2:
            # æ£€æŸ¥æ˜¯å¦ä»ç®€å•åˆ°å¤æ‚
            modality_order = {'molecular': 1, 'morphological': 2, 'projection': 3}

            progressions = 0
            for i in range(len(modalities) - 1):
                order1 = modality_order.get(modalities[i], 2)
                order2 = modality_order.get(modalities[i + 1], 2)

                if order2 >= order1:
                    progressions += 1

            progressive_score = progressions / (len(modalities) - 1)
        else:
            progressive_score = 0.5

        score += progressive_score * 0.4

        # 2. ç­–ç•¥è°ƒæ•´ (30%)
        # æ£€æŸ¥æ˜¯å¦æ ¹æ®ç»“æœè°ƒæ•´
        purposes = [s.get('purpose', '').lower() for s in steps]

        # ç»Ÿè®¡ä¸åŒç±»å‹çš„ç›®æ ‡
        types = []
        for purpose in purposes:
            if 'compare' in purpose:
                types.append('compare')
            elif 'find' in purpose or 'identify' in purpose:
                types.append('find')
            elif 'analyze' in purpose or 'characterize' in purpose:
                types.append('analyze')
            else:
                types.append('other')

        unique_types = len(set(types))

        if unique_types >= 2:
            adjustment_score = 1.0
        elif unique_types == 1:
            adjustment_score = 0.5
        else:
            adjustment_score = 0.3

        score += adjustment_score * 0.3

        # 3. ç›®æ ‡èšç„¦ (30%)
        # æ£€æŸ¥åç»­æ­¥éª¤æ˜¯å¦æ›´åŠ èšç„¦
        # é€šè¿‡æ­¥éª¤æè¿°çš„å…·ä½“æ€§å˜åŒ–æ¥è¯„ä¼°
        specificities = []

        for purpose in purposes:
            # å…·ä½“æ€§ = ä¸“ä¸šæœ¯è¯­æ•°é‡ / æ€»è¯æ•°
            words = purpose.split()
            specific_terms = sum(1 for w in words if len(w) > 6 or w[0].isupper())
            specificity = specific_terms / len(words) if words else 0
            specificities.append(specificity)

        if len(specificities) >= 2:
            # æ£€æŸ¥æ˜¯å¦é€’å¢
            increasing = sum(1 for i in range(len(specificities) - 1) if specificities[i + 1] >= specificities[i])
            focus_score = increasing / (len(specificities) - 1)
        else:
            focus_score = 0.5

        score += focus_score * 0.3

        return min(score, 1.0)


# ==================== ğŸ”¬ NLU Capability Evaluator ====================

class NLUCapabilityEvaluator:
    """
    è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›è¯„ä¼°å™¨

    è¯„ä¼°ç»´åº¦ï¼š
    1. Query Understanding - é—®é¢˜ç†è§£
    2. Intent Recognition - æ„å›¾è¯†åˆ«
    3. Ambiguity Resolution - æ­§ä¹‰è§£æ
    """

    def __init__(self):
        pass

    def evaluate(self, question_data: Dict, agent_output: Dict, method_name: str) -> Dict[str, float]:
        """è¯„ä¼°NLUèƒ½åŠ›"""

        question = question_data.get('question', '')
        executed_steps = agent_output.get('executed_steps', [])
        entities_recognized = agent_output.get('entities_recognized', [])
        answer = agent_output.get('answer', '')

        # 1. Query Understanding
        understanding = self._evaluate_query_understanding(question, executed_steps, entities_recognized)

        # 2. Intent Recognition
        intent = self._evaluate_intent_recognition(question_data, executed_steps)

        # 3. Ambiguity Resolution
        ambiguity = self._evaluate_ambiguity_resolution(question, answer)

        # Overall NLU capability
        nlu_capability = np.mean([understanding, intent, ambiguity])

        return {
            'nlu_capability': nlu_capability,
            'query_understanding': understanding,
            'intent_recognition': intent,
            'ambiguity_resolution': ambiguity,
        }

    def _evaluate_query_understanding(self, question: str, steps: List[Dict], entities: List) -> float:
        """
        è¯„ä¼°é—®é¢˜ç†è§£

        æ£€æŸ¥ï¼š
        - å…³é”®å®ä½“è¯†åˆ«
        - é—®é¢˜ç„¦ç‚¹æŠŠæ¡
        - å¿…è¦ä¿¡æ¯æå–
        """

        if not question:
            return 0.0

        score = 0.0

        # 1. å®ä½“è¯†åˆ«å‡†ç¡®æ€§ (40%)
        # æå–é—®é¢˜ä¸­çš„å®ä½“
        question_entities = set()

        # åŸºå› /è›‹ç™½
        genes = re.findall(r'\b([A-Z][a-z]{2,8})\+?', question)
        question_entities.update(g for g in genes if g not in {'What', 'Which', 'Where', 'Tell'})

        # è„‘åŒº
        regions = re.findall(r'\b([A-Z]{2,5})\b', question)
        known_regions = {'MOp', 'MOs', 'SSp', 'VISp', 'AUDp', 'ACA', 'CLA', 'RSP', 'TH'}
        question_entities.update(r for r in regions if r in known_regions)

        # è¯†åˆ«çš„å®ä½“
        recognized = set()
        for entity in entities:
            if isinstance(entity, dict):
                recognized.add(entity.get('text', '').lower())
            else:
                recognized.add(str(entity).lower())

        question_entities_lower = set(e.lower() for e in question_entities)

        if question_entities_lower:
            entity_accuracy = len(question_entities_lower & recognized) / len(question_entities_lower)
        else:
            entity_accuracy = 0.7  # æ²¡æœ‰æ˜æ˜¾å®ä½“ï¼Œç»™é»˜è®¤åˆ†

        score += entity_accuracy * 0.4

        # 2. ç„¦ç‚¹æŠŠæ¡ (30%)
        # æ£€æŸ¥ç¬¬ä¸€æ­¥æ˜¯å¦é’ˆå¯¹ä¸»è¦é—®é¢˜
        if steps:
            first_purpose = steps[0].get('purpose', '').lower()

            # æå–é—®é¢˜å…³é”®è¯
            question_lower = question.lower()

            keywords = []
            if 'profile' in question_lower or 'characterize' in question_lower or 'about' in question_lower:
                keywords.extend(['profile', 'characterize', 'analyze'])
            if 'compare' in question_lower or 'versus' in question_lower:
                keywords.extend(['compare', 'versus'])
            if 'project' in question_lower or 'target' in question_lower:
                keywords.extend(['project', 'target', 'connect'])

            if keywords:
                focus_match = any(kw in first_purpose for kw in keywords)
                focus_score = 1.0 if focus_match else 0.5
            else:
                focus_score = 0.7

        else:
            focus_score = 0.0

        score += focus_score * 0.3

        # 3. æ‰§è¡ŒåŒ¹é… (30%)
        # æ£€æŸ¥æ‰§è¡Œçš„æ­¥éª¤æ˜¯å¦ç¬¦åˆé—®é¢˜éœ€æ±‚
        expected_modalities = self._infer_expected_modalities(question)
        actual_modalities = set(s.get('modality') for s in steps if s.get('modality'))

        if expected_modalities:
            execution_match = len(expected_modalities & actual_modalities) / len(expected_modalities)
        else:
            execution_match = 0.7

        score += execution_match * 0.3

        return min(score, 1.0)

    def _evaluate_intent_recognition(self, question_data: Dict, steps: List[Dict]) -> float:
        """
        è¯„ä¼°æ„å›¾è¯†åˆ«

        æ£€æŸ¥ï¼š
        - ç­–ç•¥é€‰æ‹©æ­£ç¡®æ€§
        - ä»»åŠ¡ç±»å‹è¯†åˆ«
        - æ·±åº¦åŒ¹é…
        """

        score = 0.0

        # 1. ç­–ç•¥é€‰æ‹© (40%)
        expected_strategy = question_data.get('expected_strategy', 'adaptive')

        # ä»æ­¥éª¤æ¨æ–­å®é™…ç­–ç•¥
        if not steps:
            inferred_strategy = 'none'
        else:
            modalities = set(s.get('modality') for s in steps if s.get('modality'))
            purposes = [s.get('purpose', '').lower() for s in steps]

            if any('compare' in p for p in purposes):
                inferred_strategy = 'comparative'
            elif len(modalities) >= 3:
                inferred_strategy = 'focus_driven'
            elif len(steps) >= 5:
                inferred_strategy = 'screening'
            else:
                inferred_strategy = 'adaptive'

        if inferred_strategy == expected_strategy:
            strategy_score = 1.0
        elif expected_strategy == 'adaptive':
            strategy_score = 0.8  # adaptiveå…è®¸å„ç§ç­–ç•¥
        else:
            strategy_score = 0.5

        score += strategy_score * 0.4

        # 2. ä»»åŠ¡ç±»å‹è¯†åˆ« (30%)
        task_type = question_data.get('task_type')

        if task_type:
            # æ£€æŸ¥æ­¥éª¤æ˜¯å¦åŒ¹é…ä»»åŠ¡ç±»å‹
            if task_type == 'profiling':
                # éœ€è¦å¤šæ¨¡æ€
                correct = len(set(s.get('modality') for s in steps if s.get('modality'))) >= 2
            elif task_type == 'discovery':
                # éœ€è¦ç³»ç»Ÿåˆ†æ
                correct = len(steps) >= 3
            elif task_type == 'validation':
                # éœ€è¦æ¯”è¾ƒ
                correct = any('compare' in s.get('purpose', '').lower() for s in steps)
            else:  # lookup
                correct = len(steps) >= 1

            task_score = 1.0 if correct else 0.5
        else:
            task_score = 0.7

        score += task_score * 0.3

        # 3. æ·±åº¦åŒ¹é… (30%)
        expected_depth = question_data.get('expected_depth', 'medium')
        actual_steps = len(steps)

        depth_map = {'shallow': 1, 'medium': 3, 'deep': 5}
        expected_min = depth_map.get(expected_depth, 3)

        if actual_steps >= expected_min:
            depth_score = 1.0
        else:
            depth_score = actual_steps / expected_min

        score += depth_score * 0.3

        return min(score, 1.0)

    def _evaluate_ambiguity_resolution(self, question: str, answer: str) -> float:
        """
        è¯„ä¼°æ­§ä¹‰è§£æ

        æ£€æŸ¥ï¼š
        - å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢
        - æ¾„æ¸…å‡è®¾
        - æä¾›å®Œæ•´ç­”æ¡ˆ
        """

        if not question or not answer:
            return 0.0

        score = 0.0

        # 1. è¯†åˆ«æ­§ä¹‰ (40%)
        # æ£€æŸ¥é—®é¢˜æ˜¯å¦æ¨¡ç³Š
        ambiguous_patterns = [
            r'\btell me about\b',
            r'\bwhat are\b',
            r'\bhow many\b',
            r'\bcompare\b',
        ]

        is_ambiguous = any(re.search(pattern, question.lower()) for pattern in ambiguous_patterns)

        if is_ambiguous:
            # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ¾„æ¸…äº†å‡è®¾
            clarification_markers = [
                'specifically', 'in particular', 'focusing on', 'considering',
                'based on', 'regarding', 'with respect to'
            ]

            has_clarification = any(marker in answer.lower() for marker in clarification_markers)

            ambiguity_score = 1.0 if has_clarification else 0.6
        else:
            ambiguity_score = 0.8  # ä¸æ¨¡ç³Šï¼Œç»™é»˜è®¤é«˜åˆ†

        score += ambiguity_score * 0.4

        # 2. å‡è®¾è¯´æ˜ (30%)
        assumption_markers = [
            'assuming', 'given', 'if', 'when', 'provided that',
            'under the condition', 'in the case'
        ]

        has_assumptions = any(marker in answer.lower() for marker in assumption_markers)

        if has_assumptions:
            assumption_score = 1.0
        else:
            assumption_score = 0.7

        score += assumption_score * 0.3

        # 3. ç­”æ¡ˆå®Œæ•´æ€§ (30%)
        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å…¨é¢
        word_count = len(answer.split())

        if word_count >= 100:
            completeness_score = 1.0
        elif word_count >= 50:
            completeness_score = 0.8
        elif word_count >= 20:
            completeness_score = 0.5
        else:
            completeness_score = 0.3

        score += completeness_score * 0.3

        return min(score, 1.0)

    def _infer_expected_modalities(self, question: str) -> set:
        """ä»é—®é¢˜æ¨æ–­é¢„æœŸæ¨¡æ€"""

        expected = set()
        question_lower = question.lower()

        if any(kw in question_lower for kw in ['marker', 'express', 'cluster', 'cell type', 'gene']):
            expected.add('molecular')

        if any(kw in question_lower for kw in ['morphology', 'axon', 'dendrite', 'branch', 'length']):
            expected.add('morphological')

        if any(kw in question_lower for kw in ['project', 'target', 'connect', 'pathway', 'circuit']):
            expected.add('projection')

        return expected


# ==================== Comprehensive Evaluator (v4.0) ====================

class ComprehensiveEvaluatorV4:
    """ç»¼åˆè¯„ä¼°å™¨ v4.0 - Nature Methods"""

    def __init__(self):
        # åˆå§‹åŒ–æ‰€æœ‰è¯„ä¼°å™¨
        self.planning_eval = PlanningQualityEvaluator()
        self.reasoning_eval = ReasoningCapabilityEvaluator()
        self.cot_eval = CoTQualityEvaluator()
        self.reflection_eval = ReflectionCapabilityEvaluator()
        self.nlu_eval = NLUCapabilityEvaluator()

        # ä¿ç•™åŸæœ‰è¯„ä¼°å™¨
        from evaluators import (
            AdaptivePlanningEvaluator,
            EntityRecognitionEvaluator,
            AnswerQualityEvaluator,
            BiologicalTaskEvaluator
        )

        self.adaptive_eval = AdaptivePlanningEvaluator()
        self.entity_eval = EntityRecognitionEvaluator()
        self.answer_eval = AnswerQualityEvaluator()
        self.task_eval = BiologicalTaskEvaluator()

        self.config = EVALUATION_CONFIG

    def evaluate_full(self,
                      question_data: Dict,
                      agent_output: Dict,
                      method_name: str) -> NMEvaluationMetrics:
        """å®Œæ•´è¯„ä¼° (v4.0)"""

        metrics = NMEvaluationMetrics()

        # ğŸ”¬ NMæ ¸å¿ƒèƒ½åŠ›è¯„ä¼°

        # 1. Planning Quality
        planning_metrics = self.planning_eval.evaluate(question_data, agent_output, method_name)
        metrics.planning_quality = planning_metrics['planning_quality']
        metrics.planning_coherence = planning_metrics['planning_coherence']
        metrics.planning_optimality = planning_metrics['planning_optimality']
        metrics.planning_adaptability = planning_metrics['planning_adaptability']

        # 2. Reasoning Capability
        reasoning_metrics = self.reasoning_eval.evaluate(question_data, agent_output, method_name)
        metrics.reasoning_capability = reasoning_metrics['reasoning_capability']
        metrics.logical_consistency = reasoning_metrics['logical_consistency']
        metrics.evidence_integration = reasoning_metrics['evidence_integration']
        metrics.multi_hop_depth_score = reasoning_metrics['multi_hop_depth_score']

        # 3. CoT Quality
        cot_metrics = self.cot_eval.evaluate(question_data, agent_output, method_name)
        metrics.cot_quality = cot_metrics['cot_quality']
        metrics.cot_clarity = cot_metrics['cot_clarity']
        metrics.cot_completeness = cot_metrics['cot_completeness']
        metrics.intermediate_steps_quality = cot_metrics['intermediate_steps_quality']

        # 4. Reflection Capability
        reflection_metrics = self.reflection_eval.evaluate(question_data, agent_output, method_name)
        metrics.reflection_capability = reflection_metrics['reflection_capability']
        metrics.error_detection = reflection_metrics['error_detection']
        metrics.self_correction = reflection_metrics['self_correction']
        metrics.iterative_refinement = reflection_metrics['iterative_refinement']

        # 5. NLU Capability
        nlu_metrics = self.nlu_eval.evaluate(question_data, agent_output, method_name)
        metrics.nlu_capability = nlu_metrics['nlu_capability']
        metrics.query_understanding = nlu_metrics['query_understanding']
        metrics.intent_recognition = nlu_metrics['intent_recognition']
        metrics.ambiguity_resolution = nlu_metrics['ambiguity_resolution']

        # ä¼ ç»ŸæŒ‡æ ‡

        # Entity Recognition
        entity_metrics = self.entity_eval.evaluate(question_data, agent_output)
        metrics.entity_precision = entity_metrics['entity_precision']
        metrics.entity_recall = entity_metrics['entity_recall']
        metrics.entity_f1 = entity_metrics['entity_f1']

        # Answer Quality
        answer_metrics = self.answer_eval.evaluate(question_data, agent_output)
        metrics.factual_accuracy = answer_metrics['factual_accuracy']
        metrics.answer_completeness = answer_metrics['answer_completeness']
        metrics.scientific_rigor = answer_metrics['scientific_rigor']

        # System capabilities
        adaptive_metrics = self.adaptive_eval.evaluate(question_data, agent_output, method_name)
        metrics.reasoning_depth = adaptive_metrics.get('reasoning_depth')
        metrics.modality_coverage = adaptive_metrics.get('modality_coverage')

        closed_loop_score = adaptive_metrics.get('closed_loop')
        if closed_loop_score is not None:
            metrics.closed_loop_achieved = closed_loop_score >= 0.9
        else:
            metrics.closed_loop_achieved = None

        # Efficiency
        steps = agent_output.get('executed_steps', [])
        metrics.execution_time = agent_output.get('execution_time', 0.0)
        metrics.api_calls = len(steps)

        if steps:
            successful = sum(1 for s in steps if s.get('success', True))
            metrics.query_success_rate = successful / len(steps)
        else:
            metrics.query_success_rate = 1.0

        modalities = set(s.get('modality') for s in steps if s.get('modality'))
        metrics.modalities_used = list(modalities)

        # Task Completion
        if question_data.get('task_type'):
            metrics.task_completion = self.task_eval.evaluate_task_completion(
                question_data, agent_output
            )

        # Biological Insight Score
        metrics.biological_insight_score = self._evaluate_biological_insight(
            question_data, agent_output, metrics
        )

        # Overall Scores
        metrics.overall_score = self._calculate_overall_score(metrics, method_name)
        metrics.nm_capability_score = self._calculate_nm_capability_score(metrics)

        return metrics

    def _evaluate_biological_insight(self, question_data: Dict, agent_output: Dict,
                                     metrics: NMEvaluationMetrics) -> float:
        """è¯„ä¼°ç”Ÿç‰©å­¦æ´å¯ŸåŠ›"""

        answer = agent_output.get('answer', '')

        if not answer:
            return 0.0

        score = 0.0

        # 1. è·¨æ¨¡æ€æ•´åˆ (30%)
        if len(metrics.modalities_used) >= 3:
            cross_modal = 1.0
        elif len(metrics.modalities_used) == 2:
            cross_modal = 0.7
        else:
            cross_modal = 0.3

        score += cross_modal * 0.3

        # 2. å®šé‡åˆ†æ (30%)
        score += metrics.scientific_rigor * 0.3

        # 3. ç”Ÿç‰©å­¦ç›¸å…³æ€§ (40%)
        bio_keywords = [
            'neuron', 'cell', 'cluster', 'marker', 'express',
            'project', 'connect', 'circuit', 'pathway',
            'morphology', 'axon', 'dendrite', 'synapse',
            'cortex', 'region', 'brain', 'neural'
        ]

        answer_lower = answer.lower()
        keyword_count = sum(1 for kw in bio_keywords if kw in answer_lower)

        if keyword_count >= 8:
            bio_relevance = 1.0
        elif keyword_count >= 5:
            bio_relevance = 0.8
        elif keyword_count >= 3:
            bio_relevance = 0.5
        else:
            bio_relevance = 0.3

        score += bio_relevance * 0.4

        return min(score, 1.0)

    def _calculate_nm_capability_score(self, metrics: NMEvaluationMetrics) -> float:
        """è®¡ç®—NMæ ¸å¿ƒèƒ½åŠ›æ€»åˆ†"""

        nm_scores = []

        if metrics.planning_quality is not None:
            nm_scores.append(metrics.planning_quality)
        if metrics.reasoning_capability is not None:
            nm_scores.append(metrics.reasoning_capability)
        if metrics.cot_quality is not None:
            nm_scores.append(metrics.cot_quality)
        if metrics.reflection_capability is not None:
            nm_scores.append(metrics.reflection_capability)
        if metrics.nlu_capability is not None:
            nm_scores.append(metrics.nlu_capability)

        return np.mean(nm_scores) if nm_scores else 0.0

    def _calculate_overall_score(self, metrics: NMEvaluationMetrics, method_name: str) -> float:
        """è®¡ç®—åŠ æƒOverallåˆ†æ•°"""

        weights = self.config['nm_method_weights'].get(method_name, {})

        if not weights:
            # Fallback
            core_scores = [
                metrics.entity_f1,
                metrics.factual_accuracy,
                metrics.scientific_rigor,
            ]
            return np.mean([s for s in core_scores if s is not None])

        weighted_sum = 0.0
        total_weight = 0.0

        metric_values = {
            # NMæ ¸å¿ƒèƒ½åŠ›
            'planning_quality': metrics.planning_quality,
            'reasoning_capability': metrics.reasoning_capability,
            'cot_quality': metrics.cot_quality,
            'reflection_capability': metrics.reflection_capability,
            'nlu_capability': metrics.nlu_capability,
            # ä¼ ç»ŸæŒ‡æ ‡
            'entity_f1': metrics.entity_f1,
            'factual_accuracy': metrics.factual_accuracy,
            'scientific_rigor': metrics.scientific_rigor,
            'modality_coverage': metrics.modality_coverage,
            'closed_loop': 1.0 if metrics.closed_loop_achieved else (
                0.0 if metrics.closed_loop_achieved is not None else None),
        }

        for metric_name, weight in weights.items():
            value = metric_values.get(metric_name)

            if value is not None:
                weighted_sum += value * weight
                total_weight += weight

        return weighted_sum / total_weight if total_weight > 0 else 0.0


# ==================== Export ====================

__all__ = [
    'NMEvaluationMetrics',
    'PlanningQualityEvaluator',
    'ReasoningCapabilityEvaluator',
    'CoTQualityEvaluator',
    'ReflectionCapabilityEvaluator',
    'NLUCapabilityEvaluator',
    'ComprehensiveEvaluatorV4',
    'EVALUATION_CONFIG',
]

if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("âœ… Enhanced evaluators.py v4.0 (Nature Methods) loaded successfully!")
    print("=" * 80)

    print("\nğŸ”¬ New NM Core Dimensions:")
    for dim, config in EVALUATION_CONFIG['nm_core_dimensions'].items():
        print(f"  - {dim}: {config['description']}")

    print("\n" + "=" * 80)