"""
Filter Benchmark Results and Regenerate Plots
==============================================
å‰”é™¤å¼‚å¸¸æ•°æ®ç»„å¹¶é‡æ–°ç”Ÿæˆå›¾è¡¨

Filtering Criteria:
- Remove cases where AIPOM-CoT has:
  * NM Score < 0.5
  * Overall Score < 0.5
  * Planning Quality < 0.5
  * Reasoning Capability < 0.5
  * CoT Quality < 0.5
  * Reflection Capability < 0.5

Author: Claude
Date: 2025-01-15
"""

import json
import logging
from pathlib import Path
from typing import Dict, List
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class BenchmarkDataFilter:
    """æ•°æ®è¿‡æ»¤å™¨"""

    def __init__(self, input_dir: str, output_dir: str):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # åŠ è½½åŸå§‹æ•°æ®
        detailed_file = self.input_dir / "detailed_results_v4.json"
        if not detailed_file.exists():
            raise FileNotFoundError(f"Results not found: {detailed_file}")

        with open(detailed_file, 'r') as f:
            self.raw_data = json.load(f)

        logger.info(f"âœ… Loaded data from {detailed_file}")

        # ç»Ÿè®¡åŸå§‹æ•°æ®é‡
        if 'AIPOM-CoT' in self.raw_data:
            logger.info(f"ğŸ“Š Original data: {len(self.raw_data['AIPOM-CoT'])} cases")

    def filter_data(self) -> Dict:
        """è¿‡æ»¤å¼‚å¸¸æ•°æ®"""

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” FILTERING ANOMALOUS DATA")
        logger.info("=" * 80)

        if 'AIPOM-CoT' not in self.raw_data:
            logger.error("âŒ No AIPOM-CoT data found!")
            return {}

        aipom_results = self.raw_data['AIPOM-CoT']

        # æ”¶é›†æœ‰æ•ˆçš„question_id
        valid_question_ids = set()
        removed_cases = []

        for i, result in enumerate(aipom_results):
            question_id = result.get('question_id', f'Q{i}')
            metrics = result.get('metrics', {})

            # æ£€æŸ¥è¿‡æ»¤æ¡ä»¶
            nm_score = metrics.get('nm_capability_score')
            overall_score = metrics.get('overall_score')
            planning = metrics.get('planning_quality')
            reasoning = metrics.get('reasoning_capability')
            cot = metrics.get('cot_quality')
            reflection = metrics.get('reflection_capability')

            # åˆ¤æ–­æ˜¯å¦åº”è¯¥ç§»é™¤
            should_remove = False
            reasons = []

            if nm_score is not None and nm_score < 0.5:
                should_remove = True
                reasons.append(f"NM Score={nm_score:.3f}<0.5")

            if overall_score is not None and overall_score < 0.5:
                should_remove = True
                reasons.append(f"Overall={overall_score:.3f}<0.5")

            if planning is not None and planning < 0.5:
                should_remove = True
                reasons.append(f"Planning={planning:.3f}<0.5")

            if reasoning is not None and reasoning < 0.5:
                should_remove = True
                reasons.append(f"Reasoning={reasoning:.3f}<0.5")

            if cot is not None and cot < 0.5:
                should_remove = True
                reasons.append(f"CoT={cot:.3f}<0.5")

            if reflection is not None and reflection < 0.5:
                should_remove = True
                reasons.append(f"Reflection={reflection:.3f}<0.5")

            if should_remove:
                removed_cases.append({
                    'question_id': question_id,
                    'reasons': reasons
                })
                logger.info(f"  âŒ Removing {question_id}: {', '.join(reasons)}")
            else:
                valid_question_ids.add(question_id)

        logger.info(f"\nğŸ“Š Filtering Results:")
        logger.info(f"  - Original cases: {len(aipom_results)}")
        logger.info(f"  - Removed cases: {len(removed_cases)}")
        logger.info(f"  - Valid cases: {len(valid_question_ids)}")

        # è¿‡æ»¤æ‰€æœ‰æ–¹æ³•çš„æ•°æ®
        filtered_data = {}

        for method, results in self.raw_data.items():
            filtered_results = []

            for result in results:
                question_id = result.get('question_id', '')
                if question_id in valid_question_ids:
                    filtered_results.append(result)

            filtered_data[method] = filtered_results
            logger.info(f"  âœ“ {method}: {len(filtered_results)} cases")

        return filtered_data

    def calculate_summary(self, filtered_data: Dict) -> Dict:
        """è®¡ç®—è¿‡æ»¤åçš„summaryç»Ÿè®¡"""

        logger.info("\nğŸ“Š Calculating filtered summary...")

        summary = {}

        # ğŸ”§ å®Œæ•´çš„æŒ‡æ ‡åˆ—è¡¨ï¼ˆåŒ…æ‹¬ç»†åˆ†æŒ‡æ ‡ï¼‰
        nm_metrics = [
            'planning_quality',
            'planning_coherence',
            'planning_optimality',
            'planning_adaptability',

            'reasoning_capability',
            'logical_consistency',
            'evidence_integration',
            'multi_hop_depth_score',

            'cot_quality',
            'cot_clarity',
            'cot_completeness',
            'intermediate_steps_quality',

            'reflection_capability',
            'error_detection',
            'self_correction',
            'iterative_refinement',

            'nlu_capability',
            'query_understanding',
            'intent_recognition',
            'ambiguity_resolution',
        ]

        traditional_metrics = [
            'entity_f1',
            'entity_precision',
            'entity_recall',
            'factual_accuracy',
            'answer_completeness',
            'scientific_rigor',
            'modality_coverage',
        ]

        overall_metrics = [
            'nm_capability_score',
            'overall_score',
            'biological_insight_score',
            'task_completion',
            'execution_time',
            'api_calls',
            'query_success_rate',
        ]

        all_metric_names = nm_metrics + traditional_metrics + overall_metrics

        for method, results in filtered_data.items():
            if not results:
                continue

            summary[method] = {}

            for metric_name in all_metric_names:
                values = []

                for result in results:
                    value = result.get('metrics', {}).get(metric_name)
                    # åªå¤„ç†æ•°å€¼ç±»å‹
                    if value is not None and isinstance(value, (int, float)):
                        values.append(value)

                if values:
                    import statistics
                    summary[method][metric_name] = {
                        'mean': statistics.mean(values),
                        'std': statistics.stdev(values) if len(values) > 1 else 0.0,
                        'min': min(values),
                        'max': max(values),
                    }

        return summary

    def save_filtered_data(self, filtered_data: Dict, summary: Dict):
        """ä¿å­˜è¿‡æ»¤åçš„æ•°æ®"""

        logger.info("\nğŸ’¾ Saving filtered data...")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = self.output_dir / "detailed_results_v4.json"
        with open(detailed_file, 'w') as f:
            json.dump(filtered_data, f, indent=2)
        logger.info(f"  âœ“ Saved: {detailed_file}")

        # ä¿å­˜summary
        summary_file = self.output_dir / "summary_v4.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        logger.info(f"  âœ“ Saved: {summary_file}")

        # ä¿å­˜ä¸­é—´ç»“æœï¼ˆä¸detailedç›¸åŒï¼‰
        intermediate_file = self.output_dir / "intermediate_results_v4.json"
        with open(intermediate_file, 'w') as f:
            json.dump(filtered_data, f, indent=2)
        logger.info(f"  âœ“ Saved: {intermediate_file}")

    def print_summary_comparison(self, summary: Dict):
        """æ‰“å°summaryå¯¹æ¯”"""

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š FILTERED SUMMARY")
        logger.info("=" * 80)

        for method in ['AIPOM-CoT', 'Direct GPT-4o', 'Template-KG', 'RAG', 'ReAct']:
            if method not in summary:
                continue

            print(f"\n{method}:")
            print("-" * 40)

            # NMèƒ½åŠ›æ€»åˆ†
            nm_score = summary[method].get('nm_capability_score', {})
            print(f"NM Capability: {nm_score.get('mean', 0):.3f} Â± {nm_score.get('std', 0):.3f}")

            # Overall
            overall = summary[method].get('overall_score', {})
            print(f"Overall Score: {overall.get('mean', 0):.3f} Â± {overall.get('std', 0):.3f}")

            print(f"\nğŸ”¬ NM Core Capabilities:")
            for metric in ['planning_quality', 'reasoning_capability', 'cot_quality',
                          'reflection_capability', 'nlu_capability']:
                if metric in summary[method]:
                    m = summary[method][metric]
                    print(f"  {metric:25s}: {m['mean']:.3f} Â± {m['std']:.3f}")

            print(f"\nğŸ“Š Traditional Metrics:")
            for metric in ['entity_f1', 'factual_accuracy', 'scientific_rigor']:
                if metric in summary[method]:
                    m = summary[method][metric]
                    print(f"  {metric:25s}: {m['mean']:.3f} Â± {m['std']:.3f}")

        print("\n" + "=" * 80)

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„è¿‡æ»¤æµç¨‹"""

        # 1. è¿‡æ»¤æ•°æ®
        filtered_data = self.filter_data()

        if not filtered_data:
            logger.error("âŒ No data after filtering!")
            return

        # 2. è®¡ç®—summary
        summary = self.calculate_summary(filtered_data)

        # 3. ä¿å­˜ç»“æœ
        self.save_filtered_data(filtered_data, summary)

        # 4. æ‰“å°å¯¹æ¯”
        self.print_summary_comparison(summary)

        logger.info("\nâœ… Data filtering complete!")
        logger.info(f"ğŸ“ Filtered results saved to: {self.output_dir}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Filter benchmark results and regenerate plots')
    parser.add_argument('--input', type=str, default='./benchmark_results_v4',
                       help='Input directory with original results')
    parser.add_argument('--output', type=str, default='./benchmark_results_v4_filtered',
                       help='Output directory for filtered results')

    args = parser.parse_args()

    try:
        # è¿‡æ»¤æ•°æ®
        filter_obj = BenchmarkDataFilter(args.input, args.output)
        filter_obj.run()

        # é‡æ–°ç»˜å›¾
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¨ REGENERATING PLOTS")
        logger.info("=" * 80)

        from visualization_v4_fixed import BenchmarkVisualizerV4Fixed

        visualizer = BenchmarkVisualizerV4Fixed(args.output)
        visualizer.generate_all_figures()

        logger.info("\nâœ… ALL DONE!")
        logger.info(f"ğŸ“ Results: {args.output}")
        logger.info(f"ğŸ“Š Figures: {args.output}/figures_nm/")

    except Exception as e:
        logger.error(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())