"""
Intelligent Entity Recognition System
=====================================
KG-powered entity discovery with NLP and fuzzy matching

ä¼˜åŒ–ç‚¹:
1. ä¸€æ¬¡æ€§æ„å»ºå®Œæ•´KGç´¢å¼• (åˆå§‹åŒ–æ—¶å®Œæˆ)
2. ä½¿ç”¨rapidfuzzåŠ é€Ÿæ¨¡ç³ŠåŒ¹é…
3. æ”¯æŒä¸­è‹±æ–‡ã€ç¼©å†™ã€å…¨åå¤šç§å½¢å¼
4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å®ä½“èšç±»

Author: Claude & PrometheusTT
Date: 2025-01-12
"""

import re
import time
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict
from dataclasses import dataclass, field
import logging

import spacy
from rapidfuzz import fuzz, process

from neo4j_exec import Neo4jExec
from aipom_cot_true_agent_v2 import RealSchemaCache

logger = logging.getLogger(__name__)


# ==================== Entity Data Structures ====================

@dataclass
class EntityMatch:
    """
    å®ä½“åŒ¹é…ç»“æœ

    ğŸ”§ å®Œæ•´å®šä¹‰ï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
    """
    # å¿…éœ€å­—æ®µ
    text: str  # åœ¨é—®é¢˜ä¸­çš„åŸå§‹æ–‡æœ¬
    entity_type: str  # å®ä½“ç±»å‹ (Region/GeneMarker/Subclass/Cluster)
    entity_id: str  # å®ä½“ID (acronym/gene/name)
    confidence: float  # ç½®ä¿¡åº¦ (0.0-1.0)
    match_type: str  # åŒ¹é…ç±»å‹ (exact/fuzzy/regex_fallback/llm)

    # å¯é€‰å­—æ®µ
    span: Tuple[int, int] = (0, 0)  # æ–‡æœ¬ä½ç½® (start, end)
    metadata: Dict = field(default_factory=dict)  # é¢å¤–å…ƒæ•°æ®

    def __post_init__(self):
        """éªŒè¯å­—æ®µ"""
        if not 0.0 <= self.confidence <= 1.0:
            raise ValueError(f"Confidence must be between 0.0 and 1.0, got {self.confidence}")

        if self.entity_type not in ['Region', 'GeneMarker', 'Subclass', 'Cluster', 'Unknown']:
            logger.warning(f"Unexpected entity_type: {self.entity_type}")


@dataclass
class EntityCluster:
    """ç›¸å…³å®ä½“çš„èšåˆ"""
    primary_entity: EntityMatch
    related_entities: List[EntityMatch]
    cluster_type: str  # 'gene_marker' | 'region' | 'cell_type'
    relevance_score: float  # ä¸é—®é¢˜çš„ç›¸å…³æ€§


# ==================== Entity Index Builder ====================

class KGEntityIndexer:
    """
    æ„å»ºå®Œæ•´KGå®ä½“ç´¢å¼• (åˆå§‹åŒ–æ—¶ä¸€æ¬¡æ€§å®Œæˆ)
    ä¼˜åŒ–: ä½¿ç”¨å†…å­˜ç´¢å¼• + rapidfuzzåŠ é€ŸæŸ¥è¯¢
    """

    def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
        self.db = db
        self.schema = schema

        # ç´¢å¼•ç»“æ„
        self.indices = {
            'nodes': {},  # {label: [(id, name, acronym, props), ...]}
            'gene_markers': set(),  # Set of gene names
            'regions': {},  # {acronym: (id, full_name, props)}
            'clusters': {},  # {cluster_name: (id, markers, props)}
        }

        logger.info("ğŸ” Building comprehensive KG entity index...")
        self._build_all_indices()
        logger.info("âœ… Entity index ready!")

    def _build_all_indices(self):
        """æ„å»ºæ‰€æœ‰ç´¢å¼•"""
        self._index_regions()
        self._index_clusters_and_genes()
        self._index_cell_types()
        self._index_neurons()

    def _index_regions(self):
        """ç´¢å¼•æ‰€æœ‰è„‘åŒº"""
        query = """
        MATCH (r:Region)
        RETURN elementId(r) AS id,
               r.acronym AS acronym,
               r.name AS name,
               r.full_name AS full_name,
               properties(r) AS props
        LIMIT 5000
        """

        result = self.db.run(query)
        if not result['success']:
            logger.warning("Failed to index regions")
            return

        for row in result['data']:
            acronym = row['acronym']
            if acronym:
                self.indices['regions'][acronym.upper()] = {
                    'id': row['id'],
                    'name': row['name'],
                    'full_name': row['full_name'],
                    'props': row['props']
                }

        logger.info(f"  âœ“ Indexed {len(self.indices['regions'])} regions")

    def _index_clusters_and_genes(self):
        """ç´¢å¼•Clusterå’Œæå–gene markers"""
        query = """
        MATCH (c:Cluster)
        RETURN elementId(c) AS id,
               c.name AS name,
               c.markers AS markers,
               c.number_of_neurons AS neuron_count,
               properties(c) AS props
        LIMIT 3000
        """

        result = self.db.run(query)
        if not result['success']:
            logger.warning("Failed to index clusters")
            return

        for row in result['data']:
            cluster_name = row['name']
            if cluster_name:
                self.indices['clusters'][cluster_name] = {
                    'id': row['id'],
                    'markers': row['markers'],
                    'neuron_count': row['neuron_count'],
                    'props': row['props']
                }

            # æå–gene markers
            markers = row['markers']
            if markers:
                genes = [g.strip() for g in markers.split(',')]
                self.indices['gene_markers'].update(genes)

        logger.info(f"  âœ“ Indexed {len(self.indices['clusters'])} clusters")
        logger.info(f"  âœ“ Extracted {len(self.indices['gene_markers'])} unique gene markers")

    def _index_cell_types(self):
        """ç´¢å¼•ç»†èƒç±»å‹ (Class, Subclass, Supertype)"""
        for label in ['Class', 'Subclass', 'Supertype']:
            query = f"""
            MATCH (n:{label})
            RETURN elementId(n) AS id,
                   n.name AS name,
                   properties(n) AS props
            LIMIT 1000
            """

            result = self.db.run(query)
            if result['success']:
                self.indices['nodes'][label] = [
                    {
                        'id': row['id'],
                        'name': row['name'],
                        'props': row['props']
                    }
                    for row in result['data']
                ]
                logger.info(f"  âœ“ Indexed {len(self.indices['nodes'][label])} {label} nodes")

    def _index_neurons(self):
        """ç´¢å¼•ç¥ç»å…ƒæ ·æœ¬ (ç”¨äºå½¢æ€å­¦æŸ¥è¯¢)"""
        query = """
        MATCH (n:Neuron)
        RETURN elementId(n) AS id,
               properties(n) AS props
        LIMIT 500
        """

        result = self.db.run(query)
        if result['success']:
            self.indices['nodes']['Neuron'] = [
                {'id': row['id'], 'props': row['props']}
                for row in result['data']
            ]
            logger.info(f"  âœ“ Indexed {len(self.indices['nodes']['Neuron'])} neurons")


# ==================== Intelligent Entity Recognizer ====================

# class IntelligentEntityRecognizer:
#     """
#     ä¸»å®ä½“è¯†åˆ«å™¨
#
#     Pipeline:
#     1. NLP tokenization (spaCy)
#     2. Multi-level matching (exact, fuzzy, pattern)
#     3. Context-aware filtering
#     """
#
#     def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
#         self.db = db
#         self.schema = schema
#
#         # åŠ è½½spaCy
#         try:
#             self.nlp = spacy.load("en_core_web_sm")
#         except OSError:
#             logger.warning("spaCy model not found, installing...")
#             import subprocess
#             subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
#             self.nlp = spacy.load("en_core_web_sm")
#
#         # æ„å»ºç´¢å¼•
#         self.indexer = KGEntityIndexer(db, schema)
#
#     def recognize_entities(self, question: str) -> List[EntityMatch]:
#         """
#         è¯†åˆ«å®ä½“ (ä¿®å¤ç‰ˆ - ä¸ä¾èµ–ç¼ºå¤±çš„æ–¹æ³•)
#
#         ğŸ”§ ä¿®å¤:
#         1. ç§»é™¤å¯¹ _match_neurons çš„è°ƒç”¨
#         2. æ·»åŠ è°ƒè¯•æ—¥å¿—
#         3. ç®€åŒ–æµç¨‹
#         """
#         logger.info(f"ğŸ” Recognizing entities in: {question}")
#
#         all_matches = []
#
#         # Extract tokens
#         tokens = self._extract_tokens(question)
#         logger.debug(f"   Tokens extracted: {tokens[:10]}")
#
#         # 1. Gene markers
#         try:
#             gene_matches = self._match_gene_markers(tokens, question)
#             all_matches.extend(gene_matches)
#             if gene_matches:
#                 logger.debug(f"   Found {len(gene_matches)} gene markers")
#         except Exception as e:
#             logger.error(f"   Error matching gene markers: {e}")
#
#         # 2. Regions (æœ€é‡è¦!)
#         try:
#             region_matches = self._match_regions(tokens, question)
#             all_matches.extend(region_matches)
#             if region_matches:
#                 logger.debug(f"   Found {len(region_matches)} regions")
#         except Exception as e:
#             logger.error(f"   Error matching regions: {e}")
#             import traceback
#             traceback.print_exc()
#
#         # 3. Cell types
#         try:
#             cell_type_matches = self._match_cell_types(tokens, question)
#             all_matches.extend(cell_type_matches)
#             if cell_type_matches:
#                 logger.debug(f"   Found {len(cell_type_matches)} cell types")
#         except Exception as e:
#             logger.error(f"   Error matching cell types: {e}")
#
#         # ğŸ”§ ç§»é™¤å¯¹ _match_neurons çš„è°ƒç”¨ (å› ä¸ºè¯¥æ–¹æ³•ä¸å­˜åœ¨)
#         # neuron_matches = self._match_neurons(tokens, question)
#         # all_matches.extend(neuron_matches)
#
#         # Report results
#         if all_matches:
#             logger.info(f"   âœ… Found {len(all_matches)} entities")
#             for m in all_matches[:5]:
#                 logger.debug(f"      â€¢ {m.text} ({m.entity_type}) via {m.context.get('source', 'unknown')}")
#         else:
#             logger.warning(f"   âš ï¸ No entities found in: {question}")
#
#         # Deduplicate
#         seen = set()
#         unique_matches = []
#         for match in all_matches:
#             key = (match.entity_id, match.entity_type)
#             if key not in seen:
#                 seen.add(key)
#                 unique_matches.append(match)
#
#         # Sort by confidence
#         unique_matches.sort(key=lambda x: x.confidence, reverse=True)
#
#         return unique_matches
#
#     def _extract_tokens(self, text: str) -> List[str]:
#         """
#         æå–æœ‰æ„ä¹‰çš„tokens
#
#         ä½¿ç”¨:
#         - NLPè¯æ€§æ ‡æ³¨
#         - é¢†åŸŸç‰¹å®šæ¨¡å¼ (å¦‚Car3+, L5, IT-type)
#         - å¤§å†™ç¼©å†™è¯†åˆ«
#         """
#         doc = self.nlp(text)
#         tokens = []
#
#         # 1. NLP entities
#         for ent in doc.ents:
#             tokens.append(ent.text)
#
#         # 2. Nouns and proper nouns
#         for token in doc:
#             if token.pos_ in ['NOUN', 'PROPN', 'ADJ'] and len(token.text) > 2:
#                 tokens.append(token.text)
#
#         # 3. Gene-like patterns (e.g., Car3, Pvalb, Sst)
#         gene_pattern = r'\b[A-Z][a-z]{2,}[0-9]?\b'
#         genes = re.findall(gene_pattern, text)
#         tokens.extend(genes)
#
#         # 4. Gene+ patterns (e.g., Car3+)
#         plus_pattern = r'\b([A-Z][a-z]+[0-9]*)\+\b'
#         plus_genes = re.findall(plus_pattern, text)
#         tokens.extend(plus_genes)
#
#         # 5. Uppercase acronyms (2-6 letters)
#         acronyms = re.findall(r'\b[A-Z]{2,6}\b', text)
#         tokens.extend(acronyms)
#
#         # 6. Special patterns
#         # "layer 5" -> "L5"
#         layer_match = re.findall(r'layer\s+(\d+)', text, re.IGNORECASE)
#         tokens.extend([f"L{l}" for l in layer_match])
#
#         # Deduplicate
#         return list(set(tokens))
#
#     def _match_gene_markers(self, tokens: List[str], full_text: str) -> List[EntityMatch]:
#         """åŒ¹é…åŸºå› marker"""
#         matches = []
#
#         gene_markers = self.indexer.indices['gene_markers']
#
#         for token in tokens:
#             # Exact match
#             if token in gene_markers:
#                 # Check if "+" nearby (higher confidence)
#                 confidence = 0.95 if (token + '+') in full_text else 0.85
#
#                 matches.append(EntityMatch(
#                     text=token,
#                     entity_id=token,
#                     entity_type='GeneMarker',
#                     match_type='exact',
#                     confidence=confidence,
#                     metadata={'source': 'Cluster.markers'}
#                 ))
#             else:
#                 # Fuzzy match
#                 best_matches = process.extract(
#                     token,
#                     gene_markers,
#                     scorer=fuzz.ratio,
#                     limit=3,
#                     score_cutoff=80
#                 )
#
#                 for gene, score, _ in best_matches:
#                     matches.append(EntityMatch(
#                         text=token,
#                         entity_id=gene,
#                         entity_type='GeneMarker',
#                         match_type='fuzzy',
#                         confidence=score / 100.0 * 0.9,  # Slight penalty for fuzzy
#                         metadata={'matched_gene': gene, 'source': 'Cluster.markers'}
#                     ))
#
#         return matches
#
#     def _match_regions(self, tokens: List[str], full_text: str) -> List[EntityMatch]:
#         """åŒ¹é…regions (å¸¦åœç”¨è¯è¿‡æ»¤)"""
#         matches = []
#
#         # ğŸ”§ åœç”¨è¯åˆ—è¡¨ (é¿å…è¯¯åŒ¹é…å¸¸è§è¯)
#         STOPWORDS = {
#             'ME', 'US', 'IT', 'IS', 'IN', 'ON', 'AT', 'TO', 'OF', 'AND', 'OR',
#             'THE', 'A', 'AN', 'FOR', 'WITH', 'AS', 'BY', 'FROM', 'UP', 'OUT'
#         }
#
#         # è·å–æˆ–é‡å»ºregion index
#         region_acronyms = self.indexer.indices.get('region_acronyms', {})
#
#         if not region_acronyms:
#             logger.warning("   Region index empty, rebuilding...")
#             try:
#                 query = "MATCH (r:Region) RETURN r.acronym AS acronym LIMIT 500"
#                 result = self.indexer.db.run(query)
#                 if result['success'] and result['data']:
#                     region_acronyms = {row['acronym']: row['acronym'] for row in result['data'] if row.get('acronym')}
#                     self.indexer.indices['region_acronyms'] = region_acronyms
#                     logger.info(f"   Rebuilt region index: {len(region_acronyms)} regions")
#             except Exception as e:
#                 logger.error(f"   Failed to rebuild region index: {e}")
#                 return matches
#
#         import re
#
#         # Strategy 1: Direct token matching (with stopword filter)
#         for token in tokens:
#             token_upper = token.strip('.,!?;: ').upper()
#
#             # ğŸ”§ è·³è¿‡åœç”¨è¯
#             if token_upper in STOPWORDS:
#                 continue
#
#             if token_upper in region_acronyms:
#                 if not any(m.entity_id == token_upper for m in matches):
#                     matches.append(EntityMatch(
#                         text=token_upper,
#                         entity_id=token_upper,
#                         entity_type='Region',
#                         match_type='exact',
#                         confidence=0.95,
#                         metadata={'source': 'token'}
#                     ))
#
#         # Strategy 2: "Compare A and B" pattern
#         pattern = r'compare\s+(\w+)\s+and\s+(\w+)'
#         for m in re.finditer(pattern, full_text, re.IGNORECASE):
#             for idx in [1, 2]:
#                 entity = m.group(idx).upper()
#
#                 # ğŸ”§ è·³è¿‡åœç”¨è¯
#                 if entity in STOPWORDS:
#                     continue
#
#                 if entity in region_acronyms:
#                     if not any(match.entity_id == entity for match in matches):
#                         matches.append(EntityMatch(
#                             text=entity,
#                             entity_id=entity,
#                             entity_type='Region',
#                             match_type='pattern',
#                             confidence=0.95,
#                             metadata={'source': 'compare'}
#                         ))
#
#         # Strategy 3: "A vs B" pattern
#         pattern = r'(\w+)\s+vs\.?\s+(\w+)'
#         for m in re.finditer(pattern, full_text, re.IGNORECASE):
#             for idx in [1, 2]:
#                 entity = m.group(idx).upper()
#
#                 # ğŸ”§ è·³è¿‡åœç”¨è¯
#                 if entity in STOPWORDS:
#                     continue
#
#                 if entity in region_acronyms:
#                     if not any(match.entity_id == entity for match in matches):
#                         matches.append(EntityMatch(
#                             text=entity,
#                             entity_id=entity,
#                             entity_type='Region',
#                             match_type='pattern',
#                             confidence=0.95,
#                             metadata={'source': 'vs'}
#                         ))
#
#         # Strategy 4: Word-by-word fallback (with stopword filter)
#         if not matches:
#             for word in re.findall(r'\b\w+\b', full_text):
#                 word_upper = word.upper()
#
#                 # ğŸ”§ è·³è¿‡åœç”¨è¯
#                 if word_upper in STOPWORDS:
#                     continue
#
#                 if word_upper in region_acronyms:
#                     if not any(m.entity_id == word_upper for m in matches):
#                         matches.append(EntityMatch(
#                             text=word_upper,
#                             entity_id=word_upper,
#                             entity_type='Region',
#                             match_type='exact',
#                             confidence=0.90,
#                             metadata={'source': 'fallback'}
#                         ))
#
#         if matches:
#             logger.info(f"   Matched regions: {[m.entity_id for m in matches]}")
#
#         return matches
#
#     def _match_cell_types(self, tokens: List[str], full_text: str) -> List[EntityMatch]:
#         """åŒ¹é…ç»†èƒç±»å‹"""
#         matches = []
#
#         # Predefined cell type keywords
#         known_types = {
#             'IT': 'Intratelencephalic',
#             'ET': 'Extratelencephalic',
#             'CT': 'Corticothalamic',
#             'PT': 'Pyramidal tract',
#             'NP': 'Near-projecting',
#             'interneuron': 'Interneuron',
#             'pyramidal': 'Pyramidal',
#             'excitatory': 'Excitatory',
#             'inhibitory': 'Inhibitory'
#         }
#
#         for token in tokens:
#             token_lower = token.lower()
#
#             if token in known_types or token_lower in known_types:
#                 cell_type = known_types.get(token) or known_types.get(token_lower)
#                 matches.append(EntityMatch(
#                     text=token,
#                     entity_id=cell_type,
#                     entity_type='CellType',
#                     match_type='keyword',
#                     confidence=0.85,
#                     metadata={'full_name': cell_type}
#                 ))
#
#         # Match against Subclass nodes
#         for label in ['Subclass', 'Class', 'Supertype']:
#             if label in self.indexer.indices['nodes']:
#                 nodes = self.indexer.indices['nodes'][label]
#                 node_names = [n['name'] for n in nodes if n['name']]
#
#                 for token in tokens:
#                     best_matches = process.extract(
#                         token.lower(),
#                         [n.lower() for n in node_names],
#                         scorer=fuzz.ratio,
#                         limit=2,
#                         score_cutoff=80
#                     )
#
#                     for matched_name, score, _ in best_matches:
#                         # Find original name
#                         original = next(n for n in node_names if n.lower() == matched_name)
#                         node_info = next(n for n in nodes if n['name'] == original)
#
#                         matches.append(EntityMatch(
#                             text=token,
#                             entity_id=node_info['id'],
#                             entity_type=label,
#                             match_type='node_name',
#                             confidence=score / 100.0 * 0.8,
#                             metadata={'name': original}
#                         ))
#
#         return matches
#
#     def _deduplicate(self, matches: List[EntityMatch]) -> List[EntityMatch]:
#         """å»é‡,ä¿ç•™æœ€é«˜confidenceçš„åŒ¹é…"""
#         seen = {}
#
#         for match in matches:
#             key = (match.entity_type, match.entity_id)
#
#             if key not in seen or match.confidence > seen[key].confidence:
#                 seen[key] = match
#
#         return list(seen.values())
class IntelligentEntityRecognizer:
    """
    æ™ºèƒ½å®ä½“è¯†åˆ«å™¨ - å¢å¼ºç‰ˆï¼ˆå¸¦å¤šå±‚Fallbackï¼‰

    è¯†åˆ«ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:
    1. ç²¾ç¡®åŒ¹é… (Exact Match)
    2. æ¨¡ç³ŠåŒ¹é… (Fuzzy Match)
    3. æ­£åˆ™è¡¨è¾¾å¼æå– (Regex Fallback)
    4. LLMè¾…åŠ©è¯†åˆ« (LLM Fallback - å¯é€‰)

    ğŸ”§ å…³é”®æ”¹è¿›ï¼š
    - æ·»åŠ æ­£åˆ™fallbackï¼Œç¡®ä¿å¸¸è§å®ä½“ä¸ä¼šæ¼æ‰
    - æ·»åŠ KGéªŒè¯ï¼Œé¿å…false positives
    - æ·»åŠ ç½®ä¿¡åº¦åˆ†çº§
    """

    def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
        self.db = db
        self.schema = schema

        # ç¼“å­˜ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        self._entity_cache = {}
        self._last_cache_time = time.time()
        self._cache_ttl = 3600  # 1å°æ—¶

    def recognize_entities(self, question: str) -> List[EntityMatch]:
        """
        æ™ºèƒ½å®ä½“è¯†åˆ« - ä¸»å…¥å£

        å¤šå±‚ç­–ç•¥ï¼š
        1. ç²¾ç¡®åŒ¹é…ï¼ˆç½®ä¿¡åº¦1.0ï¼‰
        2. æ¨¡ç³ŠåŒ¹é…ï¼ˆç½®ä¿¡åº¦0.7-0.9ï¼‰
        3. æ­£åˆ™fallbackï¼ˆç½®ä¿¡åº¦0.5-0.6ï¼‰

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            EntityMatchåˆ—è¡¨ï¼ŒæŒ‰ç½®ä¿¡åº¦æ’åº
        """
        logger.info(f"ğŸ” Recognizing entities in: {question}")

        matches = []

        # ===== Strategy 1: ç²¾ç¡®åŒ¹é… =====
        exact_matches = self._exact_match(question)
        matches.extend(exact_matches)

        if exact_matches:
            logger.info(f"   âœ“ Exact match found: {len(exact_matches)} entities")

        # ===== Strategy 2: æ¨¡ç³ŠåŒ¹é… =====
        if len(matches) < 3:  # å¦‚æœç²¾ç¡®åŒ¹é…å¤ªå°‘ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            fuzzy_matches = self._fuzzy_match(question)

            # é¿å…é‡å¤
            existing_texts = set([m.text.lower() for m in matches])
            for fm in fuzzy_matches:
                if fm.text.lower() not in existing_texts:
                    matches.append(fm)

            if fuzzy_matches:
                logger.info(f"   âœ“ Fuzzy match found: {len(fuzzy_matches)} entities")

        # ===== Strategy 3: æ­£åˆ™Fallback =====
        if not matches:
            logger.warning("   âš ï¸ Primary strategies failed, using regex fallback...")
            regex_matches = self._regex_fallback(question)
            matches.extend(regex_matches)

            if regex_matches:
                logger.info(f"   âœ“ Regex fallback found: {len(regex_matches)} entities")
            else:
                logger.warning(f"   âš ï¸ No entities found in: {question}")

        # ===== Strategy 4: LLM Fallback (å¯é€‰) =====
        # if not matches:
        #     llm_matches = self._llm_fallback(question)
        #     matches.extend(llm_matches)

        # å»é‡å’Œæ’åº
        matches = self._deduplicate_and_rank(matches)

        # æ‰“å°è¯†åˆ«ç»“æœ
        if matches:
            logger.info(f"   ğŸ“Š Recognized {len(matches)} entities:")
            for m in matches[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"      â€¢ {m.text} ({m.entity_type}) [{m.confidence:.2f}] via {m.match_type}")

        return matches

    def _exact_match(self, question: str) -> List[EntityMatch]:
        """
        ç­–ç•¥1: ç²¾ç¡®åŒ¹é…

        æŸ¥æ‰¾schemaä¸­å®šä¹‰çš„æ‰€æœ‰å®ä½“ç±»å‹ï¼Œåœ¨é—®é¢˜ä¸­ç²¾ç¡®åŒ¹é…
        """
        matches = []

        # è·å–æ‰€æœ‰å®ä½“ç±»å‹
        entity_types = ['Region', 'GeneMarker', 'Subclass', 'Cluster']

        for entity_type in entity_types:
            # ä»ç¼“å­˜æˆ–æ•°æ®åº“è·å–å®ä½“åˆ—è¡¨
            entities = self._get_entities_of_type(entity_type)

            for entity in entities:
                # æ£€æŸ¥nameå’Œacronym
                names_to_check = []

                if 'acronym' in entity:
                    names_to_check.append(entity['acronym'])
                if 'name' in entity:
                    names_to_check.append(entity['name'])
                if 'gene' in entity:
                    names_to_check.append(entity['gene'])

                for name in names_to_check:
                    if not name:
                        continue

                    # ç²¾ç¡®åŒ¹é…ï¼ˆword boundaryï¼‰
                    pattern = r'\b' + re.escape(name) + r'\b'

                    for match in re.finditer(pattern, question, re.IGNORECASE):
                        matches.append(EntityMatch(
                            text=match.group(),
                            entity_type=entity_type,
                            entity_id=entity.get('acronym') or entity.get('gene') or entity.get('name'),
                            confidence=1.0,
                            match_type='exact',
                            span=(match.start(), match.end()),
                            metadata=entity
                        ))

        return matches

    def _fuzzy_match(self, question: str) -> List[EntityMatch]:
        """
        ç­–ç•¥2: æ¨¡ç³ŠåŒ¹é…ï¼ˆä¸¥æ ¼ç‰ˆ - è¿‡æ»¤å¸¸è§å•è¯ï¼‰
        """
        matches = []

        # æå–å€™é€‰è¯
        words = re.findall(r'\b[A-Za-z]{2,8}\b', question)

        # ğŸ”§ è¶…ä¸¥æ ¼çš„å¸¸ç”¨è¯é»‘åå•
        common_words = {
            # ç–‘é—®è¯
            'what', 'which', 'where', 'when', 'who', 'why', 'how',
            # beåŠ¨è¯
            'are', 'is', 'was', 'were', 'be', 'been', 'being', 'am',
            # åŠ©åŠ¨è¯
            'do', 'does', 'did', 'done', 'doing',
            'have', 'has', 'had', 'having',
            'can', 'could', 'will', 'would', 'shall', 'should',
            'may', 'might', 'must',
            # ä»‹è¯
            'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
            'into', 'onto', 'upon', 'off', 'out', 'over', 'under',
            # è¿è¯
            'and', 'or', 'but', 'so', 'yet', 'nor',
            # å† è¯
            'the', 'an', 'a',
            # ä»£è¯
            'it', 'its', 'they', 'their', 'them', 'this', 'that', 'these', 'those',
            'he', 'she', 'his', 'her', 'him', 'me', 'my', 'we', 'our', 'us',
            # å¸¸è§åŠ¨è¯
            'get', 'got', 'give', 'gave', 'given', 'show', 'tell', 'told',
            'make', 'made', 'take', 'took', 'taken', 'come', 'came',
            # å¸¸è§å½¢å®¹è¯
            'not', 'all', 'some', 'any', 'each', 'every', 'both', 'few', 'more',
            'most', 'other', 'such', 'no', 'nor', 'only', 'own', 'same', 'than',
            # å¸¸è§å‰¯è¯
            'too', 'very', 'just', 'now', 'then', 'also', 'here', 'there',
            'well', 'even', 'still', 'already', 'yet',
            # ç¥ç»ç§‘å­¦é€šç”¨è¯ï¼ˆä¸æ˜¯å®ä½“ï¼‰
            'cells', 'neurons', 'brain', 'regions', 'region', 'area', 'areas',
            'about', 'between', 'within', 'across', 'through',
            'types', 'type', 'kind', 'kinds', 'group', 'groups'
        }

        entity_types = ['Region']  # ğŸ”§ åªåœ¨fuzzy matchä¸­æŸ¥æ‰¾Region

        for entity_type in entity_types:
            entities = self._get_entities_of_type(entity_type)

            for word in words:
                word_lower = word.lower()

                # ğŸ”§ ä¸¥æ ¼è¿‡æ»¤
                if word_lower in common_words:
                    continue

                # ğŸ”§ é•¿åº¦æ£€æŸ¥
                if len(word) < 3:
                    continue

                for entity in entities:
                    names_to_check = []

                    if 'acronym' in entity:
                        names_to_check.append(entity['acronym'])

                    for name in names_to_check:
                        if not name:
                            continue

                        name_lower = name.lower()

                        if word_lower == name_lower:
                            continue  # å·²åœ¨exact matchå¤„ç†

                        # éƒ¨åˆ†åŒ¹é…
                        if word_lower in name_lower or name_lower in word_lower:
                            confidence = 0.8
                        else:
                            similarity = self._string_similarity(word_lower, name_lower)
                            if similarity < 0.7:
                                continue
                            confidence = similarity

                        span_match = re.search(r'\b' + re.escape(word) + r'\b', question, re.IGNORECASE)
                        if span_match:
                            matches.append(EntityMatch(
                                text=span_match.group(),
                                entity_type=entity_type,
                                entity_id=entity.get('acronym', name),
                                confidence=confidence,
                                match_type='fuzzy',
                                span=(span_match.start(), span_match.end()),
                                metadata=entity
                            ))

        return matches

    def _regex_fallback(self, question: str) -> List[EntityMatch]:
        """
        ç­–ç•¥3: æ­£åˆ™è¡¨è¾¾å¼Fallback

        å½“ç²¾ç¡®/æ¨¡ç³ŠåŒ¹é…å¤±è´¥æ—¶ï¼Œä½¿ç”¨æ­£åˆ™æå–æ˜æ˜¾çš„å®ä½“pattern

        Pattern:
        1. è„‘åŒºç¼©å†™: 2-5ä¸ªè¿ç»­å¤§å†™å­—æ¯ (å¦‚ ACAd, MOs, CA1)
        2. åŸºå› æ ‡è®°: é¦–å­—æ¯å¤§å†™+å°å†™+å¯é€‰æ•°å­— (å¦‚ Pvalb, Sst, Car3)

        ğŸ”§ å…³é”®: æå–åå¿…é¡»åœ¨KGä¸­éªŒè¯ï¼Œé¿å…false positives
        """
        matches = []

        # ===== Pattern 1: è„‘åŒºç¼©å†™ =====
        region_pattern = r'\b[A-Z]{2,5}\b'

        for match in re.finditer(region_pattern, question):
            text = match.group()

            # æ’é™¤å¸¸è§è‹±æ–‡å•è¯ï¼ˆé¿å…è¯¯è¯†åˆ«ï¼‰
            if text in ['WHAT', 'WHERE', 'WHICH', 'WHEN', 'WHO', 'WHY', 'HOW', 'ARE', 'IS', 'DO', 'DOES']:
                continue

            # ğŸ”§ å…³é”®: åœ¨KGä¸­éªŒè¯
            validation = self._validate_entity_in_kg('Region', text)

            if validation['exists']:
                matches.append(EntityMatch(
                    text=text,
                    entity_type='Region',
                    entity_id=validation.get('id', text),
                    confidence=0.6,  # è¾ƒä½ç½®ä¿¡åº¦
                    match_type='regex_fallback',
                    span=(match.start(), match.end()),
                    metadata=validation.get('data', {})
                ))
                logger.info(f"      Regex fallback validated Region: {text}")

        # ===== Pattern 2: åŸºå› æ ‡è®° =====
        gene_pattern = r'\b[A-Z][a-z]{2,8}\d*\+?\b'

        for match in re.finditer(gene_pattern, question):
            text = match.group()

            # ç§»é™¤å¯èƒ½çš„ + åç¼€
            gene_name = text.rstrip('+')

            # æ’é™¤å¸¸è§è‹±æ–‡å•è¯
            common_words = [
                'what', 'which', 'where', 'when', 'cells', 'neurons',
                'brain', 'regions', 'does', 'have', 'show', 'tell',
                'about', 'between', 'compare', 'difference'
            ]
            if gene_name.lower() in common_words:
                continue

            # ğŸ”§ å…³é”®: åœ¨KGä¸­éªŒè¯
            validation = self._validate_entity_in_kg('GeneMarker', gene_name)

            if validation['exists']:
                matches.append(EntityMatch(
                    text=text,
                    entity_type='GeneMarker',
                    entity_id=validation.get('id', gene_name),
                    confidence=0.5,  # æ›´ä½ç½®ä¿¡åº¦
                    match_type='regex_fallback',
                    span=(match.start(), match.end()),
                    metadata=validation.get('data', {})
                ))
                logger.info(f"      Regex fallback validated Gene: {text}")

        # ===== Pattern 3: Subclass/Clusteråç§° =====
        # ä¾‹å¦‚: "Pvalb+ interneurons", "Sst cells"
        subclass_pattern = r'\b([A-Z][a-z]{2,8}\d*)\+?\s+(neuron|cell|interneuron)'

        for match in re.finditer(subclass_pattern, question, re.IGNORECASE):
            marker_name = match.group(1)

            # éªŒè¯è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªSubclass
            validation = self._validate_entity_in_kg('Subclass', marker_name)

            if validation['exists']:
                matches.append(EntityMatch(
                    text=match.group(),
                    entity_type='Subclass',
                    entity_id=validation.get('id', marker_name),
                    confidence=0.7,  # ä¸­ç­‰ç½®ä¿¡åº¦ï¼ˆå› ä¸ºæœ‰ä¸Šä¸‹æ–‡ï¼‰
                    match_type='regex_fallback',
                    span=(match.start(), match.end()),
                    metadata=validation.get('data', {})
                ))
                logger.info(f"      Regex fallback validated Subclass: {marker_name}")

        return matches

    def _get_entities_of_type(self, entity_type: str) -> List[Dict]:
        """
        è·å–æŒ‡å®šç±»å‹çš„æ‰€æœ‰å®ä½“ï¼ˆé€‚é…å®é™…Schemaï¼‰

        ğŸ”§ æ ¹æ®å®é™…Schemaè°ƒæ•´ï¼š
        - Region: æœ‰ acronym å’Œ name
        - Cluster: markerså­—æ®µåŒ…å«åŸºå› æ ‡è®°
        - Subclass: markerså­—æ®µåŒ…å«åŸºå› æ ‡è®°
        - æ²¡æœ‰ç‹¬ç«‹çš„GeneMarkerèŠ‚ç‚¹
        """
        cache_key = f"entities_{entity_type}"

        if cache_key in self._entity_cache:
            cache_time = self._entity_cache[cache_key].get('time', 0)
            if time.time() - cache_time < self._cache_ttl:
                return self._entity_cache[cache_key]['data']

        if entity_type == 'Region':
            query = """
            MATCH (r:Region)
            RETURN r.acronym AS acronym, r.name AS name
            LIMIT 500
            """
        elif entity_type == 'GeneMarker':
            # ğŸ”§ ä»Clusterçš„markerså­—æ®µæå–åŸºå› 
            query = """
            MATCH (c:Cluster)
            WHERE c.markers IS NOT NULL
            WITH split(c.markers, ',') AS marker_list
            UNWIND marker_list AS marker
            RETURN DISTINCT trim(marker) AS gene
            LIMIT 1000
            """
        elif entity_type == 'Subclass':
            query = """
            MATCH (sc:Subclass)
            RETURN DISTINCT sc.name AS name
            LIMIT 500
            """
        elif entity_type == 'Cluster':
            query = """
            MATCH (c:Cluster)
            RETURN c.name AS name
            LIMIT 500
            """
        else:
            return []

        result = self.db.run(query)

        if result['success'] and result['data']:
            entities = result['data']

            self._entity_cache[cache_key] = {
                'data': entities,
                'time': time.time()
            }

            return entities
        else:
            return []

    def _validate_entity_in_kg(self, entity_type: str, entity_name: str) -> Dict:
        """
        åœ¨KGä¸­éªŒè¯å®ä½“æ˜¯å¦å­˜åœ¨ï¼ˆé€‚é…å®é™…Schemaï¼‰
        """

        if entity_type == 'Region':
            query = """
            MATCH (r:Region)
            WHERE r.acronym = $name OR r.name = $name
            RETURN r.acronym AS id, r.name AS name, r AS data
            LIMIT 1
            """
        elif entity_type == 'GeneMarker':
            # ğŸ”§ ä»Cluster.markersä¸­æŸ¥æ‰¾
            query = """
            MATCH (c:Cluster)
            WHERE c.markers CONTAINS $name
            RETURN $name AS id, c AS data
            LIMIT 1
            """
        elif entity_type == 'Subclass':
            query = """
            MATCH (sc:Subclass)
            WHERE sc.name CONTAINS $name OR sc.markers CONTAINS $name
            RETURN sc.name AS id, sc AS data
            LIMIT 1
            """
        elif entity_type == 'Cluster':
            query = """
            MATCH (c:Cluster)
            WHERE c.name CONTAINS $name OR c.markers CONTAINS $name
            RETURN c.name AS id, c AS data
            LIMIT 1
            """
        else:
            return {'exists': False}

        result = self.db.run(query, {'name': entity_name})

        if result['success'] and result['data']:
            row = result['data'][0]
            return {
                'exists': True,
                'id': row.get('id', entity_name),
                'data': row.get('data', {})
            }
        else:
            return {'exists': False}

    def _string_similarity(self, s1: str, s2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆLevenshteinï¼‰

        Returns:
            0.0-1.0ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if s1 == s2:
            return 1.0

        if len(s1) == 0 or len(s2) == 0:
            return 0.0

        # ç®€åŒ–ç®—æ³•ï¼šlongest common substring ratio
        max_len = max(len(s1), len(s2))

        # æ‰¾æœ€é•¿å…¬å…±å­ä¸²
        lcs_len = 0
        for i in range(len(s1)):
            for j in range(len(s2)):
                k = 0
                while (i + k < len(s1) and
                       j + k < len(s2) and
                       s1[i + k] == s2[j + k]):
                    k += 1
                lcs_len = max(lcs_len, k)

        return lcs_len / max_len

    def _deduplicate_and_rank(self, matches: List[EntityMatch]) -> List[EntityMatch]:
        """
        å»é‡å’Œæ’åº

        è§„åˆ™:
        1. ç›¸åŒtext + entity_typeè§†ä¸ºé‡å¤ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
        2. æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
        """
        # å»é‡
        seen = {}
        for match in matches:
            key = (match.text.lower(), match.entity_type)

            if key not in seen:
                seen[key] = match
            else:
                # ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„
                if match.confidence > seen[key].confidence:
                    seen[key] = match

        # æ’åº
        unique_matches = list(seen.values())
        unique_matches.sort(key=lambda m: m.confidence, reverse=True)

        return unique_matches

    def _llm_fallback(self, question: str) -> List[EntityMatch]:
        """
        ç­–ç•¥4: LLMè¾…åŠ©è¯†åˆ«ï¼ˆå¯é€‰ï¼‰

        å½“æ‰€æœ‰è§„åˆ™æ–¹æ³•å¤±è´¥æ—¶ï¼Œä½¿ç”¨LLMæå–å®ä½“

        âš ï¸ æ³¨æ„ï¼šéœ€è¦OpenAI APIï¼Œæˆæœ¬è¾ƒé«˜ï¼Œä»…åœ¨å¿…è¦æ—¶å¯ç”¨
        """
        # TODO: å®ç°LLM fallback
        # 1. è°ƒç”¨GPT-4æå–å®ä½“
        # 2. éªŒè¯æå–çš„å®ä½“åœ¨KGä¸­å­˜åœ¨
        # 3. è¿”å›EntityMatchåˆ—è¡¨

        return []

# ==================== Entity Clustering ====================

class EntityClusteringEngine:
    """
    å°†è¯†åˆ«çš„å®ä½“èšç±»æˆæœ‰æ„ä¹‰çš„ç»„

    ä¾‹å¦‚:
    - Car3 (gene) + MOs (region) -> 'gene_in_region' cluster
    - Pvalb (gene) + Sst (gene) -> 'gene_comparison' cluster
    """

    def __init__(self, db: Neo4jExec, schema: RealSchemaCache):
        self.db = db
        self.schema = schema

    def cluster_entities(self,
                         matches: List[EntityMatch],
                         question: str) -> List[EntityCluster]:
        """
        èšç±»å®ä½“

        ç­–ç•¥:
        1. æŒ‰ç±»å‹åˆ†ç»„
        2. æŸ¥è¯¢KGæ‰¾ç›¸å…³æ€§
        3. è®¡ç®—relevance score
        """
        clusters = []

        # Group by type
        genes = [m for m in matches if m.entity_type == 'GeneMarker']
        regions = [m for m in matches if m.entity_type == 'Region']
        cell_types = [m for m in matches if m.entity_type in ['CellType', 'Subclass', 'Class']]

        # Create clusters
        if genes:
            cluster = self._create_gene_cluster(genes, regions, question)
            if cluster:
                clusters.append(cluster)

        if regions and not genes:
            cluster = self._create_region_cluster(regions, question)
            if cluster:
                clusters.append(cluster)

        if cell_types:
            cluster = self._create_celltype_cluster(cell_types, question)
            if cluster:
                clusters.append(cluster)

        # Rank by relevance
        clusters.sort(key=lambda c: c.relevance_score, reverse=True)

        return clusters

    def _create_gene_cluster(self,
                             genes: List[EntityMatch],
                             regions: List[EntityMatch],
                             question: str) -> Optional[EntityCluster]:
        """åˆ›å»ºåŸºå› ä¸ºä¸­å¿ƒçš„cluster"""
        if not genes:
            return None

        primary_gene = genes[0]
        gene_name = primary_gene.entity_id

        # Query KG for related clusters
        query = """
        MATCH (c:Cluster)
        WHERE c.markers CONTAINS $gene
        RETURN c.name AS cluster,
               c.markers AS markers,
               elementId(c) AS id
        LIMIT 10
        """

        result = self.db.run(query, {'gene': gene_name})

        related = []
        if result['success']:
            for row in result['data']:
                related.append(EntityMatch(
                    text=row['cluster'],
                    entity_id=row['id'],
                    entity_type='Cluster',
                    match_type='related_to_gene',
                    confidence=0.8,
                    metadata={'markers': row['markers']}
                ))

        # Add regions if any
        related.extend(regions)

        # Calculate relevance
        relevance = 0.9
        question_lower = question.lower()
        if any(kw in question_lower for kw in ['gene', 'marker', 'express']):
            relevance *= 1.2

        return EntityCluster(
            primary_entity=primary_gene,
            related_entities=related,
            cluster_type='gene_marker',
            relevance_score=min(1.0, relevance)
        )

    def _create_region_cluster(self,
                               regions: List[EntityMatch],
                               question: str) -> Optional[EntityCluster]:
        """åˆ›å»ºåŒºåŸŸä¸ºä¸­å¿ƒçš„cluster"""
        if not regions:
            return None

        primary_region = regions[0]

        relevance = 0.85
        question_lower = question.lower()
        if any(kw in question_lower for kw in ['region', 'area', 'brain']):
            relevance *= 1.2

        return EntityCluster(
            primary_entity=primary_region,
            related_entities=regions[1:],
            cluster_type='region',
            relevance_score=min(1.0, relevance)
        )

    def _create_celltype_cluster(self,
                                 cell_types: List[EntityMatch],
                                 question: str) -> Optional[EntityCluster]:
        """åˆ›å»ºç»†èƒç±»å‹cluster"""
        if not cell_types:
            return None

        primary = cell_types[0]

        relevance = 0.8
        question_lower = question.lower()
        if any(kw in question_lower for kw in ['cell', 'neuron', 'type']):
            relevance *= 1.2

        return EntityCluster(
            primary_entity=primary,
            related_entities=cell_types[1:],
            cluster_type='cell_type',
            relevance_score=min(1.0, relevance)
        )


# ==================== Test ====================

if __name__ == "__main__":
    import os
    from neo4j_exec import Neo4jExec
    from aipom_cot_true_agent_v2 import RealSchemaCache

    # Initialize
    db = Neo4jExec(
        uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
        user=os.getenv("NEO4J_USER", "neo4j"),
        pwd=os.getenv("NEO4J_PASSWORD", "neuroxiv"),
        database=os.getenv("NEO4J_DATABASE", "neo4j")
    )

    schema = RealSchemaCache("./schema_output/schema.json")

    # Test entity recognition
    recognizer = IntelligentEntityRecognizer(db, schema)

    test_questions = [
        "Tell me about Car3+ neurons",
        "Compare Pvalb and Sst interneurons in MOs",
        "What are the projection targets of claustrum?",
        "Analyze layer 5 pyramidal neurons morphology"
    ]

    for q in test_questions:
        print(f"\n{'=' * 60}")
        print(f"Q: {q}")
        print('=' * 60)

        matches = recognizer.recognize_entities(q)

        for m in matches[:5]:
            print(f"  â€¢ {m.text} ({m.entity_type}) [{m.confidence:.2f}]")
            print(f"    Match: {m.match_type}, ID: {m.entity_id}")