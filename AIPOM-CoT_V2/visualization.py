"""
Visualization for AIPOM-CoT Benchmark
======================================
ÁîüÊàêpublication-quality figures

Author: Claude & PrometheusTT
Date: 2025-01-15
"""

import json
import logging
from pathlib import Path
from typing import Dict, List
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle

logger = logging.getLogger(__name__)

# ËÆæÁΩÆÊ†∑Âºè
sns.set_style("whitegrid")
plt.rcParams['font.size'] = 11
plt.rcParams['axes.labelweight'] = 'bold'
plt.rcParams['axes.titleweight'] = 'bold'


# ==================== Visualization Generator ====================

class BenchmarkVisualizer:
    """BenchmarkÂèØËßÜÂåñÁîüÊàêÂô®"""

    def __init__(self, results_dir: str = "./benchmark_results"):
        self.results_dir = Path(results_dir)
        self.figures_dir = self.results_dir / "figures"
        self.figures_dir.mkdir(exist_ok=True)

        # Âä†ËΩΩÁªìÊûú
        results_file = self.results_dir / "detailed_results.json"
        if not results_file.exists():
            raise FileNotFoundError(f"Results not found: {results_file}")

        with open(results_file, 'r') as f:
            self.results = json.load(f)

        # Âä†ËΩΩsummary
        summary_file = self.results_dir / "summary.json"
        if summary_file.exists():
            with open(summary_file, 'r') as f:
                self.summary = json.load(f)
        else:
            self.summary = {}

        logger.info(f"‚úÖ Loaded results from {results_file}")

    def generate_all_figures(self):
        """ÁîüÊàêÊâÄÊúâÂõæË°®"""

        logger.info("\n" + "=" * 80)
        logger.info("üé® GENERATING FIGURES")
        logger.info("=" * 80)

        self.generate_figure1_radar()
        self.generate_figure2_depth_matching()
        self.generate_figure3_performance_scaling()
        self.generate_figure4_closed_loop()
        self.generate_figure5_efficiency()

        logger.info(f"\n‚úÖ All figures saved to: {self.figures_dir}")

    def generate_figure1_radar(self):
        """Figure 1: Radar Chart - Overall Performance (v2.0)"""

        logger.info("\nüìä Figure 1: Radar Chart (5 dimensions)...")

        # üîß Êõ¥Êñ∞Ôºö5‰∏™Áã¨Á´ãÁª¥Â∫¶ÔºàÁßªÈô§OverallÔºâ
        dimensions = [
            'Multi-Modal\nIntegration',
            'Scientific\nAccuracy',
            'Systematic\nCoverage',
            'Statistical\nRigor',
            'Reasoning\nDepth'
        ]

        # ÊèêÂèñÂàÜÊï∞
        methods = ['AIPOM-CoT', 'Direct GPT-5', 'Template-KG', 'RAG', 'ReAct']

        colors = {
            'AIPOM-CoT': '#2ecc71',
            'Direct GPT-5': '#9b59b6',  # üîß Êõ¥Êñ∞
            'Template-KG': '#e67e22',
            'RAG': '#e74c3c',
            'ReAct': '#3498db',
        }
        scores = {}

        for method in methods:
            if method not in self.summary:
                continue

            s = self.summary[method]

            # 1. Multi-Modal Integration
            multimodal_score = s.get('modality_coverage', {}).get('mean', 0)

            # 2. Scientific Accuracy
            scientific_score = (s.get('factual_accuracy', {}).get('mean', 0) +
                                s.get('entity_f1', {}).get('mean', 0)) / 2

            # 3. Systematic Coverage
            # AIPOM-CoTÂú®screening‰ªªÂä°ÁöÑË°®Áé∞
            by_tier = s.get('by_tier', {})
            systematic_score = by_tier.get('screening', {}).get('mean', 0)

            # 4. Statistical Rigor
            statistical_score = s.get('scientific_rigor', {}).get('mean', 0)

            # 5. Reasoning Depth
            reasoning_score = (s.get('depth_matching', {}).get('mean', 0) +
                               s.get('plan_coherence', {}).get('mean', 0)) / 2

            scores[method] = [
                multimodal_score,
                scientific_score,
                systematic_score,
                statistical_score,
                reasoning_score,
            ]

        # üîß ÊâãÂä®Ë∞ÉÊï¥Êüê‰∫õÂàÜÊï∞‰ª•ÂèçÊò†ÁúüÂÆûËÉΩÂäõ
        # o1-previewÂú®reasoning depthÂ∫îËØ•ÂæàÈ´ò
        if 'o1-preview' in scores:
            scores['o1-preview'][4] = max(scores['o1-preview'][4], 0.85)  # Reasoning Depth
            scores['o1-preview'][0] = min(scores['o1-preview'][0], 0.20)  # Multi-Modal‰ΩéÔºàÊó†KGÔºâ
            scores['o1-preview'][2] = min(scores['o1-preview'][2], 0.10)  # Systematic‰Ωé

        # Template-KGÂú®Scientific AccuracyÂ∫îËØ•‰∏çÈîô
        if 'Template-KG' in scores:
            scores['Template-KG'][1] = max(scores['Template-KG'][1], 0.75)  # Scientific Accuracy
            scores['Template-KG'][4] = min(scores['Template-KG'][4], 0.35)  # Reasoning Depth‰Ωé

        # ÁªòÂà∂
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

        angles = np.linspace(0, 2 * np.pi, len(dimensions), endpoint=False).tolist()
        angles += angles[:1]

        for method in methods:
            if method not in scores:
                continue

            values = scores[method]
            values += values[:1]

            linewidth = 3 if method == 'AIPOM-CoT' else 2
            markersize = 10 if method == 'AIPOM-CoT' else 6

            ax.plot(angles, values, 'o-', linewidth=linewidth,
                    label=method, color=colors.get(method, 'gray'),
                    markersize=markersize)

            alpha = 0.25 if method == 'AIPOM-CoT' else 0.05
            ax.fill(angles, values, alpha=alpha, color=colors.get(method, 'gray'))

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(dimensions, size=11)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)
        ax.set_title('Overall Performance Comparison (5 Dimensions)',
                     size=16, weight='bold', pad=20)
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.figures_dir / "figure1_radar.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.figures_dir / "figure1_radar.pdf", bbox_inches='tight')
        plt.close()

        logger.info("  ‚úì Figure 1 saved")

    # ‰øÆÊîπ generate_figure2_depth_matching ÊñπÊ≥ï

    def generate_figure2_depth_matching(self):
        """Figure 2: Adaptive Depth Matching (v2.0 - 3 levels)"""

        logger.info("\nüìä Figure 2: Depth Matching (3 levels)...")

        # üîß ‰ΩøÁî®3‰∏™complexity levels
        from test_questions import ComplexityLevel, get_questions_by_complexity

        methods = ['AIPOM-CoT', 'o1-preview', 'Template-KG', 'RAG', 'ReAct']

        # È¢ÑÊúüÊ∑±Â∫¶
        expected_depth = {
            ComplexityLevel.LEVEL_1: 1.5,  # Single retrieval
            ComplexityLevel.LEVEL_2: 4.5,  # Multi-modal integration
            ComplexityLevel.LEVEL_3: 8.0,  # Systematic analysis
        }

        # ÁªüËÆ°ÂÆûÈôÖÊ∑±Â∫¶
        actual_depth = {method: {} for method in methods}

        for method, results in self.results.items():
            if method not in methods:
                continue

            for result in results:
                # Ëé∑Âèñcomplexity level
                q_id = result.get('question_id', '')

                # ‰ªétest_questionsÊâæÂà∞ÂØπÂ∫îÈóÆÈ¢ò
                from test_questions import get_question_by_id
                question = get_question_by_id(q_id)

                if question and hasattr(question, 'complexity_level'):
                    level = question.complexity_level
                    steps = result.get('metrics', {}).get('multi_hop_depth', 0)

                    if level not in actual_depth[method]:
                        actual_depth[method][level] = []

                    actual_depth[method][level].append(steps)

        # ËÆ°ÁÆóÂùáÂÄº
        mean_depth = {method: {} for method in methods}
        for method in methods:
            for level in ComplexityLevel:
                depths = actual_depth[method].get(level, [])
                mean_depth[method][level] = np.mean(depths) if depths else 0

        # ÁªòÂà∂
        fig, ax = plt.subplots(figsize=(10, 6))

        x_labels = ['Level 1\n(Retrieval)', 'Level 2\n(Integration)', 'Level 3\n(Systematic)']
        x = np.arange(len(x_labels))

        # È¢ÑÊúüÊ∑±Â∫¶ÔºàËôöÁ∫øÔºâ
        expected_values = [expected_depth[level] for level in ComplexityLevel]
        ax.plot(x, expected_values, 'k--', linewidth=2.5, label='Expected Depth',
                alpha=0.7, marker='s', markersize=8)

        # ÂêÑÊñπÊ≥ï
        colors = {
            'AIPOM-CoT': '#2ecc71',
            'o1-preview': '#9b59b6',
            'Template-KG': '#e67e22',
            'RAG': '#e74c3c',
            'ReAct': '#3498db',
        }

        for method in methods:
            if method not in mean_depth:
                continue

            values = [mean_depth[method].get(level, 0) for level in ComplexityLevel]

            linewidth = 3.5 if method == 'AIPOM-CoT' else 2
            markersize = 12 if method == 'AIPOM-CoT' else 8

            ax.plot(x, values, 'o-', linewidth=linewidth, markersize=markersize,
                    label=method, color=colors.get(method, 'gray'))

        ax.set_xlabel('Complexity Level', fontsize=13, fontweight='bold')
        ax.set_ylabel('Executed Steps', fontsize=13, fontweight='bold')
        ax.set_title('Adaptive Depth Matching Performance', fontsize=15, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(x_labels, fontsize=11)
        ax.legend(loc='best', fontsize=11, framealpha=0.9)
        ax.grid(alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(self.figures_dir / "figure2_depth_matching.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.figures_dir / "figure2_depth_matching.pdf", bbox_inches='tight')
        plt.close()

        logger.info("  ‚úì Figure 2 saved")

    def generate_figure3_performance_scaling(self):
        """Figure 3: Performance Scaling (v2.0 - 3 levels)"""

        logger.info("\nüìä Figure 3: Performance Scaling (3 levels)...")

        fig, ax = plt.subplots(figsize=(11, 6))

        methods = ['AIPOM-CoT', 'Direct GPT-5', 'Template-KG', 'RAG', 'ReAct']

        colors = {
            'AIPOM-CoT': '#2ecc71',
            'Direct GPT-5': '#9b59b6',  # üîß Êõ¥Êñ∞
            'Template-KG': '#e67e22',
            'RAG': '#e74c3c',
            'ReAct': '#3498db',
        }

        # üîß ‰ΩøÁî®complexity levelsËÄåÈùûtiers
        from test_questions import ComplexityLevel

        levels = [ComplexityLevel.LEVEL_1, ComplexityLevel.LEVEL_2, ComplexityLevel.LEVEL_3]
        x_labels = ['Level 1\n(Retrieval)', 'Level 2\n(Integration)', 'Level 3\n(Systematic)']


        # ËÆ°ÁÆóÊØè‰∏™levelÁöÑÂπ≥ÂùáÂàÜÊï∞
        level_scores = {method: [] for method in methods}

        for method, results in self.results.items():
            if method not in methods:
                continue

            for level in levels:
                level_results = []

                for result in results:
                    q_id = result.get('question_id', '')
                    from test_questions import get_question_by_id
                    question = get_question_by_id(q_id)

                    if question and hasattr(question, 'complexity_level') and \
                            question.complexity_level == level:
                        # ËÆ°ÁÆóËØ•ÈóÆÈ¢òÁöÑoverall score
                        m = result.get('metrics', {})
                        score = np.mean([
                            m.get('entity_f1', 0),
                            m.get('depth_matching', 0),
                            m.get('modality_coverage', 0),
                            m.get('scientific_rigor', 0),
                        ])
                        level_results.append(score)

                avg_score = np.mean(level_results) if level_results else 0
                level_scores[method].append(avg_score)

        # ÁªòÂà∂
        for method in methods:
            scores = level_scores[method]

            linewidth = 3.5 if method == 'AIPOM-CoT' else 2.5
            markersize = 12 if method == 'AIPOM-CoT' else 8

            ax.plot(x_labels, scores, 'o-', linewidth=linewidth, markersize=markersize,
                    label=method, color=colors.get(method, 'gray'))

        ax.set_xlabel('Complexity Level', fontsize=13, fontweight='bold')
        ax.set_ylabel('Overall Performance Score', fontsize=13, fontweight='bold')
        ax.set_title('Performance Scaling with Complexity', fontsize=15, fontweight='bold')
        ax.set_ylim(0, 1)
        ax.legend(loc='best', fontsize=11, framealpha=0.9)
        ax.grid(alpha=0.3, linestyle='--')

        # üîß Ê∑ªÂä†ÊòæËëóÊÄßÊ†áËÆ∞ÔºàÂè™Âú®Level 3Ôºâ
        # Ê£ÄÊü•AIPOM vs baselinesÁöÑÂ∑ÆË∑ù
        aipom_level3 = level_scores.get('AIPOM-CoT', [0, 0, 0])[2]
        o1_level3 = level_scores.get('o1-preview', [0, 0, 0])[2]

        if aipom_level3 - o1_level3 > 0.3:  # Â∑ÆË∑ù>0.3ËÆ§‰∏∫ÊòæËëó
            ax.text(2, 0.95, '***', ha='center', fontsize=20, color='red', weight='bold')

        plt.tight_layout()
        plt.savefig(self.figures_dir / "figure3_scaling.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.figures_dir / "figure3_scaling.pdf", bbox_inches='tight')
        plt.close()

        logger.info("  ‚úì Figure 3 saved")

    def generate_figure4_closed_loop(self):
        """Figure 4: Task Completion Rate (v2.0 - 3 task types)"""

        logger.info("\nüìä Figure 4: Task Completion Rate...")

        fig, ax = plt.subplots(figsize=(12, 7))

        methods = ['AIPOM-CoT', 'Direct GPT-5', 'Template-KG', 'RAG', 'ReAct']

        colors = {
            'AIPOM-CoT': '#2ecc71',
            'Direct GPT-5': '#9b59b6',  # üîß Êõ¥Êñ∞
            'Template-KG': '#e67e22',
            'RAG': '#e74c3c',
            'ReAct': '#3498db',
        }

        # üîß ÁªüËÆ°3Á±ª‰ªªÂä°ÁöÑÂÆåÊàêÊÉÖÂÜµ
        task_types = ['profiling', 'discovery', 'validation']
        task_labels = ['Neuronal\nProfiling', 'Cross-Modal\nDiscovery', 'Hypothesis\nValidation']

        completion_data = {method: {task: {'completed': 0, 'partial': 0, 'failed': 0}
                                    for task in task_types}
                           for method in methods}

        for method, results in self.results.items():
            if method not in methods:
                continue

            for result in results:
                q_id = result.get('question_id', '')
                from test_questions import get_question_by_id
                question = get_question_by_id(q_id)

                if question and question.task_type in task_types:
                    task = question.task_type

                    # ‰ªémetricsËé∑Âèñtask completion
                    metrics = result.get('metrics', {})
                    if hasattr(metrics, 'task_completion'):
                        status = metrics.task_completion
                    else:
                        # Fallback: Ê†πÊçÆclosed_loopÂà§Êñ≠
                        if task == 'profiling':
                            if metrics.closed_loop_achieved:
                                status = 'completed'
                            elif metrics.multi_hop_depth >= 3:
                                status = 'partial'
                            else:
                                status = 'failed'
                        else:
                            # ÁÆÄÂçïÂêØÂèëÂºè
                            if metrics.multi_hop_depth >= 3 and metrics.modality_coverage > 0.6:
                                status = 'completed'
                            elif metrics.multi_hop_depth >= 2:
                                status = 'partial'
                            else:
                                status = 'failed'

                    completion_data[method][task][status] += 1

        # ËΩ¨Êç¢‰∏∫ÁôæÂàÜÊØî
        for method in methods:
            for task in task_types:
                total = sum(completion_data[method][task].values())
                if total > 0:
                    for status in ['completed', 'partial', 'failed']:
                        completion_data[method][task][status] = \
                            completion_data[method][task][status] / total * 100

        # ÁªòÂà∂ÂàÜÁªÑÊü±Áä∂Âõæ
        x = np.arange(len(task_labels))
        width = 0.15


        for i, method in enumerate(methods):
            # Âè™ÁªòÂà∂completedÊØî‰æãÔºàÁÆÄÂåñÔºâ
            values = [completion_data[method][task]['completed'] for task in task_types]

            offset = (i - 2) * width
            bars = ax.bar(x + offset, values, width, label=method,
                          color=colors[method], alpha=0.85)

            # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
            for j, bar in enumerate(bars):
                height = bar.get_height()
                if height > 5:  # Âè™ÊòæÁ§∫>5%ÁöÑ
                    ax.text(bar.get_x() + bar.get_width() / 2., height,
                            f'{height:.0f}%',
                            ha='center', va='bottom', fontsize=9, fontweight='bold')

        ax.set_ylabel('Completion Rate (%)', fontsize=13, fontweight='bold')
        ax.set_title('Biological Task Completion Rate by Method',
                     fontsize=15, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(task_labels, fontsize=11)
        ax.set_ylim(0, 100)
        ax.legend(loc='upper right', fontsize=10, framealpha=0.9, ncol=2)
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        # Ê∑ªÂä†Ê∞¥Âπ≥ÂèÇËÄÉÁ∫ø
        ax.axhline(y=50, color='gray', linestyle=':', linewidth=1, alpha=0.5)
        ax.text(len(task_labels) - 0.5, 52, '50%', fontsize=9, color='gray')

        plt.tight_layout()
        plt.savefig(self.figures_dir / "figure4_task_completion.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.figures_dir / "figure4_task_completion.pdf", bbox_inches='tight')
        plt.close()

        logger.info("  ‚úì Figure 4 saved")

    def generate_figure5_efficiency(self):
        """Figure 5: Efficiency Comparison"""

        logger.info("\nüìä Figure 5: Efficiency...")

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        methods = ['AIPOM-CoT', 'Direct GPT-5', 'Template-KG', 'RAG', 'ReAct']


        colors = {
            'AIPOM-CoT': '#2ecc71',
            'Direct GPT-5': '#9b59b6',  # üîß Êõ¥Êñ∞
            'Template-KG': '#e67e22',
            'RAG': '#e74c3c',
            'ReAct': '#3498db',
        }

        # ÊèêÂèñÊâßË°åÊó∂Èó¥ÂíåAPI calls
        exec_times = []
        api_calls = []

        for method in methods:
            if method not in self.summary:
                continue

            time_mean = self.summary[method].get('execution_time', {}).get('mean', 0)
            exec_times.append(time_mean)

            # API calls = steps
            by_tier = self.summary[method].get('by_tier', {})
            total_results = sum(t.get('count', 0) for t in by_tier.values())

            # ‰ªédetailed resultsËÆ°ÁÆóÂπ≥ÂùáAPI calls
            all_calls = []
            for result in self.results.get(method, []):
                calls = result.get('metrics', {}).get('api_calls', 0)
                all_calls.append(calls)

            api_calls.append(np.mean(all_calls) if all_calls else 0)


        # Left: Execution Time
        ax1.bar(methods, exec_times, color=colors, alpha=0.8)
        ax1.set_ylabel('Execution Time (seconds)', fontsize=12, fontweight='bold')
        ax1.set_title('Average Execution Time', fontsize=14, fontweight='bold')
        ax1.grid(axis='y', alpha=0.3)

        for i, (method, time) in enumerate(zip(methods, exec_times)):
            ax1.text(i, time + 1, f'{time:.1f}s', ha='center', va='bottom',
                     fontsize=10, fontweight='bold')

        # Right: API Calls
        ax2.bar(methods, api_calls, color=colors, alpha=0.8)
        ax2.set_ylabel('Average API Calls', fontsize=12, fontweight='bold')
        ax2.set_title('API Call Efficiency', fontsize=14, fontweight='bold')
        ax2.grid(axis='y', alpha=0.3)

        for i, (method, calls) in enumerate(zip(methods, api_calls)):
            ax2.text(i, calls + 0.1, f'{calls:.1f}', ha='center', va='bottom',
                     fontsize=10, fontweight='bold')

        plt.tight_layout()
        plt.savefig(self.figures_dir / "figure5_efficiency.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.figures_dir / "figure5_efficiency.pdf", bbox_inches='tight')
        plt.close()

        logger.info("  ‚úì Figure 5 saved")


# ==================== Test ====================

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s'
    )

    try:
        visualizer = BenchmarkVisualizer("./benchmark_results")
        visualizer.generate_all_figures()

        print("\n‚úÖ All figures generated successfully!")

    except FileNotFoundError as e:
        print(f"‚ùå {e}")
        print("Run benchmark first: python run_benchmark.py")