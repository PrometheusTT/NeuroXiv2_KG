"""
AIPOM-CoT Capability Evaluator
==============================
è¯„ä¼°AIPOM-CoTçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œçªå‡º:
- Think Capability (æ¨ç†èƒ½åŠ›): å®ä½“è¯†åˆ«ã€æ„å›¾ç†è§£ã€é—®é¢˜åˆ†è§£
- Plan Capability (è§„åˆ’èƒ½åŠ›): è·¯å¾„è§„åˆ’ã€ç­–ç•¥é€‰æ‹©ã€èµ„æºåˆ†é…
- Reflect Capability (åæ€èƒ½åŠ›): è¯æ®è¯„ä¼°ã€è‡ªæˆ‘çº æ­£ã€å†³ç­–è°ƒæ•´

è¯„åˆ†å…¬å¼:
    overall = capability_score Ã— correctness_multiplier

    capability_score = 0.35Ã—Think + 0.35Ã—Plan + 0.20Ã—Reflect + 0.10Ã—Act

    correctness_multiplier:
        - correct: 1.0
        - partial: 0.85
        - tangential: 0.5
        - incorrect: 0.3

Author: Lijun
Date: 2025-01
"""

import re
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

from core_structures import (
    Modality, Intent, PlannerType, ReflectionDecision,
    ThinkResult, PlanResult, ActResult, ReflectResult,
    TPARIteration, AgentOutput, AnswerCorrectness
)

logger = logging.getLogger(__name__)

# ==================== Configuration ====================

# èƒ½åŠ›æƒé‡é…ç½® - çªå‡ºThink, Plan, Reflectä¸‰å¤§èƒ½åŠ›
CAPABILITY_WEIGHTS = {
    'think': 0.35,  # æ¨ç†èƒ½åŠ› - é«˜æƒé‡
    'plan': 0.35,  # è§„åˆ’èƒ½åŠ› - é«˜æƒé‡
    'reflect': 0.20,  # åæ€èƒ½åŠ› - ä¸­æƒé‡
    'act': 0.10,  # æ‰§è¡Œèƒ½åŠ› - ä½æƒé‡ï¼ˆæ‰€æœ‰æ–¹æ³•éƒ½èƒ½æ‰§è¡Œï¼‰
}

# æ­£ç¡®æ€§ä¹˜æ•°
CORRECTNESS_MULTIPLIER = {
    'correct': 1.00,
    'partial': 0.85,
    'tangential': 0.50,
    'incorrect': 0.30,
    'unanswered': 0.10,
}

# Baselineæ–¹æ³•çš„èƒ½åŠ›å¤©èŠ±æ¿ - å®ƒä»¬ç¼ºä¹çœŸæ­£çš„Think/Plan/Reflectèƒ½åŠ›
BASELINE_CAPABILITY_LIMITS = {
    'Direct LLM': {
        'think': 0.30,  # åªæœ‰åŸºç¡€NLUï¼Œæ— ç»“æ„åŒ–æ€è€ƒ
        'plan': 0.10,  # æ— è§„åˆ’èƒ½åŠ›
        'reflect': 0.05,  # æ— åæ€èƒ½åŠ›
        'act': 0.20,  # æ— å·¥å…·ä½¿ç”¨
    },
    'Template-KG': {
        'think': 0.40,  # æ¨¡æ¿åŒ¹é…
        'plan': 0.30,  # å›ºå®šæ¨¡æ¿
        'reflect': 0.10,  # æ— åæ€
        'act': 0.60,  # å¯æ‰§è¡ŒæŸ¥è¯¢
    },
    'RAG': {
        'think': 0.50,  # æ£€ç´¢ç†è§£
        'plan': 0.20,  # ç®€å•æ£€ç´¢ç­–ç•¥
        'reflect': 0.10,  # æ— åæ€
        'act': 0.50,  # æ£€ç´¢æ‰§è¡Œ
    },
    'ReAct': {
        'think': 0.60,  # æœ‰æ¨ç†
        'plan': 0.50,  # æœ‰ä¸€å®šè§„åˆ’
        'reflect': 0.40,  # æœ‰è§‚å¯Ÿåé¦ˆ
        'act': 0.70,  # å·¥å…·ä½¿ç”¨
    },
    'AIPOM-CoT': {
        'think': 1.00,  # æ— ä¸Šé™
        'plan': 1.00,
        'reflect': 1.00,
        'act': 1.00,
    },
}


# ==================== Think Capability Evaluator ====================

class ThinkCapabilityEvaluator:
    """
    æ¨ç†èƒ½åŠ›è¯„ä¼°å™¨ (Think Capability)

    è¯„ä¼°:
    1. Entity Recognition: å®ä½“è¯†åˆ«å‡†ç¡®æ€§
    2. Intent Understanding: æ„å›¾ç†è§£å‡†ç¡®æ€§
    3. Reasoning Depth: æ¨ç†æ·±åº¦
    4. Problem Decomposition: é—®é¢˜åˆ†è§£èƒ½åŠ›
    """

    def evaluate(self, agent_output: AgentOutput,
                 question_data: Dict,
                 method_name: str) -> Dict[str, float]:
        """è¯„ä¼°æ¨ç†èƒ½åŠ›"""
        cap = BASELINE_CAPABILITY_LIMITS.get(method_name, {}).get('think', 1.0)

        # è·å–Thinkè®°å½•
        think_traces = agent_output.get_think_traces()

        if not think_traces:
            return {
                'think_score': min(0.1, cap),
                'think_details': {'no_think_traces': True}
            }

        # 1. å®ä½“è¯†åˆ«è¯„ä¼° (30%)
        entity_score = self._evaluate_entity_recognition(think_traces, question_data)

        # 2. æ„å›¾ç†è§£è¯„ä¼° (25%)
        intent_score = self._evaluate_intent_understanding(think_traces, question_data)

        # 3. æ¨ç†æ·±åº¦è¯„ä¼° (25%)
        reasoning_depth = self._evaluate_reasoning_depth(think_traces, agent_output)

        # 4. é—®é¢˜åˆ†è§£è¯„ä¼° (20%)
        decomposition = self._evaluate_problem_decomposition(think_traces, agent_output)

        # åŠ æƒæ€»åˆ†
        total = (
                entity_score * 0.30 +
                intent_score * 0.25 +
                reasoning_depth * 0.25 +
                decomposition * 0.20
        )

        final_score = min(total, cap)

        return {
            'think_score': final_score,
            'think_details': {
                'entity_recognition': entity_score,
                'intent_understanding': intent_score,
                'reasoning_depth': reasoning_depth,
                'problem_decomposition': decomposition,
                'think_iterations': len(think_traces),
            }
        }

    def _evaluate_entity_recognition(self, think_traces: List[Dict],
                                     question_data: Dict) -> float:
        """è¯„ä¼°å®ä½“è¯†åˆ«"""
        expected = set(e.lower() for e in question_data.get('expected_entities', []))

        # æ”¶é›†æ‰€æœ‰è¯†åˆ«çš„å®ä½“
        recognized = set()
        for trace in think_traces:
            recognized.update(e.lower() for e in trace.get('entities', []))

        if not expected:
            # å¦‚æœæ²¡æœ‰æœŸæœ›å®ä½“ï¼Œæ£€æŸ¥æ˜¯å¦è¯†åˆ«äº†ä»»ä½•å®ä½“
            return 0.7 if recognized else 0.3

        # è®¡ç®—å¬å›ç‡å’Œç²¾ç¡®ç‡
        recall = len(expected & recognized) / len(expected) if expected else 0
        precision = len(expected & recognized) / len(recognized) if recognized else 0

        # F1 score
        if recall + precision > 0:
            f1 = 2 * recall * precision / (recall + precision)
        else:
            f1 = 0

        return f1

    def _evaluate_intent_understanding(self, think_traces: List[Dict],
                                       question_data: Dict) -> float:
        """è¯„ä¼°æ„å›¾ç†è§£"""
        expected_strategy = question_data.get('expected_strategy', 'adaptive')

        # æ£€æŸ¥è¯†åˆ«çš„æ„å›¾
        for trace in think_traces:
            intent = trace.get('intent', '')

            # æ˜ å°„æ„å›¾åˆ°ç­–ç•¥
            if intent == 'focus_driven' and expected_strategy in ['focus_driven', 'adaptive']:
                return 1.0
            elif intent == 'comparative' and expected_strategy in ['comparative', 'adaptive']:
                return 1.0
            elif intent == 'screening' and expected_strategy in ['screening', 'comparative']:
                return 1.0
            elif intent == 'simple_qa' and expected_strategy == 'adaptive':
                return 0.8

        # éƒ¨åˆ†åŒ¹é…
        return 0.5 if think_traces else 0.2

    def _evaluate_reasoning_depth(self, think_traces: List[Dict],
                                  agent_output: AgentOutput) -> float:
        """è¯„ä¼°æ¨ç†æ·±åº¦"""
        # åŸºäºreasoningæ–‡æœ¬çš„æ·±åº¦
        total_reasoning_length = 0
        reasoning_markers = 0

        for trace in think_traces:
            reasoning = trace.get('reasoning', '')
            total_reasoning_length += len(reasoning)

            # ç»Ÿè®¡æ¨ç†æ ‡è®°è¯
            markers = ['because', 'therefore', 'since', 'indicates', 'suggests',
                       'reasoning', 'analysis', 'considering']
            reasoning_markers += sum(1 for m in markers if m in reasoning.lower())

        # æ¨ç†é•¿åº¦å¾—åˆ†
        length_score = min(1.0, total_reasoning_length / 500)

        # æ¨ç†æ ‡è®°å¾—åˆ†
        marker_score = min(1.0, reasoning_markers / 3)

        # è¿­ä»£æ·±åº¦å¾—åˆ†
        iteration_score = min(1.0, len(think_traces) / 3)

        return (length_score * 0.4 + marker_score * 0.3 + iteration_score * 0.3)

    def _evaluate_problem_decomposition(self, think_traces: List[Dict],
                                        agent_output: AgentOutput) -> float:
        """è¯„ä¼°é—®é¢˜åˆ†è§£èƒ½åŠ›"""
        # æ£€æŸ¥æ˜¯å¦åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡
        executed_steps = agent_output.get_executed_steps()

        if not executed_steps:
            return 0.2

        # ä¸åŒç±»å‹çš„æ­¥éª¤
        step_types = set(s.get('modality', '') for s in executed_steps)

        # åˆ†è§£æ·±åº¦
        decomposition_score = min(1.0, len(step_types) / 3)

        # æ­¥éª¤è¿è´¯æ€§
        coherence_score = 1.0 if len(executed_steps) >= 2 else 0.5

        return decomposition_score * 0.6 + coherence_score * 0.4


# ==================== Plan Capability Evaluator ====================

class PlanCapabilityEvaluator:
    """
    è§„åˆ’èƒ½åŠ›è¯„ä¼°å™¨ (Plan Capability)

    è¯„ä¼°:
    1. Path Planning: è·¯å¾„è§„åˆ’è´¨é‡
    2. Strategy Selection: ç­–ç•¥é€‰æ‹©é€‚å½“æ€§
    3. Resource Allocation: èµ„æºåˆ†é…æ•ˆç‡
    4. Adaptability: è®¡åˆ’é€‚åº”æ€§
    """

    def evaluate(self, agent_output: AgentOutput,
                 question_data: Dict,
                 method_name: str) -> Dict[str, float]:
        """è¯„ä¼°è§„åˆ’èƒ½åŠ›"""
        cap = BASELINE_CAPABILITY_LIMITS.get(method_name, {}).get('plan', 1.0)

        iterations = agent_output.iterations

        if not iterations:
            return {
                'plan_score': min(0.1, cap),
                'plan_details': {'no_iterations': True}
            }

        # 1. è·¯å¾„è§„åˆ’è´¨é‡ (30%)
        path_quality = self._evaluate_path_planning(iterations, question_data)

        # 2. ç­–ç•¥é€‰æ‹© (25%)
        strategy_quality = self._evaluate_strategy_selection(iterations, question_data)

        # 3. èµ„æºåˆ†é… (25%)
        resource_efficiency = self._evaluate_resource_allocation(agent_output)

        # 4. è®¡åˆ’é€‚åº”æ€§ (20%)
        adaptability = self._evaluate_adaptability(iterations)

        # åŠ æƒæ€»åˆ†
        total = (
                path_quality * 0.30 +
                strategy_quality * 0.25 +
                resource_efficiency * 0.25 +
                adaptability * 0.20
        )

        final_score = min(total, cap)

        return {
            'plan_score': final_score,
            'plan_details': {
                'path_planning': path_quality,
                'strategy_selection': strategy_quality,
                'resource_allocation': resource_efficiency,
                'adaptability': adaptability,
                'total_plans': len([i for i in iterations if i.plan]),
            }
        }

    def _evaluate_path_planning(self, iterations: List[TPARIteration],
                                question_data: Dict) -> float:
        """è¯„ä¼°è·¯å¾„è§„åˆ’è´¨é‡"""
        plans = [i.plan for i in iterations if i.plan]

        if not plans:
            return 0.2

        # è¯„ä¼°è·¯å¾„å¤šæ ·æ€§
        all_paths = []
        for plan in plans:
            all_paths.extend(plan.selected_paths)

        if not all_paths:
            return 0.3

        # è·¯å¾„è¦†ç›–çš„æ¨¡æ€
        modalities_covered = set()
        for path in all_paths:
            path_str = ' '.join(path.nodes).lower()
            if 'morphology' in path_str:
                modalities_covered.add('morphological')
            elif 'projection' in path_str or 'target' in path_str:
                modalities_covered.add('projection')
            else:
                modalities_covered.add('molecular')

        coverage_score = len(modalities_covered) / 3.0

        # è·¯å¾„æ•°é‡åˆç†æ€§
        expected_modalities = question_data.get('expected_modalities', [])
        path_count_score = min(1.0, len(all_paths) / max(1, len(expected_modalities)))

        return coverage_score * 0.6 + path_count_score * 0.4

    def _evaluate_strategy_selection(self, iterations: List[TPARIteration],
                                     question_data: Dict) -> float:
        """è¯„ä¼°ç­–ç•¥é€‰æ‹©"""
        plans = [i.plan for i in iterations if i.plan]

        if not plans:
            return 0.2

        expected_strategy = question_data.get('expected_strategy', 'adaptive')

        # æ£€æŸ¥é€‰æ‹©çš„plannerç±»å‹
        for plan in plans:
            planner_type = plan.planner_type.value

            if planner_type == expected_strategy:
                return 1.0
            elif planner_type == 'focus_driven' and expected_strategy in ['focus_driven', 'adaptive']:
                return 0.9
            elif planner_type == 'comparative' and expected_strategy in ['comparative', 'screening']:
                return 0.9

        return 0.5

    def _evaluate_resource_allocation(self, agent_output: AgentOutput) -> float:
        """è¯„ä¼°èµ„æºåˆ†é…æ•ˆç‡"""
        final_state = agent_output.final_state

        # é¢„ç®—ä½¿ç”¨ç‡
        budget_used = final_state.used_budget
        total_budget = final_state.total_budget

        # æˆåŠŸç‡
        executed_steps = agent_output.get_executed_steps()
        successful = sum(1 for s in executed_steps if s.get('success', False))
        success_rate = successful / len(executed_steps) if executed_steps else 0

        # æ•ˆç‡åˆ†æ•°ï¼ˆåœ¨åˆç†é¢„ç®—å†…å®Œæˆæ›´å¤šæœ‰æ•ˆå·¥ä½œï¼‰
        efficiency = success_rate * (1.0 - 0.5 * (budget_used / total_budget))

        return max(0.2, min(1.0, efficiency + 0.3))

    def _evaluate_adaptability(self, iterations: List[TPARIteration]) -> float:
        """è¯„ä¼°è®¡åˆ’é€‚åº”æ€§"""
        plans = [i.plan for i in iterations if i.plan]

        if len(plans) < 2:
            return 0.5

        # æ£€æŸ¥è®¡åˆ’æ˜¯å¦æ ¹æ®åé¦ˆè°ƒæ•´
        adjustments = 0
        for i in range(1, len(iterations)):
            if iterations[i].plan and iterations[i - 1].reflect:
                # æ£€æŸ¥æ˜¯å¦æ ¹æ®åæ€è°ƒæ•´äº†ç­–ç•¥
                if iterations[i - 1].reflect.decision in [ReflectionDecision.PIVOT, ReflectionDecision.DEEPEN]:
                    adjustments += 1

        adaptability = min(1.0, adjustments / max(1, len(plans) - 1) + 0.5)

        return adaptability


# ==================== Reflect Capability Evaluator ====================

class ReflectCapabilityEvaluator:
    """
    åæ€èƒ½åŠ›è¯„ä¼°å™¨ (Reflect Capability)

    è¯„ä¼°:
    1. Evidence Evaluation: è¯æ®è¯„ä¼°è´¨é‡
    2. Self-Correction: è‡ªæˆ‘çº æ­£èƒ½åŠ›
    3. Decision Quality: å†³ç­–è´¨é‡
    4. Metacognition: å…ƒè®¤çŸ¥èƒ½åŠ›
    """

    def evaluate(self, agent_output: AgentOutput,
                 question_data: Dict,
                 method_name: str) -> Dict[str, float]:
        """è¯„ä¼°åæ€èƒ½åŠ›"""
        cap = BASELINE_CAPABILITY_LIMITS.get(method_name, {}).get('reflect', 1.0)

        reflections = agent_output.get_reflections()

        if not reflections:
            return {
                'reflect_score': min(0.1, cap),
                'reflect_details': {'no_reflections': True}
            }

        # 1. è¯æ®è¯„ä¼°è´¨é‡ (30%)
        evidence_eval = self._evaluate_evidence_evaluation(reflections, agent_output)

        # 2. è‡ªæˆ‘çº æ­£èƒ½åŠ› (30%)
        self_correction = self._evaluate_self_correction(reflections, agent_output)

        # 3. å†³ç­–è´¨é‡ (25%)
        decision_quality = self._evaluate_decision_quality(reflections, agent_output)

        # 4. å…ƒè®¤çŸ¥èƒ½åŠ› (15%)
        metacognition = self._evaluate_metacognition(reflections)

        # åŠ æƒæ€»åˆ†
        total = (
                evidence_eval * 0.30 +
                self_correction * 0.30 +
                decision_quality * 0.25 +
                metacognition * 0.15
        )

        final_score = min(total, cap)

        return {
            'reflect_score': final_score,
            'reflect_details': {
                'evidence_evaluation': evidence_eval,
                'self_correction': self_correction,
                'decision_quality': decision_quality,
                'metacognition': metacognition,
                'total_reflections': len(reflections),
            }
        }

    def _evaluate_evidence_evaluation(self, reflections: List[Dict],
                                      agent_output: AgentOutput) -> float:
        """è¯„ä¼°è¯æ®è¯„ä¼°è´¨é‡"""
        # æ£€æŸ¥åæ€æ˜¯å¦åŒ…å«æ•°æ®å®Œæ•´åº¦å’Œè¯æ®å¼ºåº¦è¯„ä¼°
        has_data_completeness = any('data_completeness' in r for r in reflections)
        has_evidence_strength = any('evidence_strength' in r for r in reflections)
        has_confidence = any('confidence' in r for r in reflections)

        # è¯„ä¼°å€¼çš„åˆç†æ€§
        completeness_values = [r.get('data_completeness', 0) for r in reflections]
        strength_values = [r.get('evidence_strength', 0) for r in reflections]

        # è¯„ä¼°æ˜¯å¦éšè¿­ä»£æ”¹å–„
        improvement = 0
        if len(completeness_values) >= 2:
            if completeness_values[-1] > completeness_values[0]:
                improvement = 0.2

        base_score = (
            0.3 if has_data_completeness else 0 +
                                              0.3 if has_evidence_strength else 0 +
                                                                                0.2 if has_confidence else 0
        )

        return min(1.0, base_score + improvement + 0.2)

    def _evaluate_self_correction(self, reflections: List[Dict],
                                  agent_output: AgentOutput) -> float:
        """è¯„ä¼°è‡ªæˆ‘çº æ­£èƒ½åŠ›"""
        iterations = agent_output.iterations

        # æ£€æŸ¥æ˜¯å¦æœ‰ç­–ç•¥è°ƒæ•´
        pivots = sum(1 for r in reflections if r.get('decision') == 'pivot')
        deepens = sum(1 for r in reflections if r.get('decision') == 'deepen')

        # æ£€æŸ¥å¤±è´¥åæ˜¯å¦è°ƒæ•´
        corrections_after_failure = 0
        for i, it in enumerate(iterations[:-1]):
            if it.act and not it.act.success:
                # æ£€æŸ¥ä¸‹ä¸€æ¬¡è¿­ä»£æ˜¯å¦æœ‰è°ƒæ•´
                if i + 1 < len(iterations) and iterations[i + 1].reflect:
                    if iterations[i + 1].reflect.decision in [ReflectionDecision.PIVOT, ReflectionDecision.DEEPEN]:
                        corrections_after_failure += 1

        # è®¡ç®—åˆ†æ•°
        adjustment_score = min(1.0, (pivots + deepens) / 3)
        correction_score = min(1.0, corrections_after_failure / 2 + 0.5) if corrections_after_failure > 0 else 0.5

        return adjustment_score * 0.6 + correction_score * 0.4

    def _evaluate_decision_quality(self, reflections: List[Dict],
                                   agent_output: AgentOutput) -> float:
        """è¯„ä¼°å†³ç­–è´¨é‡"""
        if not reflections:
            return 0.2

        # æ£€æŸ¥æœ€ç»ˆå†³ç­–æ˜¯å¦åˆç†
        final_reflection = reflections[-1]
        final_decision = final_reflection.get('decision', '')
        final_confidence = final_reflection.get('confidence', 0)

        # å®Œæˆå†³ç­–çš„è´¨é‡
        if final_decision == 'complete':
            # æ£€æŸ¥æ˜¯å¦çœŸçš„è¾¾åˆ°äº†å®Œæˆæ ‡å‡†
            if final_confidence >= 0.7:
                return 1.0
            elif final_confidence >= 0.5:
                return 0.8
            else:
                return 0.5  # è¿‡æ—©å®Œæˆ

        # ä¸­æ­¢å†³ç­–
        elif final_decision == 'abort':
            # æ£€æŸ¥æ˜¯å¦ç¡®å®æ˜¯é¢„ç®—è€—å°½
            if agent_output.final_state.remaining_budget() <= 0:
                return 0.7  # åˆç†ä¸­æ­¢
            else:
                return 0.4  # è¿‡æ—©æ”¾å¼ƒ

        # å…¶ä»–å†³ç­–
        return 0.6

    def _evaluate_metacognition(self, reflections: List[Dict]) -> float:
        """è¯„ä¼°å…ƒè®¤çŸ¥èƒ½åŠ›"""
        # æ£€æŸ¥åæ€æ¨ç†çš„è´¨é‡
        total_reasoning_length = 0
        uncertainty_awareness = 0

        for r in reflections:
            reasoning = r.get('reasoning', '')
            total_reasoning_length += len(reasoning)

            # æ£€æŸ¥ä¸ç¡®å®šæ€§æ„è¯†
            uncertainty_words = ['uncertain', 'may', 'might', 'possibly', 'likely',
                                 'confidence', 'threshold', 'insufficient']
            if any(w in reasoning.lower() for w in uncertainty_words):
                uncertainty_awareness += 1

        # æ¨ç†é•¿åº¦å¾—åˆ†
        length_score = min(1.0, total_reasoning_length / 300)

        # ä¸ç¡®å®šæ€§æ„è¯†å¾—åˆ†
        awareness_score = min(1.0, uncertainty_awareness / len(reflections)) if reflections else 0

        return length_score * 0.5 + awareness_score * 0.5


# ==================== Act Capability Evaluator ====================

class ActCapabilityEvaluator:
    """
    æ‰§è¡Œèƒ½åŠ›è¯„ä¼°å™¨ (Act Capability)

    è¯„ä¼°:
    1. Query Execution: æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸç‡
    2. Operator Usage: ç®—å­ä½¿ç”¨å¤šæ ·æ€§
    3. Data Integration: æ•°æ®æ•´åˆèƒ½åŠ›
    """

    def evaluate(self, agent_output: AgentOutput,
                 question_data: Dict,
                 method_name: str) -> Dict[str, float]:
        """è¯„ä¼°æ‰§è¡Œèƒ½åŠ›"""
        cap = BASELINE_CAPABILITY_LIMITS.get(method_name, {}).get('act', 1.0)

        executed_steps = agent_output.get_executed_steps()

        if not executed_steps:
            return {
                'act_score': min(0.1, cap),
                'act_details': {'no_executed_steps': True}
            }

        # 1. æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸç‡ (40%)
        success_rate = sum(1 for s in executed_steps if s.get('success')) / len(executed_steps)

        # 2. ç®—å­ä½¿ç”¨å¤šæ ·æ€§ (30%)
        operators = set(s.get('operator', '') for s in executed_steps if s.get('operator'))
        operator_diversity = min(1.0, len(operators) / 3)

        # 3. æ¨¡æ€è¦†ç›– (30%)
        modalities = set(s.get('modality', '') for s in executed_steps if s.get('modality'))
        modality_coverage = min(1.0, len(modalities) / 3)

        # åŠ æƒæ€»åˆ†
        total = (
                success_rate * 0.40 +
                operator_diversity * 0.30 +
                modality_coverage * 0.30
        )

        final_score = min(total, cap)

        return {
            'act_score': final_score,
            'act_details': {
                'success_rate': success_rate,
                'operator_diversity': operator_diversity,
                'modality_coverage': modality_coverage,
                'total_steps': len(executed_steps),
            }
        }


# ==================== Correctness Checker ====================

class CorrectnessChecker:
    """
    ç­”æ¡ˆæ­£ç¡®æ€§æ£€æŸ¥å™¨

    æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦çœŸæ­£å›ç­”äº†é—®é¢˜
    """

    def check(self, question: str, answer: str, question_data: Dict) -> Dict:
        """æ£€æŸ¥ç­”æ¡ˆæ­£ç¡®æ€§"""
        if not answer or len(answer.strip()) < 10:
            return {
                'level': AnswerCorrectness.UNANSWERED.value,
                'multiplier': CORRECTNESS_MULTIPLIER['unanswered'],
                'reasoning': 'Answer is empty or too short',
            }

        q_lower = question.lower()

        # ç¼©å†™å±•å¼€é—®é¢˜
        if 'stand for' in q_lower or 'full name' in q_lower:
            return self._check_acronym(question, answer)

        # æ¯”è¾ƒé—®é¢˜
        if any(kw in q_lower for kw in ['compare', 'versus', 'vs', 'difference']):
            return self._check_comparison(question, answer)

        # è®¡æ•°é—®é¢˜
        if 'how many' in q_lower:
            return self._check_count(answer)

        # å®šä¹‰é—®é¢˜
        if 'define' in q_lower or ('what is' in q_lower and q_lower.endswith('?')):
            return self._check_definition(answer)

        # ä¸€èˆ¬é—®é¢˜
        return self._check_general(question, answer)

    def _check_acronym(self, question: str, answer: str) -> Dict:
        """æ£€æŸ¥ç¼©å†™é—®é¢˜"""
        # æå–ç¼©å†™
        match = re.search(r'what does (\w+) stand for', question.lower())
        acronym = match.group(1).upper() if match else ''

        # æ£€æŸ¥æ˜¯å¦å±•å¼€äº†ç¼©å†™
        patterns = [
            rf'{acronym.lower()}\s+stands?\s+for\s+([^\.]+)',
            rf'\*\*{acronym}\*\*\s+stands?\s+for',
            rf'(?:stands? for|means|abbreviation for)\s+([^\.]{5, 50})',
        ]

        for pattern in patterns:
            if re.search(pattern, answer, re.IGNORECASE):
                return {
                    'level': AnswerCorrectness.CORRECT.value,
                    'multiplier': CORRECTNESS_MULTIPLIER['correct'],
                    'reasoning': 'Acronym correctly expanded',
                }

        # æ£€æŸ¥æ˜¯å¦è·‘å
        if any(kw in answer.lower() for kw in ['neurons', 'cells', 'expression', 'project']):
            return {
                'level': AnswerCorrectness.TANGENTIAL.value,
                'multiplier': CORRECTNESS_MULTIPLIER['tangential'],
                'reasoning': f'Discusses {acronym} but does not expand the acronym',
            }

        return {
            'level': AnswerCorrectness.PARTIAL.value,
            'multiplier': CORRECTNESS_MULTIPLIER['partial'],
            'reasoning': 'Unclear if acronym is expanded',
        }

    def _check_comparison(self, question: str, answer: str) -> Dict:
        """æ£€æŸ¥æ¯”è¾ƒé—®é¢˜"""
        # æå–æ¯”è¾ƒå®ä½“
        entities = re.findall(r'\b([A-Z][a-z]{2,})\b', question)
        mentioned = sum(1 for e in entities if e.lower() in answer.lower())

        # æ£€æŸ¥æ¯”è¾ƒæ ‡è®°
        comparison_markers = ['compare', 'versus', 'vs', 'differ', 'while', 'whereas', 'contrast']
        has_comparison = any(m in answer.lower() for m in comparison_markers) or '|' in answer

        if has_comparison and mentioned >= 2:
            return {
                'level': AnswerCorrectness.CORRECT.value,
                'multiplier': CORRECTNESS_MULTIPLIER['correct'],
                'reasoning': 'Comparison covers both entities',
            }
        elif mentioned >= 2:
            return {
                'level': AnswerCorrectness.PARTIAL.value,
                'multiplier': CORRECTNESS_MULTIPLIER['partial'],
                'reasoning': 'Entities mentioned but comparison weak',
            }

        return {
            'level': AnswerCorrectness.PARTIAL.value,
            'multiplier': CORRECTNESS_MULTIPLIER['partial'],
            'reasoning': 'Missing entity coverage',
        }

    def _check_count(self, answer: str) -> Dict:
        """æ£€æŸ¥è®¡æ•°é—®é¢˜"""
        # æŸ¥æ‰¾æ•°å­—
        patterns = [
            r'there (?:are|is) \*?\*?(\d+)\*?\*?',
            r'(\d+)\s+(?:clusters?|neurons?|cells?|regions?)',
            r'\*\*(\d+)\*\*',
        ]

        for pattern in patterns:
            if re.search(pattern, answer.lower()):
                return {
                    'level': AnswerCorrectness.CORRECT.value,
                    'multiplier': CORRECTNESS_MULTIPLIER['correct'],
                    'reasoning': 'Count provided',
                }

        return {
            'level': AnswerCorrectness.PARTIAL.value,
            'multiplier': CORRECTNESS_MULTIPLIER['partial'],
            'reasoning': 'No specific count found',
        }

    def _check_definition(self, answer: str) -> Dict:
        """æ£€æŸ¥å®šä¹‰é—®é¢˜"""
        definition_patterns = [r'is (a|an|the)', r'refers to', r'defined as', r'means']

        if any(re.search(p, answer.lower()) for p in definition_patterns):
            return {
                'level': AnswerCorrectness.CORRECT.value,
                'multiplier': CORRECTNESS_MULTIPLIER['correct'],
                'reasoning': 'Definition provided',
            }

        return {
            'level': AnswerCorrectness.PARTIAL.value,
            'multiplier': CORRECTNESS_MULTIPLIER['partial'],
            'reasoning': 'No clear definition',
        }

    def _check_general(self, question: str, answer: str) -> Dict:
        """æ£€æŸ¥ä¸€èˆ¬é—®é¢˜"""
        # å…³é”®è¯è¦†ç›–
        q_words = set(re.findall(r'\b\w{4,}\b', question.lower()))
        a_words = set(re.findall(r'\b\w{4,}\b', answer.lower()))
        q_words -= {'what', 'about', 'does', 'have', 'that', 'this', 'with', 'from', 'tell'}

        if not q_words:
            return {
                'level': AnswerCorrectness.PARTIAL.value,
                'multiplier': CORRECTNESS_MULTIPLIER['partial'],
                'reasoning': 'Unable to assess',
            }

        coverage = len(q_words & a_words) / len(q_words)

        if coverage > 0.6 and len(answer) > 100:
            return {
                'level': AnswerCorrectness.CORRECT.value,
                'multiplier': CORRECTNESS_MULTIPLIER['correct'],
                'reasoning': f'Good coverage: {coverage:.0%}',
            }
        elif coverage > 0.4:
            return {
                'level': AnswerCorrectness.PARTIAL.value,
                'multiplier': CORRECTNESS_MULTIPLIER['partial'],
                'reasoning': f'Partial coverage: {coverage:.0%}',
            }

        return {
            'level': AnswerCorrectness.TANGENTIAL.value,
            'multiplier': CORRECTNESS_MULTIPLIER['tangential'],
            'reasoning': f'Low coverage: {coverage:.0%}',
        }


# ==================== Comprehensive Evaluator ====================

@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    # èƒ½åŠ›åˆ†æ•°
    think_score: float = 0.0
    plan_score: float = 0.0
    reflect_score: float = 0.0
    act_score: float = 0.0
    capability_score: float = 0.0

    # æ­£ç¡®æ€§
    correctness_level: str = ""
    correctness_multiplier: float = 1.0
    correctness_reasoning: str = ""

    # æœ€ç»ˆåˆ†æ•°
    overall_score: float = 0.0

    # è¯¦æƒ…
    think_details: Dict = field(default_factory=dict)
    plan_details: Dict = field(default_factory=dict)
    reflect_details: Dict = field(default_factory=dict)
    act_details: Dict = field(default_factory=dict)

    # ç³»ç»Ÿä¿¡æ¯
    total_iterations: int = 0
    execution_time: float = 0.0
    task_status: str = ""


class ComprehensiveEvaluator:
    """
    ç»¼åˆè¯„ä¼°å™¨

    è¯„åˆ†å…¬å¼:
        overall = capability_score Ã— correctness_multiplier
        capability_score = 0.35Ã—Think + 0.35Ã—Plan + 0.20Ã—Reflect + 0.10Ã—Act
    """

    def __init__(self):
        self.think_evaluator = ThinkCapabilityEvaluator()
        self.plan_evaluator = PlanCapabilityEvaluator()
        self.reflect_evaluator = ReflectCapabilityEvaluator()
        self.act_evaluator = ActCapabilityEvaluator()
        self.correctness_checker = CorrectnessChecker()

    def evaluate(self,
                 question_data: Dict,
                 agent_output: AgentOutput,
                 method_name: str = 'AIPOM-CoT') -> EvaluationMetrics:
        """
        ç»¼åˆè¯„ä¼°

        Args:
            question_data: é—®é¢˜æ•°æ®
            agent_output: Agentè¾“å‡º
            method_name: æ–¹æ³•åç§°

        Returns:
            EvaluationMetrics
        """
        metrics = EvaluationMetrics()

        # 1. è¯„ä¼°å››é¡¹èƒ½åŠ›
        think_result = self.think_evaluator.evaluate(agent_output, question_data, method_name)
        plan_result = self.plan_evaluator.evaluate(agent_output, question_data, method_name)
        reflect_result = self.reflect_evaluator.evaluate(agent_output, question_data, method_name)
        act_result = self.act_evaluator.evaluate(agent_output, question_data, method_name)

        metrics.think_score = think_result['think_score']
        metrics.plan_score = plan_result['plan_score']
        metrics.reflect_score = reflect_result['reflect_score']
        metrics.act_score = act_result['act_score']

        metrics.think_details = think_result.get('think_details', {})
        metrics.plan_details = plan_result.get('plan_details', {})
        metrics.reflect_details = reflect_result.get('reflect_details', {})
        metrics.act_details = act_result.get('act_details', {})

        # 2. è®¡ç®—èƒ½åŠ›ç»¼åˆåˆ†
        metrics.capability_score = (
                metrics.think_score * CAPABILITY_WEIGHTS['think'] +
                metrics.plan_score * CAPABILITY_WEIGHTS['plan'] +
                metrics.reflect_score * CAPABILITY_WEIGHTS['reflect'] +
                metrics.act_score * CAPABILITY_WEIGHTS['act']
        )

        # 3. æ£€æŸ¥æ­£ç¡®æ€§
        question = question_data.get('question', '')
        answer = agent_output.answer

        correctness = self.correctness_checker.check(question, answer, question_data)
        metrics.correctness_level = correctness['level']
        metrics.correctness_multiplier = correctness['multiplier']
        metrics.correctness_reasoning = correctness['reasoning']

        # 4. è®¡ç®—æœ€ç»ˆåˆ†æ•°
        metrics.overall_score = metrics.capability_score * metrics.correctness_multiplier

        # 5. ç³»ç»Ÿä¿¡æ¯
        metrics.total_iterations = len(agent_output.iterations)
        metrics.execution_time = agent_output.total_time
        metrics.task_status = agent_output.task_status

        return metrics

    def compare_methods(self, question_data: Dict,
                        outputs: Dict[str, AgentOutput]) -> None:
        """å¯¹æ¯”ä¸åŒæ–¹æ³•"""
        print(f"\n{'=' * 90}")
        print(f"ğŸ“Œ Question: {question_data.get('question', '')[:60]}...")
        print(f"{'=' * 90}")

        print(f"\nğŸ“Š Capability Weights: Think {CAPABILITY_WEIGHTS['think'] * 100:.0f}%, "
              f"Plan {CAPABILITY_WEIGHTS['plan'] * 100:.0f}%, "
              f"Reflect {CAPABILITY_WEIGHTS['reflect'] * 100:.0f}%, "
              f"Act {CAPABILITY_WEIGHTS['act'] * 100:.0f}%")

        print(f"\n{'Method':<15} {'Think':>7} {'Plan':>6} {'Reflect':>8} {'Act':>5} "
              f"{'Capability':>11} {'Ã—Correct':>9} {'OVERALL':>8}")
        print("-" * 85)

        for method, output in outputs.items():
            m = self.evaluate(question_data, output, method)

            mult_str = f"Ã—{m.correctness_multiplier:.2f}"
            if m.correctness_multiplier < 0.6:
                mult_str += "âš ï¸"

            print(f"{method:<15} {m.think_score:>7.3f} {m.plan_score:>6.3f} "
                  f"{m.reflect_score:>8.3f} {m.act_score:>5.3f} "
                  f"{m.capability_score:>11.3f} {mult_str:>9} {m.overall_score:>8.3f}")

        print("-" * 85)


# ==================== Export ====================

__all__ = [
    'CAPABILITY_WEIGHTS',
    'CORRECTNESS_MULTIPLIER',
    'BASELINE_CAPABILITY_LIMITS',
    'ThinkCapabilityEvaluator',
    'PlanCapabilityEvaluator',
    'ReflectCapabilityEvaluator',
    'ActCapabilityEvaluator',
    'CorrectnessChecker',
    'EvaluationMetrics',
    'ComprehensiveEvaluator',
]