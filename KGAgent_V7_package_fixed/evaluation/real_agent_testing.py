#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
KGAgent V7 çœŸå®Agentæµ‹è¯•é›†æˆ
å®Œæ•´å±•ç¤ºå¦‚ä½•å°†è¯„ä¼°ç³»ç»Ÿä¸æ‚¨çš„çœŸå®Agentè¿æ¥
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== STEP 1: å¯¼å…¥æ‚¨çš„çœŸå®Agent ====================
# å°†agent_v7ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    # å¯¼å…¥æ‚¨çš„çœŸå®Agentç±»
    from agent_v7.neo4j_exec import Neo4jExec
    from agent_v7.agent_v7 import KGAgentV7
    from agent_v7.schema_cache import SchemaCache
    from agent_v7.llm import LLMClient

    AGENT_AVAILABLE = True
    logger.info("âœ… Successfully imported KGAgent V7")
except ImportError as e:
    logger.warning(f"âš ï¸ Could not import KGAgent V7: {e}")
    logger.warning("Using Mock Agent for demonstration")
    AGENT_AVAILABLE = False


# ==================== STEP 2: é…ç½®è¿æ¥å‚æ•° ====================
class AgentConfig:
    """Agenté…ç½®ç®¡ç†"""

    @staticmethod
    def get_config() -> Dict[str, Any]:
        """è·å–Agenté…ç½®"""
        return {
            # Neo4jæ•°æ®åº“é…ç½®
            'neo4j_uri': os.getenv('NEO4J_URI', 'bolt://100.88.72.32:7687'),
            'neo4j_user': os.getenv('NEO4J_USER', 'neo4j'),
            'neo4j_pwd': os.getenv('NEO4J_PASSWORD', 'neuroxiv'),
            'database': os.getenv('NEO4J_DATABASE', 'neo4j'),

            # OpenAIé…ç½®
            'openai_api_key': os.getenv('OPENAI_API_KEY', ''),
            'planner_model': os.getenv('PLANNER_MODEL', 'gpt-5'),
            'summarizer_model': os.getenv('SUMMARIZER_MODEL', 'gpt-4o'),

            # è¯„ä¼°é…ç½®
            'max_rounds': 3,
            'timeout': 60,  # æ¯ä¸ªæµ‹è¯•çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        }

    @staticmethod
    def validate_config(config: Dict[str, Any]) -> bool:
        """éªŒè¯é…ç½®æ˜¯å¦å®Œæ•´"""
        required = ['neo4j_uri', 'neo4j_user', 'neo4j_pwd', 'openai_api_key']
        missing = [k for k in required if not config.get(k)]

        if missing:
            logger.error(f"âŒ Missing required config: {missing}")
            logger.info("Please set environment variables or update config")
            return False

        logger.info("âœ… Configuration validated")
        return True


# ==================== STEP 3: åˆ›å»ºAgentåŒ…è£…å™¨ ====================
class RealAgentWrapper:
    """çœŸå®Agentçš„åŒ…è£…å™¨ï¼Œç”¨äºè¯„ä¼°ç³»ç»Ÿ"""

    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–çœŸå®Agent"""
        self.config = config

        if AGENT_AVAILABLE:
            # åˆ›å»ºçœŸå®çš„Agentå®ä¾‹
            self.agent = KGAgentV7(
                neo4j_uri=config['neo4j_uri'],
                neo4j_user=config['neo4j_user'],
                neo4j_pwd=config['neo4j_pwd'],
                database=config['database'],
                openai_api_key=config['openai_api_key'],
                planner_model=config['planner_model'],
                summarizer_model=config['summarizer_model']
            )
            logger.info("âœ… Real KGAgent V7 initialized")
        else:
            # ä½¿ç”¨Mock Agent
            from run_evaluation import MockKGAgentV7
            self.agent = MockKGAgentV7(**config)
            logger.info("ğŸ“¦ Using Mock Agent for testing")

    def test_connection(self) -> bool:
        """æµ‹è¯•Agentè¿æ¥"""
        try:
            # æµ‹è¯•Neo4jè¿æ¥
            if AGENT_AVAILABLE:
                with self.agent.db.driver.session() as session:
                    result = session.run("MATCH (n) RETURN count(n) as count LIMIT 1")
                    count = result.single()['count']
                    logger.info(f"âœ… Neo4j connected: {count} nodes found")

            # æµ‹è¯•ä¸€ä¸ªç®€å•æŸ¥è¯¢
            test_question = "What labels exist in the knowledge graph?"
            result = self.agent.answer(test_question, max_rounds=1)

            if result and 'final' in result:
                logger.info("âœ… Agent test query successful")
                return True
            else:
                logger.error("âŒ Agent test query failed")
                return False

        except Exception as e:
            logger.error(f"âŒ Connection test failed: {e}")
            return False

    def answer(self, question: str, max_rounds: int = None) -> Dict[str, Any]:
        """è°ƒç”¨Agentå›ç­”é—®é¢˜ï¼ˆä¸è¯„ä¼°ç³»ç»Ÿçš„æ¥å£ï¼‰"""
        max_rounds = max_rounds or self.config.get('max_rounds', 2)
        return self.agent.answer(question, max_rounds=max_rounds)


# ==================== STEP 4: å¯¼å…¥è¯„ä¼°ç³»ç»Ÿ ====================
USE_IMPROVED_EVALUATION = True  # åˆ‡æ¢åˆ°æ”¹è¿›ç‰ˆ

if USE_IMPROVED_EVALUATION:
    from improved_evaluation import (
        ImprovedEvaluator,
        ImprovedEvaluationMetrics,
        run_improved_evaluation,
        ScientificVisualizer
    )
    # ä½¿ç”¨æ”¹è¿›ç‰ˆçš„TestCaseå®šä¹‰
    from evaluation import TestCase, TestSuite
else:
    from evaluation import (
        TestCase, TestSuite, EvaluationMetrics,
        KGAgentEvaluator, EvaluationVisualizer
    )


# ==================== STEP 5: è‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹ ====================
class CustomTestSuite(TestSuite):
    """è‡ªå®šä¹‰æµ‹è¯•å¥—ä»¶ï¼Œé’ˆå¯¹æ‚¨çš„KGè®¾è®¡"""

    def _create_test_cases(self) -> List[TestCase]:
        """åˆ›å»ºé’ˆå¯¹ç¥ç»ç§‘å­¦KGçš„æµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # === Level 1: åŸºç¡€KGæŸ¥è¯¢ ===
        cases.append(TestCase(
            id="neuro_basic_1",
            category="kg_navigation",
            question="What are the morphological properties of the MOp region?",
            complexity=1,
            required_capabilities=["schema_navigation", "property_extraction"],
            expected_patterns=["MOp", "morpholog", "axon", "dendrit"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_basic_2",
            category="kg_navigation",
            question="List all subclasses in the CLA region",
            complexity=1,
            required_capabilities=["relationship_traversal"],
            expected_patterns=["CLA", "HAS_SUBCLASS", "Subclass"],
            tool_requirements=[]
        ))

        # === Level 2: å…³ç³»æ¢ç´¢ ===
        cases.append(TestCase(
            id="neuro_relation_1",
            category="kg_navigation",
            question="Which regions does VISp project to with the strongest connections?",
            complexity=2,
            required_capabilities=["relationship_traversal", "sorting"],
            expected_patterns=["VISp", "PROJECT_TO", "weight", "ORDER BY"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_relation_2",
            category="kg_navigation",
            question="Find regions with high axonal branching",
            complexity=2,
            required_capabilities=["filtering", "comparison"],
            expected_patterns=["axonal_branches", "WHERE", ">"],
            tool_requirements=[]
        ))

        # === Level 3: åˆ†ææ¨ç† ===
        cases.append(TestCase(
            id="neuro_reasoning_1",
            category="reasoning",
            question="Compare the morphological diversity between motor (MOp) and visual (VISp) cortex regions",
            complexity=3,
            required_capabilities=["comparison", "aggregation", "multi_region"],
            expected_patterns=["MOp", "VISp", "morpholog"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_reasoning_2",
            category="reasoning",
            question="Which regions show the highest axon-to-dendrite length ratio and what might this indicate?",
            complexity=3,
            required_capabilities=["computation", "interpretation"],
            expected_patterns=["axonal_length", "dendritic_length", "ratio", "CASE"],
            tool_requirements=[]
        ))

        # === Level 4: å·¥å…·ä½¿ç”¨ ===
        cases.append(TestCase(
            id="neuro_tool_1",
            category="tool_use",
            question="Calculate the mismatch index between MOp and SSp regions using their morphological and transcriptomic profiles",
            complexity=4,
            required_capabilities=["tool_invocation", "vector_computation"],
            expected_patterns=["MOp", "SSp", "HAS_SUBCLASS", "morpholog"],
            tool_requirements=["compute_mismatch_index"]
        ))

        cases.append(TestCase(
            id="neuro_tool_2",
            category="tool_use",
            question="Compute statistical metrics for dendritic branching patterns across all cortical regions",
            complexity=4,
            required_capabilities=["statistics", "aggregation"],
            expected_patterns=["dendritic_branches", "Region"],
            tool_requirements=["basic_stats"]
        ))

        # === Level 5: ç»¼åˆåˆ†æ ===
        cases.append(TestCase(
            id="neuro_complex_1",
            category="complex",
            question="Analyze the relationship between morphological complexity and transcriptomic diversity across the cortical hierarchy, identify regions with significant mismatch",
            complexity=5,
            required_capabilities=["multi_hop", "correlation", "tool_use", "interpretation"],
            expected_patterns=["morpholog", "transcriptom", "HAS_SUBCLASS"],
            tool_requirements=["compute_mismatch_index", "basic_stats"]
        ))

        cases.append(TestCase(
            id="neuro_complex_2",
            category="complex",
            question="Identify Car3-expressing neurons' projection patterns and analyze their morphological characteristics compared to other interneuron subtypes",
            complexity=5,
            required_capabilities=["specific_subclass", "projection_analysis", "comparison"],
            expected_patterns=["Car3", "PROJECT_TO", "morpholog", "interneuron"],
            tool_requirements=[]
        ))

        return cases


# ==================== STEP 6: è¿è¡ŒçœŸå®è¯„ä¼° ====================
class RealEvaluationRunner:
    """çœŸå®è¯„ä¼°è¿è¡Œå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.agent_wrapper = None
        self.evaluator = None
        self.results = None

    def setup(self) -> bool:
        """è®¾ç½®è¯„ä¼°ç¯å¢ƒ"""
        logger.info("\n" + "=" * 60)
        logger.info("SETTING UP EVALUATION ENVIRONMENT")
        logger.info("=" * 60)

        # éªŒè¯é…ç½®
        if not AgentConfig.validate_config(self.config):
            return False

        # åˆ›å»ºAgent
        self.agent_wrapper = RealAgentWrapper(self.config)

        # æµ‹è¯•è¿æ¥
        if not self.agent_wrapper.test_connection():
            logger.error("âŒ Agent connection test failed")
            return False

        # åˆ›å»ºè¯„ä¼°å™¨ï¼ˆä½¿ç”¨è‡ªå®šä¹‰æµ‹è¯•å¥—ä»¶ï¼‰
        self.evaluator = KGAgentEvaluator(self.agent_wrapper)
        self.evaluator.test_suite = CustomTestSuite()

        logger.info("âœ… Evaluation environment ready")
        return True

    def run_evaluation(self,
                       test_subset: List[str] = None,
                       output_dir: str = "evaluation_results") -> pd.DataFrame:
        """
        è¿è¡Œè¯„ä¼°

        Args:
            test_subset: è¦è¿è¡Œçš„æµ‹è¯•IDåˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºè¿è¡Œæ‰€æœ‰ï¼‰
            output_dir: è¾“å‡ºç›®å½•
        """
        if not self.evaluator:
            logger.error("âŒ Evaluator not initialized. Run setup() first.")
            return None

        logger.info("\n" + "=" * 60)
        logger.info("RUNNING EVALUATION")
        logger.info("=" * 60)

        # é€‰æ‹©æµ‹è¯•ç”¨ä¾‹
        test_cases = self.evaluator.test_suite.test_cases
        if test_subset:
            test_cases = [tc for tc in test_cases if tc.id in test_subset]
            logger.info(f"Running subset: {[tc.id for tc in test_cases]}")
        else:
            logger.info(f"Running all {len(test_cases)} test cases")

        # è¿è¡Œè¯„ä¼°
        all_metrics = []
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"\n[{i}/{len(test_cases)}] Evaluating: {test_case.id}")
            logger.info(f"  Question: {test_case.question}")

            try:
                result, metrics = self.evaluator.evaluate_single(test_case)

                # è®°å½•ç»“æœ
                metric_dict = {
                    'test_id': test_case.id,
                    'category': test_case.category,
                    'complexity': test_case.complexity,
                    **metrics.__dict__
                }
                all_metrics.append(metric_dict)

                logger.info(f"  âœ… Success - Score: {metrics.final_answer_quality:.2f}")

            except Exception as e:
                logger.error(f"  âŒ Error: {e}")
                # æ·»åŠ å¤±è´¥è®°å½•
                all_metrics.append({
                    'test_id': test_case.id,
                    'category': test_case.category,
                    'complexity': test_case.complexity,
                    'error': str(e)
                })

        # åˆ›å»ºDataFrame
        self.results = pd.DataFrame(all_metrics)

        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        self.results.to_csv(output_path / "evaluation_results.csv", index=False)
        logger.info(f"\nâœ… Results saved to: {output_path / 'evaluation_results.csv'}")

        return self.results

    def generate_visualizations(self, output_dir: str = "evaluation_results"):
        """ç”Ÿæˆå¯è§†åŒ–"""
        if self.results is None or self.results.empty:
            logger.error("âŒ No results to visualize")
            return

        logger.info("\n" + "=" * 60)
        logger.info("GENERATING VISUALIZATIONS")
        logger.info("=" * 60)

        # å¯¼å…¥å¯è§†åŒ–æ¨¡å—
        from visualization import AdvancedVisualizer

        # ç”Ÿæˆå›¾è¡¨
        visualizer = AdvancedVisualizer(self.results, output_dir)
        visualizer.generate_all_figures()

        logger.info(f"âœ… Visualizations saved to: {output_dir}/")

    def print_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        if self.results is None or self.results.empty:
            return

        logger.info("\n" + "=" * 60)
        logger.info("EVALUATION SUMMARY")
        logger.info("=" * 60)

        # è¿‡æ»¤æ‰é”™è¯¯è®°å½•
        valid_results = self.results[~self.results.get('error', pd.Series()).notna()]

        if not valid_results.empty:
            # è®¡ç®—ç»Ÿè®¡
            logger.info(f"\nğŸ“Š Tests Completed: {len(valid_results)}/{len(self.results)}")

            if 'overall_score' not in valid_results.columns:
                valid_results['overall_score'] = (
                        valid_results.get('autonomy_score', 0) * 0.3 +
                        valid_results.get('planning_quality', 0) * 0.2 +
                        valid_results.get('tool_selection_accuracy', 0) * 0.2 +
                        valid_results.get('final_answer_quality', 0) * 0.3
                )

            logger.info(f"\nğŸ¯ Performance Metrics:")
            for metric in ['autonomy_score', 'planning_quality', 'tool_selection_accuracy', 'final_answer_quality']:
                if metric in valid_results.columns:
                    mean_val = valid_results[metric].mean()
                    std_val = valid_results[metric].std()
                    logger.info(f"  â€¢ {metric}: {mean_val:.3f} Â± {std_val:.3f}")

            logger.info(f"\nğŸ† Overall Score: {valid_results['overall_score'].mean():.3f}")

            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            logger.info(f"\nğŸ“ˆ By Category:")
            for cat in valid_results['category'].unique():
                cat_data = valid_results[valid_results['category'] == cat]
                logger.info(f"  â€¢ {cat}: {len(cat_data)} tests, avg score: {cat_data['overall_score'].mean():.3f}")


# ==================== STEP 7: ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒçœŸå®Agentè¯„ä¼°"""
    import argparse

    parser = argparse.ArgumentParser(description='Run KGAgent V7 Real Evaluation')
    parser.add_argument('--subset', nargs='+', help='Test IDs to run (default: all)')
    parser.add_argument('--output', default='evaluation_results', help='Output directory')
    parser.add_argument('--skip-viz', action='store_true', help='Skip visualization generation')
    parser.add_argument('--config-file', help='JSON config file path')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    if args.config_file:
        with open(args.config_file, 'r') as f:
            config = json.load(f)
        logger.info(f"Loaded config from: {args.config_file}")
    else:
        config = AgentConfig.get_config()

    # åˆ›å»ºè¿è¡Œå™¨
    runner = RealEvaluationRunner(config)

    # è®¾ç½®ç¯å¢ƒ
    if not runner.setup():
        logger.error("âŒ Setup failed. Exiting.")
        return 1

    # è¿è¡Œè¯„ä¼°
    results = runner.run_evaluation(
        test_subset=args.subset,
        output_dir=args.output
    )

    if results is not None and not results.empty:
        # ç”Ÿæˆå¯è§†åŒ–
        if not args.skip_viz:
            runner.generate_visualizations(args.output)

        # æ‰“å°æ‘˜è¦
        runner.print_summary()

        logger.info("\n" + "=" * 60)
        logger.info("âœ… EVALUATION COMPLETE")
        logger.info("=" * 60)
        return 0
    else:
        logger.error("âŒ Evaluation failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())