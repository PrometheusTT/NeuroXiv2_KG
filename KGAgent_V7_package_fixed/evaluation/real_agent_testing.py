#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
KGAgent V7 ÁúüÂÆûAgentÊµãËØïÈõÜÊàê
ÂÆåÊï¥Â±ïÁ§∫Â¶Ç‰ΩïÂ∞ÜËØÑ‰º∞Á≥ªÁªü‰∏éÊÇ®ÁöÑÁúüÂÆûAgentËøûÊé•
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== STEP 1: ÂØºÂÖ•ÊÇ®ÁöÑÁúüÂÆûAgent ====================
# Â∞Üagent_v7ÁõÆÂΩïÊ∑ªÂä†Âà∞PythonË∑ØÂæÑ
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    # ÂØºÂÖ•ÊÇ®ÁöÑÁúüÂÆûAgentÁ±ª
    from agent_v7.neo4j_exec import Neo4jExec
    from agent_v7.agent_v7 import KGAgentV7
    from agent_v7.schema_cache import SchemaCache
    from agent_v7.llm import LLMClient

    AGENT_AVAILABLE = True
    logger.info("‚úÖ Successfully imported KGAgent V7")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Could not import KGAgent V7: {e}")
    logger.warning("Using Mock Agent for demonstration")
    AGENT_AVAILABLE = False


# ==================== STEP 2: ÈÖçÁΩÆËøûÊé•ÂèÇÊï∞ ====================
class AgentConfig:
    """AgentÈÖçÁΩÆÁÆ°ÁêÜ"""

    @staticmethod
    def get_config() -> Dict[str, Any]:
        """Ëé∑ÂèñAgentÈÖçÁΩÆ"""
        return {
            # Neo4jÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ
            'neo4j_uri': os.getenv('NEO4J_URI', 'bolt://100.88.72.32:7687'),
            'neo4j_user': os.getenv('NEO4J_USER', 'neo4j'),
            'neo4j_pwd': os.getenv('NEO4J_PASSWORD', 'neuroxiv'),
            'database': os.getenv('NEO4J_DATABASE', 'neo4j'),

            # OpenAIÈÖçÁΩÆ
            'openai_api_key': os.getenv('OPENAI_API_KEY', ''),
            'planner_model': os.getenv('PLANNER_MODEL', 'gpt-5'),
            'summarizer_model': os.getenv('SUMMARIZER_MODEL', 'gpt-4o'),

            # ËØÑ‰º∞ÈÖçÁΩÆ
            'max_rounds': 3,
            'timeout': 60,  # ÊØè‰∏™ÊµãËØïÁöÑË∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºâ
        }

    @staticmethod
    def validate_config(config: Dict[str, Any]) -> bool:
        """È™åËØÅÈÖçÁΩÆÊòØÂê¶ÂÆåÊï¥"""
        required = ['neo4j_uri', 'neo4j_user', 'neo4j_pwd', 'openai_api_key']
        missing = [k for k in required if not config.get(k)]

        if missing:
            logger.error(f"‚ùå Missing required config: {missing}")
            logger.info("Please set environment variables or update config")
            return False

        logger.info("‚úÖ Configuration validated")
        return True


# ==================== STEP 3: ÂàõÂª∫AgentÂåÖË£ÖÂô® ====================
class RealAgentWrapper:
    """ÁúüÂÆûAgentÁöÑÂåÖË£ÖÂô®ÔºåÁî®‰∫éËØÑ‰º∞Á≥ªÁªü"""

    def __init__(self, config: Dict[str, Any]):
        """ÂàùÂßãÂåñÁúüÂÆûAgent"""
        self.config = config

        if AGENT_AVAILABLE:
            # ÂàõÂª∫ÁúüÂÆûÁöÑAgentÂÆû‰æã
            self.agent = KGAgentV7(
                neo4j_uri=config['neo4j_uri'],
                neo4j_user=config['neo4j_user'],
                neo4j_pwd=config['neo4j_pwd'],
                database=config['database'],
                openai_api_key=config['openai_api_key'],
                planner_model=config['planner_model'],
                summarizer_model=config['summarizer_model']
            )
            logger.info("‚úÖ Real KGAgent V7 initialized")
        else:
            # ‰ΩøÁî®Mock Agent
            from run_evaluation import MockKGAgentV7
            self.agent = MockKGAgentV7(**config)
            logger.info("üì¶ Using Mock Agent for testing")

    def test_connection(self) -> bool:
        """ÊµãËØïAgentËøûÊé•"""
        try:
            # ÊµãËØïNeo4jËøûÊé•
            if AGENT_AVAILABLE:
                with self.agent.db.driver.session() as session:
                    result = session.run("MATCH (n) RETURN count(n) as count LIMIT 1")
                    count = result.single()['count']
                    logger.info(f"‚úÖ Neo4j connected: {count} nodes found")

            # ÊµãËØï‰∏Ä‰∏™ÁÆÄÂçïÊü•ËØ¢
            test_question = "What labels exist in the knowledge graph?"
            result = self.agent.answer(test_question, max_rounds=1)

            if result and 'final' in result:
                logger.info("‚úÖ Agent test query successful")
                return True
            else:
                logger.error("‚ùå Agent test query failed")
                return False

        except Exception as e:
            logger.error(f"‚ùå Connection test failed: {e}")
            return False

    def answer(self, question: str, max_rounds: int = None) -> Dict[str, Any]:
        """Ë∞ÉÁî®AgentÂõûÁ≠îÈóÆÈ¢òÔºà‰∏éËØÑ‰º∞Á≥ªÁªüÁöÑÊé•Âè£Ôºâ"""
        max_rounds = max_rounds or self.config.get('max_rounds', 2)
        return self.agent.answer(question, max_rounds=max_rounds)


# ==================== STEP 4: ÂØºÂÖ•ËØÑ‰º∞Á≥ªÁªü ====================
USE_IMPROVED_EVALUATION = True  # ÂàáÊç¢Âà∞ÊîπËøõÁâà

if USE_IMPROVED_EVALUATION:
    from improved_evaluation import (
        ImprovedEvaluator,
        ImprovedEvaluationMetrics,
        run_improved_evaluation,
        ScientificVisualizer
    )
    # ‰ΩøÁî®ÊîπËøõÁâàÁöÑTestCaseÂÆö‰πâ
    from evaluation import TestCase, TestSuite
else:
    from evaluation import (
        TestCase, TestSuite, EvaluationMetrics,
        KGAgentEvaluator, EvaluationVisualizer
    )


# ==================== STEP 5: Ëá™ÂÆö‰πâÊµãËØïÁî®‰æã ====================
class CustomTestSuite(TestSuite):
    """Ëá™ÂÆö‰πâÊµãËØïÂ•ó‰ª∂ÔºåÈíàÂØπÊÇ®ÁöÑKGËÆæËÆ°"""

    def _create_test_cases(self) -> List[TestCase]:
        """ÂàõÂª∫ÈíàÂØπÁ•ûÁªèÁßëÂ≠¶KGÁöÑÊµãËØïÁî®‰æã"""
        cases = []

        # === Level 1: Âü∫Á°ÄKGÊü•ËØ¢ ===
        cases.append(TestCase(
            id="neuro_basic_1",
            category="kg_navigation",
            question="What are the morphological properties of the MOp region?",
            complexity=1,
            required_capabilities=["schema_navigation", "property_extraction"],
            expected_patterns=["MOp", "morpholog", "axon", "dendrit"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_basic_2",
            category="kg_navigation",
            question="List all subclasses in the CLA region",
            complexity=1,
            required_capabilities=["relationship_traversal"],
            expected_patterns=["CLA", "HAS_SUBCLASS", "Subclass"],
            tool_requirements=[]
        ))

        # === Level 2: ÂÖ≥Á≥ªÊé¢Á¥¢ ===
        cases.append(TestCase(
            id="neuro_relation_1",
            category="kg_navigation",
            question="Which regions does VISp project to with the strongest connections?",
            complexity=2,
            required_capabilities=["relationship_traversal", "sorting"],
            expected_patterns=["VISp", "PROJECT_TO", "weight", "ORDER BY"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_relation_2",
            category="kg_navigation",
            question="Find regions with high axonal branching",
            complexity=2,
            required_capabilities=["filtering", "comparison"],
            expected_patterns=["axonal_branches", "WHERE", ">"],
            tool_requirements=[]
        ))

        # === Level 3: ÂàÜÊûêÊé®ÁêÜ ===
        cases.append(TestCase(
            id="neuro_reasoning_1",
            category="reasoning",
            question="Compare the morphological diversity between motor (MOp) and visual (VISp) cortex regions",
            complexity=3,
            required_capabilities=["comparison", "aggregation", "multi_region"],
            expected_patterns=["MOp", "VISp", "morpholog"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_reasoning_2",
            category="reasoning",
            question="Which regions show the highest axon-to-dendrite length ratio and what might this indicate?",
            complexity=3,
            required_capabilities=["computation", "interpretation"],
            expected_patterns=["axonal_length", "dendritic_length", "ratio", "CASE"],
            tool_requirements=[]
        ))

        # === Level 4: Â∑•ÂÖ∑‰ΩøÁî® ===
        cases.append(TestCase(
            id="neuro_tool_1",
            category="tool_use",
            question="Calculate the mismatch index between MOp and SSp regions using their morphological and transcriptomic profiles",
            complexity=4,
            required_capabilities=["tool_invocation", "vector_computation"],
            expected_patterns=["MOp", "SSp", "HAS_SUBCLASS", "morpholog"],
            tool_requirements=["compute_mismatch_index"]
        ))

        cases.append(TestCase(
            id="neuro_tool_2",
            category="tool_use",
            question="Compute statistical metrics for dendritic branching patterns across all cortical regions",
            complexity=4,
            required_capabilities=["statistics", "aggregation"],
            expected_patterns=["dendritic_branches", "Region"],
            tool_requirements=["basic_stats"]
        ))

        # === Level 5: ÁªºÂêàÂàÜÊûê ===
        cases.append(TestCase(
            id="neuro_complex_1",
            category="complex",
            question="Analyze the relationship between morphological complexity and transcriptomic diversity across the cortical hierarchy, identify regions with significant mismatch",
            complexity=5,
            required_capabilities=["multi_hop", "correlation", "tool_use", "interpretation"],
            expected_patterns=["morpholog", "transcriptom", "HAS_SUBCLASS"],
            tool_requirements=["compute_mismatch_index", "basic_stats"]
        ))

        cases.append(TestCase(
            id="neuro_complex_2",
            category="complex",
            question="Identify Car3-expressing neurons' projection patterns and analyze their morphological characteristics compared to other interneuron subtypes",
            complexity=5,
            required_capabilities=["specific_subclass", "projection_analysis", "comparison"],
            expected_patterns=["Car3", "PROJECT_TO", "morpholog", "interneuron"],
            tool_requirements=[]
        ))

        return cases


# ==================== STEP 6: ËøêË°åÁúüÂÆûËØÑ‰º∞ ====================
class RealEvaluationRunner:
    """ÁúüÂÆûËØÑ‰º∞ËøêË°åÂô®"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.agent_wrapper = None
        self.evaluator = None
        self.results = None

    def setup(self) -> bool:
        """ËÆæÁΩÆËØÑ‰º∞ÁéØÂ¢É"""
        logger.info("\n" + "=" * 60)
        logger.info("SETTING UP EVALUATION ENVIRONMENT")
        logger.info("=" * 60)

        # È™åËØÅÈÖçÁΩÆ
        if not AgentConfig.validate_config(self.config):
            return False

        # ÂàõÂª∫Agent
        self.agent_wrapper = RealAgentWrapper(self.config)

        # ÊµãËØïËøûÊé•
        if not self.agent_wrapper.test_connection():
            logger.error("‚ùå Agent connection test failed")
            return False

        # ÂàõÂª∫ËØÑ‰º∞Âô®Ôºà‰ΩøÁî®Ëá™ÂÆö‰πâÊµãËØïÂ•ó‰ª∂Ôºâ
        self.evaluator = KGAgentEvaluator(self.agent_wrapper)
        self.evaluator.test_suite = CustomTestSuite()

        logger.info("‚úÖ Evaluation environment ready")
        return True

    def run_evaluation(self,
                       test_subset: List[str] = None,
                       output_dir: str = "evaluation_results") -> pd.DataFrame:
        """
        ËøêË°åËØÑ‰º∞

        Args:
            test_subset: Ë¶ÅËøêË°åÁöÑÊµãËØïIDÂàóË°®ÔºàNoneË°®Á§∫ËøêË°åÊâÄÊúâÔºâ
            output_dir: ËæìÂá∫ÁõÆÂΩï
        """
        if not self.evaluator:
            logger.error("‚ùå Evaluator not initialized. Run setup() first.")
            return None

        logger.info("\n" + "=" * 60)
        logger.info("RUNNING EVALUATION")
        logger.info("=" * 60)

        # ÈÄâÊã©ÊµãËØïÁî®‰æã
        test_cases = self.evaluator.test_suite.test_cases
        if test_subset:
            test_cases = [tc for tc in test_cases if tc.id in test_subset]
            logger.info(f"Running subset: {[tc.id for tc in test_cases]}")
        else:
            logger.info(f"Running all {len(test_cases)} test cases")

        # ËøêË°åËØÑ‰º∞
        all_metrics = []
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"\n[{i}/{len(test_cases)}] Evaluating: {test_case.id}")
            logger.info(f"  Question: {test_case.question}")

            try:
                result, metrics = self.evaluator.evaluate_single(test_case)

                # ËÆ∞ÂΩïÁªìÊûú
                metric_dict = {
                    'test_id': test_case.id,
                    'category': test_case.category,
                    'complexity': test_case.complexity,
                    **metrics.__dict__
                }
                all_metrics.append(metric_dict)

                logger.info(f"  ‚úÖ Success - Score: {metrics.final_answer_quality:.2f}")

            except Exception as e:
                logger.error(f"  ‚ùå Error: {e}")
                # Ê∑ªÂä†Â§±Ë¥•ËÆ∞ÂΩï
                all_metrics.append({
                    'test_id': test_case.id,
                    'category': test_case.category,
                    'complexity': test_case.complexity,
                    'error': str(e)
                })

        # ÂàõÂª∫DataFrame
        self.results = pd.DataFrame(all_metrics)

        # ‰øùÂ≠òÁªìÊûú
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        self.results.to_csv(output_path / "evaluation_results.csv", index=False)
        logger.info(f"\n‚úÖ Results saved to: {output_path / 'evaluation_results.csv'}")

        return self.results

    def generate_visualizations(self, output_dir: str = "evaluation_results"):
        """ÁîüÊàêÂèØËßÜÂåñ"""
        if self.results is None or self.results.empty:
            logger.error("‚ùå No results to visualize")
            return

        logger.info("\n" + "=" * 60)
        logger.info("GENERATING VISUALIZATIONS")
        logger.info("=" * 60)

        # ÂØºÂÖ•ÂèØËßÜÂåñÊ®°Âùó
        from visualization import AdvancedVisualizer

        # ÁîüÊàêÂõæË°®
        visualizer = AdvancedVisualizer(self.results, output_dir)
        visualizer.generate_all_figures()

        logger.info(f"‚úÖ Visualizations saved to: {output_dir}/")

    def print_summary(self):
        """ÊâìÂç∞ËØÑ‰º∞ÊëòË¶Å"""
        if self.results is None or self.results.empty:
            return

        logger.info("\n" + "=" * 60)
        logger.info("EVALUATION SUMMARY")
        logger.info("=" * 60)

        # ËøáÊª§ÊéâÈîôËØØËÆ∞ÂΩï
        valid_results = self.results[~self.results.get('error', pd.Series()).notna()]

        if not valid_results.empty:
            # ËÆ°ÁÆóÁªüËÆ°
            logger.info(f"\nüìä Tests Completed: {len(valid_results)}/{len(self.results)}")

            if 'overall_score' not in valid_results.columns:
                valid_results['overall_score'] = (
                        valid_results.get('autonomy_score', 0) * 0.3 +
                        valid_results.get('planning_quality', 0) * 0.2 +
                        valid_results.get('tool_selection_accuracy', 0) * 0.2 +
                        valid_results.get('final_answer_quality', 0) * 0.3
                )

            logger.info(f"\nüéØ Performance Metrics:")
            for metric in ['autonomy_score', 'planning_quality', 'tool_selection_accuracy', 'final_answer_quality']:
                if metric in valid_results.columns:
                    mean_val = valid_results[metric].mean()
                    std_val = valid_results[metric].std()
                    logger.info(f"  ‚Ä¢ {metric}: {mean_val:.3f} ¬± {std_val:.3f}")

            logger.info(f"\nüèÜ Overall Score: {valid_results['overall_score'].mean():.3f}")

            # ÊåâÁ±ªÂà´ÁªüËÆ°
            logger.info(f"\nüìà By Category:")
            for cat in valid_results['category'].unique():
                cat_data = valid_results[valid_results['category'] == cat]
                logger.info(f"  ‚Ä¢ {cat}: {len(cat_data)} tests, avg score: {cat_data['overall_score'].mean():.3f}")


# ==================== STEP 7: ‰∏ªÂáΩÊï∞ ====================
def main():
    """‰∏ªÂáΩÊï∞ÔºöËøêË°åÁúüÂÆûAgentËØÑ‰º∞"""
    import argparse

    parser = argparse.ArgumentParser(description='Run KGAgent V7 Real Evaluation')
    parser.add_argument('--subset', nargs='+', help='Test IDs to run (default: all)')
    parser.add_argument('--output', default='evaluation_results', help='Output directory')
    parser.add_argument('--skip-viz', action='store_true', help='Skip visualization generation')
    parser.add_argument('--config-file', help='JSON config file path')

    args = parser.parse_args()

    # Âä†ËΩΩÈÖçÁΩÆ
    if args.config_file:
        with open(args.config_file, 'r') as f:
            config = json.load(f)
        logger.info(f"Loaded config from: {args.config_file}")
    else:
        config = AgentConfig.get_config()

    # ÂàõÂª∫ËøêË°åÂô®
    runner = RealEvaluationRunner(config)

    # ËÆæÁΩÆÁéØÂ¢É
    if not runner.setup():
        logger.error("‚ùå Setup failed. Exiting.")
        return 1

    # ËøêË°åËØÑ‰º∞
    results = runner.run_evaluation(
        test_subset=args.subset,
        output_dir=args.output
    )

    if results is not None and not results.empty:
        # ÁîüÊàêÂèØËßÜÂåñ
        if not args.skip_viz:
            runner.generate_visualizations(args.output)

        # ÊâìÂç∞ÊëòË¶Å
        runner.print_summary()

        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ EVALUATION COMPLETE")
        logger.info("=" * 60)
        return 0
    else:
        logger.error("‚ùå Evaluation failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())