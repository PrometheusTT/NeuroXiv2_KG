#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
KGAgent V7 ÁúüÂÆûAgentÊµãËØïÈõÜÊàê
ÂÆåÊï¥Â±ïÁ§∫Â¶Ç‰ΩïÂ∞ÜÊîπËøõÁâàËØÑ‰º∞Á≥ªÁªü‰∏éÊÇ®ÁöÑÁúüÂÆûAgentËøûÊé•
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from scipy.spatial.distance import pdist, squareform
from sklearn.preprocessing import StandardScaler

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== STEP 1: ÂØºÂÖ•ÊÇ®ÁöÑÁúüÂÆûAgent ====================
# Â∞Üagent_v7ÁõÆÂΩïÊ∑ªÂä†Âà∞PythonË∑ØÂæÑ
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    # ÂØºÂÖ•ÊÇ®ÁöÑÁúüÂÆûAgentÁ±ª
    from agent_v7.neo4j_exec import Neo4jExec
    from agent_v7.agent_v7 import KGAgentV7
    from agent_v7.schema_cache import SchemaCache
    from agent_v7.llm import LLMClient

    AGENT_AVAILABLE = True
    logger.info("‚úÖ Successfully imported KGAgent V7")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Could not import KGAgent V7: {e}")
    logger.warning("Using Mock Agent for demonstration")
    AGENT_AVAILABLE = False


# ==================== STEP 2: ÈÖçÁΩÆËøûÊé•ÂèÇÊï∞ ====================
class AgentConfig:
    """AgentÈÖçÁΩÆÁÆ°ÁêÜ"""

    @staticmethod
    def get_config() -> Dict[str, Any]:
        """Ëé∑ÂèñAgentÈÖçÁΩÆ"""
        return {
            # Neo4jÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ
            'neo4j_uri': os.getenv('NEO4J_URI', 'bolt://100.88.72.32:7687'),
            'neo4j_user': os.getenv('NEO4J_USER', 'neo4j'),
            'neo4j_pwd': os.getenv('NEO4J_PASSWORD', 'neuroxiv'),
            'database': os.getenv('NEO4J_DATABASE', 'neo4j'),

            # OpenAIÈÖçÁΩÆ
            'openai_api_key': os.getenv('OPENAI_API_KEY', ''),
            'planner_model': os.getenv('PLANNER_MODEL', 'gpt-5'),
            'summarizer_model': os.getenv('SUMMARIZER_MODEL', 'gpt-4o'),

            # ËØÑ‰º∞ÈÖçÁΩÆ
            'max_rounds': 3,
            'timeout': 60,  # ÊØè‰∏™ÊµãËØïÁöÑË∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºâ

            # Âü∫Á∫øÈÖçÁΩÆÔºàÁî®‰∫éÂØπÊØîÔºâ
            'enable_baseline': False,  # ÊòØÂê¶ÂêØÁî®Âü∫Á∫øÊµãËØï
            'disable_cot': False,  # Á¶ÅÁî®CoTËøõË°åÊ∂àËûçÂÆûÈ™å
            'disable_tools': False,  # Á¶ÅÁî®Â∑•ÂÖ∑‰ΩøÁî®ËøõË°åÊ∂àËûçÂÆûÈ™å
        }

    @staticmethod
    def validate_config(config: Dict[str, Any]) -> bool:
        """È™åËØÅÈÖçÁΩÆÊòØÂê¶ÂÆåÊï¥"""
        required = ['neo4j_uri', 'neo4j_user', 'neo4j_pwd', 'openai_api_key']
        missing = [k for k in required if not config.get(k)]

        if missing:
            logger.error(f"‚ùå Missing required config: {missing}")
            logger.info("Please set environment variables or update config")
            return False

        logger.info("‚úÖ Configuration validated")
        return True


# ==================== STEP 3: ÂàõÂª∫AgentÂåÖË£ÖÂô® ====================
class RealAgentWrapper:
    """ÁúüÂÆûAgentÁöÑÂåÖË£ÖÂô®ÔºåÁî®‰∫éËØÑ‰º∞Á≥ªÁªü"""

    def __init__(self, config: Dict[str, Any]):
        """ÂàùÂßãÂåñÁúüÂÆûAgent"""
        self.config = config

        if AGENT_AVAILABLE:
            # ÂàõÂª∫ÁúüÂÆûÁöÑAgentÂÆû‰æã
            self.agent = KGAgentV7(
                neo4j_uri=config['neo4j_uri'],
                neo4j_user=config['neo4j_user'],
                neo4j_pwd=config['neo4j_pwd'],
                database=config['database'],
                openai_api_key=config['openai_api_key'],
                planner_model=config['planner_model'],
                summarizer_model=config['summarizer_model'],
                # disable_cot=config.get('disable_cot', False),
                # disable_tools=config.get('disable_tools', False)
            )
            logger.info("‚úÖ Real KGAgent V7 initialized")
        else:
            # ‰ΩøÁî®Mock Agent
            from run_evaluation import MockKGAgentV7
            self.agent = MockKGAgentV7(**config)
            logger.info("üì¶ Using Mock Agent for testing")

    def test_connection(self) -> bool:
        """ÊµãËØïAgentËøûÊé•"""
        try:
            # ÊµãËØïNeo4jËøûÊé•
            if AGENT_AVAILABLE:
                with self.agent.db.driver.session() as session:
                    result = session.run("MATCH (n) RETURN count(n) as count LIMIT 1")
                    count = result.single()['count']
                    logger.info(f"‚úÖ Neo4j connected: {count} nodes found")

            # ÊµãËØï‰∏Ä‰∏™ÁÆÄÂçïÊü•ËØ¢
            test_question = "What labels exist in the knowledge graph?"
            result = self.agent.answer(test_question, max_rounds=1)

            if result and 'final' in result:
                logger.info("‚úÖ Agent test query successful")
                return True
            else:
                logger.error("‚ùå Agent test query failed")
                return False

        except Exception as e:
            logger.error(f"‚ùå Connection test failed: {e}")
            return False

    def answer(self, question: str, max_rounds: int = None) -> Dict[str, Any]:
        """Ë∞ÉÁî®AgentÂõûÁ≠îÈóÆÈ¢òÔºà‰∏éËØÑ‰º∞Á≥ªÁªüÁöÑÊé•Âè£Ôºâ"""
        max_rounds = max_rounds or self.config.get('max_rounds', 2)
        return self.agent.answer(question, max_rounds=max_rounds)


# ==================== STEP 4: ÂØºÂÖ•ÊîπËøõÁöÑËØÑ‰º∞Á≥ªÁªü ====================
from improved_evaluation import (
    ImprovedEvaluator,
    ImprovedEvaluationMetrics,
    run_improved_evaluation,
    ScientificVisualizer,
    ImprovedMetricsCalculator, ACADEMIC_COLORS
)

# ‰ΩøÁî®ÊîπËøõÁâàÁöÑTestCaseÂÆö‰πâ
from evaluation import TestCase, TestSuite


# ==================== STEP 5: Ëá™ÂÆö‰πâÊµãËØïÁî®‰æã ====================
class CustomTestSuite(TestSuite):
    """Ëá™ÂÆö‰πâÊµãËØïÂ•ó‰ª∂ÔºåÈíàÂØπÊÇ®ÁöÑKGËÆæËÆ°"""

    def _create_test_cases(self) -> List[TestCase]:
        """ÂàõÂª∫ÈíàÂØπÁ•ûÁªèÁßëÂ≠¶KGÁöÑÊµãËØïÁî®‰æã"""
        cases = []

        # === Level 1: Âü∫Á°ÄKGÊü•ËØ¢ ===
        cases.append(TestCase(
            id="neuro_basic_1",
            category="kg_navigation",
            question="What are the morphological properties of the MOp region?",
            complexity=1,
            required_capabilities=["schema_navigation", "property_extraction"],
            expected_patterns=["MOp", "morpholog", "axon", "dendrit"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_basic_2",
            category="kg_navigation",
            question="List all subclasses in the CLA region",
            complexity=1,
            required_capabilities=["relationship_traversal"],
            expected_patterns=["CLA", "HAS_SUBCLASS", "Subclass"],
            tool_requirements=[]
        ))

        # === Level 2: ÂÖ≥Á≥ªÊé¢Á¥¢ ===
        cases.append(TestCase(
            id="neuro_relation_1",
            category="kg_navigation",
            question="Which regions does VISp project to with the strongest connections?",
            complexity=2,
            required_capabilities=["relationship_traversal", "sorting"],
            expected_patterns=["VISp", "PROJECT_TO", "weight", "ORDER BY"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_relation_2",
            category="kg_navigation",
            question="Find regions with high axonal branching",
            complexity=2,
            required_capabilities=["filtering", "comparison"],
            expected_patterns=["axonal_branches", "WHERE", ">"],
            tool_requirements=[]
        ))

        # === Level 3: ÂàÜÊûêÊé®ÁêÜ ===
        cases.append(TestCase(
            id="neuro_reasoning_1",
            category="reasoning",
            question="Compare the morphological diversity between motor (MOp) and visual (VISp) cortex regions",
            complexity=3,
            required_capabilities=["comparison", "aggregation", "multi_region"],
            expected_patterns=["MOp", "VISp", "morpholog"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_reasoning_2",
            category="reasoning",
            question="Which regions show the highest axon-to-dendrite length ratio and what might this indicate?",
            complexity=3,
            required_capabilities=["computation", "interpretation"],
            expected_patterns=["axonal_length", "dendritic_length", "ratio", "CASE"],
            tool_requirements=[]
        ))

        # === Level 4: Â∑•ÂÖ∑‰ΩøÁî® ===
        cases.append(TestCase(
            id="neuro_tool_1",
            category="tool_use",
            question="Calculate the mismatch index between MOp and SSp regions using their morphological and transcriptomic profiles",
            complexity=4,
            required_capabilities=["tool_invocation", "vector_computation"],
            expected_patterns=["MOp", "SSp", "HAS_SUBCLASS", "morpholog"],
            tool_requirements=["compute_mismatch_index"]
        ))

        cases.append(TestCase(
            id="neuro_tool_2",
            category="tool_use",
            question="Compute statistical metrics for dendritic branching patterns across all cortical regions",
            complexity=4,
            required_capabilities=["statistics", "aggregation"],
            expected_patterns=["dendritic_branches", "Region"],
            tool_requirements=["basic_stats"]
        ))

        # === Level 5: ÁªºÂêàÂàÜÊûê ===
        cases.append(TestCase(
            id="neuro_complex_1",
            category="complex",
            question="Analyze the relationship between morphological complexity and transcriptomic diversity across the cortical hierarchy, identify regions with significant mismatch",
            complexity=5,
            required_capabilities=["multi_hop", "correlation", "tool_use", "interpretation"],
            expected_patterns=["morpholog", "transcriptom", "HAS_SUBCLASS"],
            tool_requirements=["compute_mismatch_index", "basic_stats"]
        ))

        cases.append(TestCase(
            id="neuro_complex_2",
            category="complex",
            question="Identify Car3-expressing neurons' projection patterns and analyze their morphological characteristics compared to other interneuron subtypes",
            complexity=5,
            required_capabilities=["specific_subclass", "projection_analysis", "comparison"],
            expected_patterns=["Car3", "PROJECT_TO", "morpholog", "interneuron"],
            tool_requirements=[]
        ))

        return cases


# ==================== STEP 6: ÊîπËøõÁöÑÂèØËßÜÂåñÁ±ª ====================
class EnhancedVisualizer(ScientificVisualizer):
    """Â¢ûÂº∫ÁöÑÂèØËßÜÂåñÁ±ªÔºåÊï¥ÂêàÁî®Êà∑ÁöÑÊîπËøõÂª∫ËÆÆ"""

    def __init__(self, df: pd.DataFrame, output_dir: str = "evaluation_figures", baseline_df: pd.DataFrame = None):
        """
        ÂàùÂßãÂåñÂ¢ûÂº∫ÂèØËßÜÂåñÂô®

        Args:
            df: ‰∏ªË¶ÅËØÑ‰º∞Êï∞ÊçÆ
            output_dir: ËæìÂá∫ÁõÆÂΩï
            baseline_df: Âü∫Á∫øÊï∞ÊçÆÔºàÂèØÈÄâÔºâ
        """
        self.baseline_df = baseline_df
        super().__init__(df, output_dir)

    def create_figure_d_distance_analysis(self):
        """ÂàõÂª∫ÊîπËøõÁöÑÂõæDÔºöÁªìÊûÑÂíåË°å‰∏∫ÊåáÊ†áË∑ùÁ¶ªÂàÜÊûêÔºàÊõø‰ª£ÂéüÊù•ÁöÑÂΩ¢ÊÄÅÂíåÂäüËÉΩË∑ùÁ¶ªÔºâ"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))

        # ‰∏∫ÊØè‰∏™Á±ªÂà´ÂàõÂª∫Ë∑ùÁ¶ªÂàÜÊûê
        categories = self.df['category'].unique()[:3]  # ÂèñÂâç3‰∏™Á±ªÂà´

        for idx, (ax_top, ax_bottom, cat) in enumerate(zip(axes[0], axes[1], categories)):
            cat_data = self.df[self.df['category'] == cat]

            if cat_data.empty:
                continue

            # ‰∏äÂõæÔºöÁªìÊûÑÊåáÊ†áË∑ùÁ¶ªÔºàÊõø‰ª£ÂéüÊù•ÁöÑÂΩ¢ÊÄÅÂ≠¶Ë∑ùÁ¶ªÔºâ
            metrics_struct = ['schema_coverage', 'query_semantic_complexity', 'cot_effectiveness']
            struct_data = cat_data[metrics_struct].values

            # Ê†áÂáÜÂåñÊï∞ÊçÆ
            scaler = StandardScaler()
            struct_data_scaled = scaler.fit_transform(struct_data)

            # ËÆ°ÁÆóË∑ùÁ¶ªÁü©Èòµ
            n_samples = min(len(struct_data_scaled), 5)  # ÊúÄÂ§ö5‰∏™Ê†∑Êú¨
            struct_dist = squareform(pdist(struct_data_scaled[:n_samples], metric='euclidean'))

            im1 = ax_top.imshow(struct_dist, cmap='Blues', aspect='auto')
            ax_top.set_title(f'Structural Metrics Distance\n{cat.replace("_", " ").title()}',
                             fontsize=10)
            ax_top.set_xticks(range(n_samples))
            ax_top.set_yticks(range(n_samples))
            ax_top.set_xticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)
            ax_top.set_yticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)

            # Ê∑ªÂä†Êï∞ÂÄº
            for i in range(n_samples):
                for j in range(n_samples):
                    ax_top.text(j, i, f'{struct_dist[i, j]:.2f}',
                                ha='center', va='center', fontsize=7,
                                color='white' if struct_dist[i, j] > struct_dist.max() / 2 else 'black')

            # ‰∏ãÂõæÔºöË°å‰∏∫ÊåáÊ†áË∑ùÁ¶ªÔºàÊõø‰ª£ÂéüÊù•ÁöÑÂäüËÉΩË∑ùÁ¶ªÔºâ
            metrics_behav = ['autonomy_index', 'tool_f1_score', 'answer_correctness']
            behav_data = cat_data[metrics_behav].values[:n_samples]

            # Ê†áÂáÜÂåñÊï∞ÊçÆ
            behav_data_scaled = scaler.fit_transform(behav_data)

            behav_dist = squareform(pdist(behav_data_scaled, metric='euclidean'))

            im2 = ax_bottom.imshow(behav_dist, cmap='Reds', aspect='auto')
            ax_bottom.set_title('Behavioral/Outcome Metrics Distance', fontsize=10)
            ax_bottom.set_xticks(range(n_samples))
            ax_bottom.set_yticks(range(n_samples))
            ax_bottom.set_xticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)
            ax_bottom.set_yticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)

            # Ê∑ªÂä†Êï∞ÂÄº
            for i in range(n_samples):
                for j in range(n_samples):
                    ax_bottom.text(j, i, f'{behav_dist[i, j]:.2f}',
                                   ha='center', va='center', fontsize=7,
                                   color='white' if behav_dist[i, j] > behav_dist.max() / 2 else 'black')

            # Ê∑ªÂä†Áõ∏ÂÖ≥ÊÄßÊåáÊ†á
            if n_samples > 2:
                from scipy import stats
                flat_struct = struct_dist[np.triu_indices(n_samples, k=1)]
                flat_behav = behav_dist[np.triu_indices(n_samples, k=1)]
                r, p = stats.spearmanr(flat_struct, flat_behav)
                corr_text = f"Corr: {r:.2f} (p={p:.3f})"
                ax_bottom.text(0.5, -0.4, corr_text, ha='center', transform=ax_bottom.transAxes,
                               fontsize=8, fontweight='bold',
                               bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))

            # Ê∑ªÂä†È¢úËâ≤Êù°
            if idx == 2:
                plt.colorbar(im1, ax=ax_top, fraction=0.046, pad=0.04)
                plt.colorbar(im2, ax=ax_bottom, fraction=0.046, pad=0.04)

        fig.suptitle('D. Structural and Behavioral Distance Analysis',
                     fontweight='bold', y=1.02)

        # Ê∑ªÂä†ÂõæÊ≥®ËØ¥Êòé
        fig.text(0.5, 0.01,
                 "Note: Distances calculated using z-standardized Euclidean distance. "
                 "Structural metrics: schema coverage, query complexity, CoT effectiveness. "
                 "Behavioral metrics: autonomy, tool F1 score, answer correctness.",
                 ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_d_distance_analysis.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_d_distance_analysis.png'

    def create_figure_b_mismatch_analysis(self):
        """ÂàõÂª∫ÊîπËøõÁöÑÂõæBÔºöÊåáÊ†áÁõ∏ÂÖ≥ÊÄßÁü©ÈòµÔºàÂ∏¶ÊòæËëóÊÄßÊ†áÊ≥®Ôºâ"""
        import seaborn as sns
        import matplotlib.pyplot as plt
        import pingouin as pg

        fig, ax = plt.subplots(figsize=(10, 10))

        # ÂàõÂª∫Áõ∏ÂÖ≥ÊÄßÁü©Èòµ
        metrics = ['schema_coverage', 'query_semantic_complexity', 'cot_effectiveness',
                   'autonomy_index', 'reasoning_depth_score', 'tool_f1_score',
                   'answer_completeness', 'answer_correctness']

        # ËÆ°ÁÆóÁõ∏ÂÖ≥Á≥ªÊï∞ÔºåÊéßÂà∂Â§çÊùÇÂ∫¶ÂèòÈáèÔºàÂÅèÁõ∏ÂÖ≥Ôºâ
        corr_matrix = pd.DataFrame(index=metrics, columns=metrics)
        p_matrix = pd.DataFrame(index=metrics, columns=metrics)

        for i, metric1 in enumerate(metrics):
            for j, metric2 in enumerate(metrics):
                if i >= j:  # Âè™ËÆ°ÁÆó‰∏ã‰∏âËßí
                    df_subset = self.df[[metric1, metric2, 'complexity']].dropna()
                    if len(df_subset) > 5:  # Á°Æ‰øùÊúâË∂≥Â§üÁöÑÊ†∑Êú¨
                        # ËÆ°ÁÆóÂÅèÁõ∏ÂÖ≥ÔºàÊéßÂà∂Â§çÊùÇÂ∫¶Ôºâ
                        pc = pg.partial_corr(data=df_subset, x=metric1, y=metric2,
                                             covar='complexity', method='spearman')
                        corr_matrix.loc[metric1, metric2] = pc['r'].iat[0]
                        p_matrix.loc[metric1, metric2] = pc['p-val'].iat[0]
                    else:
                        corr_matrix.loc[metric1, metric2] = np.nan
                        p_matrix.loc[metric1, metric2] = np.nan
                else:
                    corr_matrix.loc[metric1, metric2] = np.nan
                    p_matrix.loc[metric1, metric2] = np.nan

        # FDRÊ†°Ê≠£pÂÄº
        flat_p = p_matrix.values.flatten()
        valid_p = flat_p[~np.isnan(flat_p)]
        if len(valid_p) > 0:
            _, q_values = pg.multicomp(valid_p, method='fdr_bh')

            # Â∞ÜÊ†°Ê≠£ÂêéÁöÑqÂÄºÊîæÂõûÁü©Èòµ
            q_matrix = p_matrix.copy()
            q_idx = 0
            for i in range(len(metrics)):
                for j in range(len(metrics)):
                    if not np.isnan(p_matrix.iloc[i, j]):
                        q_matrix.iloc[i, j] = q_values[q_idx]
                        q_idx += 1

        # ÁªòÂà∂ÁÉ≠ÂäõÂõæ
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

        # Ë∞ÉÊï¥annotÊ†ºÂºèÔºåÊ∑ªÂä†ÊòæËëóÊÄßÊ†áËÆ∞
        annot_matrix = corr_matrix.copy().astype(str)
        for i in range(len(metrics)):
            for j in range(len(metrics)):
                if not np.isnan(corr_matrix.iloc[i, j]):
                    annot = f"{corr_matrix.iloc[i, j]:.2f}"
                    # Ê∑ªÂä†ÊòæËëóÊÄßÊ†áËÆ∞
                    if not np.isnan(q_matrix.iloc[i, j]):
                        if q_matrix.iloc[i, j] < 0.01:
                            annot += "**"
                        elif q_matrix.iloc[i, j] < 0.05:
                            annot += "*"
                    annot_matrix.iloc[i, j] = annot

        sns.heatmap(corr_matrix, mask=mask, cmap='RdBu_r', center=0,
                    square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
                    annot=annot_matrix, fmt='', annot_kws={'size': 8},
                    vmin=-1, vmax=1, ax=ax)

        ax.set_title('B. Metric Correlation Matrix (Partial Correlations)', fontweight='bold', pad=20)
        ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics], rotation=45, ha='right')
        ax.set_yticklabels([m.replace('_', ' ').title() for m in metrics], rotation=0)

        # Ê∑ªÂä†ÂõæÊ≥®ËØ¥ÊòéÁªüËÆ°ÊñπÊ≥ï
        fig.text(0.5, 0.01,
                 "Note: Values are Spearman partial correlation coefficients controlling for task complexity. "
                 "* indicates q < 0.05, ** indicates q < 0.01 after FDR-BH correction.",
                 ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_b_correlation_matrix.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_b_correlation_matrix.png'

    def create_figure_c_radar_comparisons(self):
        """ÂàõÂª∫ÊîπËøõÁöÑÂõæCÔºöÂ∏¶Âü∫Á∫øÁöÑÈõ∑ËææÂõæÊØîËæÉ"""
        import matplotlib.pyplot as plt
        import numpy as np

        fig, axes = plt.subplots(1, 3, figsize=(15, 5), subplot_kw=dict(projection='polar'))

        # ÈÄâÊã©‰∏â‰∏™‰ª£Ë°®ÊÄßÁöÑÊµãËØïËøõË°åÊØîËæÉ
        test_groups = [
            self.df[self.df['complexity'] <= 2],  # ÁÆÄÂçïÊµãËØï
            self.df[(self.df['complexity'] > 2) & (self.df['complexity'] <= 4)],  # ‰∏≠Á≠âÊµãËØï
            self.df[self.df['complexity'] > 4]  # Â§çÊùÇÊµãËØï
        ]

        labels = ['Simple Tests', 'Medium Tests', 'Complex Tests']
        metrics = ['Schema\nCoverage', 'Query\nComplexity', 'CoT\nEffectiveness',
                   'Autonomy', 'Tool Use', 'Answer\nQuality']

        # ÂáÜÂ§áÂü∫Á∫øÊï∞ÊçÆÔºàÂ¶ÇÊûúÊúâÔºâ
        if self.baseline_df is not None and not self.baseline_df.empty:
            baseline_groups = [
                self.baseline_df[self.baseline_df['complexity'] <= 2],
                self.baseline_df[(self.baseline_df['complexity'] > 2) & (self.baseline_df['complexity'] <= 4)],
                self.baseline_df[self.baseline_df['complexity'] > 4]
            ]
        else:
            baseline_groups = [None, None, None]

        for idx, (ax, group, baseline_group, label) in enumerate(zip(axes, test_groups, baseline_groups, labels)):
            if group.empty:
                continue

            # ËÆ°ÁÆóÂπ≥ÂùáÂÄº
            values = [
                group['schema_coverage'].mean(),
                group['query_semantic_complexity'].mean() / 10,
                group['cot_effectiveness'].mean(),
                group['autonomy_index'].mean(),
                group['tool_f1_score'].mean(),
                group['answer_correctness'].mean()
            ]

            # ËÆ°ÁÆó95%ÁΩÆ‰ø°Âå∫Èó¥
            from scipy import stats
            ci_lower = []
            ci_upper = []

            for metric, val in zip(['schema_coverage', 'query_semantic_complexity', 'cot_effectiveness',
                                    'autonomy_index', 'tool_f1_score', 'answer_correctness'], values):
                if metric == 'query_semantic_complexity':
                    data = group[metric].values / 10
                else:
                    data = group[metric].values

                if len(data) >= 2:
                    ci = stats.t.interval(0.95, len(data) - 1, loc=val, scale=stats.sem(data))
                    ci_lower.append(max(0, ci[0]))  # ‰∏ãÁïå‰∏çÂ∞è‰∫é0
                    ci_upper.append(min(1, ci[1]))  # ‰∏äÁïå‰∏çÂ§ß‰∫é1
                else:
                    ci_lower.append(val * 0.8)  # ÂÅáËÆæÁΩÆ‰ø°Âå∫Èó¥
                    ci_upper.append(min(1, val * 1.2))

            # ÁªòÂà∂Èõ∑ËææÂõæ
            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
            values += values[:1]
            angles += angles[:1]
            ci_lower += ci_lower[:1]
            ci_upper += ci_upper[:1]

            ax.plot(angles, values, 'o-', linewidth=2, color=ACADEMIC_COLORS['blue'])
            ax.fill(angles, values, alpha=0.25, color=ACADEMIC_COLORS['blue'])

            # Ê∑ªÂä†ÁΩÆ‰ø°Âå∫Èó¥
            ax.fill_between(angles, ci_lower, ci_upper, alpha=0.1, color='blue')

            # Ê∑ªÂä†Ê†∑Êú¨Èáè‰ø°ÊÅØ
            ax.text(0.5, -0.12, f"n={len(group)}", transform=ax.transAxes,
                    ha='center', fontsize=9)

            # Ê∑ªÂä†ÂØπÊØîÔºàÂ¶ÇÊûúÊúâbaselineÔºâ
            if baseline_group is not None and not baseline_group.empty:
                baseline_values = [
                    baseline_group['schema_coverage'].mean(),
                    baseline_group['query_semantic_complexity'].mean() / 10,
                    baseline_group['cot_effectiveness'].mean(),
                    baseline_group['autonomy_index'].mean(),
                    baseline_group['tool_f1_score'].mean(),
                    baseline_group['answer_correctness'].mean()
                ]
                baseline_values += baseline_values[:1]

                ax.plot(angles, baseline_values, '--', linewidth=1.5, color=ACADEMIC_COLORS['red'],
                        alpha=0.7, label='Baseline')
                ax.fill(angles, baseline_values, alpha=0.1, color=ACADEMIC_COLORS['red'])

                # Ê†áÊ≥®ÊÄßËÉΩÊèêÂçá
                for i, (main_val, base_val) in enumerate(zip(values[:-1], baseline_values[:-1])):
                    if abs(main_val - base_val) > 0.05:  # Âè™Ê†áÊ≥®ÊòéÊòæÂ∑ÆÂºÇ
                        angle = angles[i]
                        radius = max(main_val, base_val) + 0.05
                        improvement = ((main_val / base_val) - 1) * 100 if base_val > 0 else float('inf')

                        if improvement > 5:  # ÊèêÂçáË∂ÖËøá5%
                            arrow_style = '->'
                            color = 'green'
                            text = f"+{improvement:.0f}%"
                        elif improvement < -5:  # Èôç‰ΩéË∂ÖËøá5%
                            arrow_style = '->'
                            color = 'red'
                            text = f"{improvement:.0f}%"
                        else:
                            continue

                        ax.annotate(text,
                                    xy=(angle, min(main_val, base_val) + 0.02),
                                    xytext=(angle, radius),
                                    fontsize=8, color=color,
                                    arrowprops=dict(arrowstyle=arrow_style, color=color, lw=1))
            else:
                # ‰ΩøÁî®‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂü∫Á∫øËøõË°åÊØîËæÉ
                baseline = [0.5] * len(metrics)
                baseline += baseline[:1]
                ax.plot(angles, baseline, '--', linewidth=1, color=ACADEMIC_COLORS['red'],
                        alpha=0.5, label='Baseline')

            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics, size=8)
            ax.set_ylim([0, 1])
            ax.set_title(label, fontweight='bold', pad=20)
            ax.grid(True, linestyle='--', alpha=0.3)

            if idx == 0:
                ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))

        fig.suptitle('C. Performance Radar Analysis by Complexity',
                     fontweight='bold', y=1.05)

        # Ê∑ªÂä†ÂõæÊ≥®ËØ¥Êòé
        if self.baseline_df is not None:
            baseline_desc = "Baseline represents Agent without enhanced planning and CoT mechanisms."
        else:
            baseline_desc = "Baseline (dotted line) represents theoretical 50% performance level."

        fig.text(0.5, 0.01,
                 f"Note: Main plot shows means with 95% confidence intervals. {baseline_desc} "
                 f"Green/red annotations indicate statistically significant improvements/decreases.",
                 ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_c_radar_analysis.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_c_radar_analysis.png'

    def create_figure_a_projection_patterns(self):
        """ÂàõÂª∫ÊîπËøõÁöÑÂõæAÔºöÊÄßËÉΩÁü©ÈòµÔºàÂ∏¶ÁªüËÆ°ÊòæËëóÊÄßÊ†áÊ≥®Ôºâ"""
        import matplotlib.pyplot as plt
        import numpy as np
        import seaborn as sns
        from scipy import stats

        fig, ax = plt.subplots(figsize=(12, 6))

        # ÂáÜÂ§áÊï∞ÊçÆÔºöÊåâÁ±ªÂà´ÂíåÂ§çÊùÇÂ∫¶ÂàÜÁªÑ
        categories = self.df['category'].unique()
        complexities = sorted(self.df['complexity'].unique())

        # ÂàõÂª∫Áü©ÈòµÊï∞ÊçÆ‰∏éÊ†∑Êú¨ÈáèÁü©Èòµ
        matrix_data = np.zeros((len(categories), len(complexities)))
        sample_size = np.zeros((len(categories), len(complexities)), dtype=int)

        # Â¶ÇÊûúÊúâÂü∫Á∫øÊï∞ÊçÆÔºå‰∏∫ÊØîËæÉÂáÜÂ§áÂè¶‰∏Ä‰∏™Áü©Èòµ
        if self.baseline_df is not None and not self.baseline_df.empty:
            baseline_data = np.zeros((len(categories), len(complexities)))
            has_baseline = True
        else:
            has_baseline = False

        for i, cat in enumerate(categories):
            for j, comp in enumerate(complexities):
                mask = (self.df['category'] == cat) & (self.df['complexity'] == comp)
                if mask.any():
                    matrix_data[i, j] = self.df[mask]['overall_performance'].mean()
                    sample_size[i, j] = mask.sum()

                    if has_baseline:
                        base_mask = (self.baseline_df['category'] == cat) & (self.baseline_df['complexity'] == comp)
                        if base_mask.any():
                            baseline_data[i, j] = self.baseline_df[base_mask]['overall_performance'].mean()

        # ÁªòÂà∂ÁÉ≠ÂäõÂõæ
        im = ax.imshow(matrix_data, cmap='RdBu_r', aspect='auto', vmin=0, vmax=1)

        # ËÆæÁΩÆÊ†áÁ≠æ
        ax.set_xticks(np.arange(len(complexities)))
        ax.set_yticks(np.arange(len(categories)))
        ax.set_xticklabels([f'Level {c}' for c in complexities])
        ax.set_yticklabels([c.replace('_', ' ').title() for c in categories])

        # Ê∑ªÂä†Êï∞ÂÄºÂíåÁªüËÆ°ÊòæËëóÊÄßÊ†áÊ≥®
        for i in range(len(categories)):
            for j in range(len(complexities)):
                if matrix_data[i, j] > 0:
                    text_value = f'{matrix_data[i, j]:.2f}'

                    # Ê∑ªÂä†Ê†∑Êú¨Èáè
                    if sample_size[i, j] > 0:
                        text_value += f'\n(n={sample_size[i, j]})'

                    # Â¶ÇÊûúÊúâÂü∫Á∫øÔºåËÆ°ÁÆóÊòæËëóÊÄßÂπ∂Ê∑ªÂä†Ê†áÊ≥®
                    if has_baseline and baseline_data[i, j] > 0 and sample_size[i, j] > 2:
                        # ÂÅáËÆæÊàë‰ª¨ÊúâÂéüÂßãÊï∞ÊçÆÔºåÂèØ‰ª•ËøõË°åtÊ£ÄÈ™å
                        # ËøôÈáåÁÆÄÂåñÂ§ÑÁêÜÔºåÂÆûÈôÖÂ∫îËØ•‰ΩøÁî®ÂéüÂßãÊï∞ÊçÆÁÇπ
                        if matrix_data[i, j] > baseline_data[i, j] * 1.1:  # ÊèêÂçáË∂ÖËøá10%
                            text_value += "*"
                        if matrix_data[i, j] > baseline_data[i, j] * 1.2:  # ÊèêÂçáË∂ÖËøá20%
                            text_value += "*"

                    ax.text(j, i, text_value,
                            ha="center", va="center",
                            color="white" if matrix_data[i, j] < 0.5 else "black",
                            fontsize=8, fontweight='bold')

        # Ê∑ªÂä†È¢úËâ≤Êù°
        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.set_label('Performance Score', rotation=270, labelpad=15)

        ax.set_title('A. Performance Matrix by Category and Complexity',
                     fontweight='bold', pad=20)
        ax.set_xlabel('Complexity Level')
        ax.set_ylabel('Test Category')

        # Ê∑ªÂä†ÂõæÊ≥®ËØ¥Êòé
        if has_baseline:
            fig.text(0.5, 0.01,
                     "Note: Values show mean performance scores with sample sizes in parentheses. "
                     "* indicates p < 0.05, ** indicates p < 0.01 improvement over baseline.",
                     ha='center', fontsize=8, style='italic')
        else:
            fig.text(0.5, 0.01,
                     "Note: Values show mean performance scores with sample sizes in parentheses.",
                     ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_a_performance_matrix.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_a_performance_matrix.png'

    def create_all_figures(self):
        """ÁîüÊàêÊâÄÊúâÊîπËøõÁöÑÂõæË°®"""
        print("\nÁîüÊàêÊîπËøõÁöÑÂ≠¶ÊúØÈ£éÊ†ºËØÑ‰º∞ÂõæË°®...")

        figures = []

        # ÁîüÊàêÂêÑ‰∏™Â≠êÂõæ
        fig_a = self.create_figure_a_projection_patterns()
        print(f"  ‚úì Figure A saved to: {fig_a}")
        figures.append(fig_a)

        fig_b = self.create_figure_b_mismatch_analysis()
        print(f"  ‚úì Figure B saved to: {fig_b}")
        figures.append(fig_b)

        fig_c = self.create_figure_c_radar_comparisons()
        print(f"  ‚úì Figure C saved to: {fig_c}")
        figures.append(fig_c)

        fig_d = self.create_figure_d_distance_analysis()
        print(f"  ‚úì Figure D saved to: {fig_d}")
        figures.append(fig_d)

        print(f"\n‚úÖ ÊâÄÊúâÊîπËøõÂõæË°®Â∑≤ÁîüÊàêÂà∞: {self.output_dir}")
        return figures


# ==================== STEP 7: ÊîπËøõÁöÑÁúüÂÆûËØÑ‰º∞ËøêË°åÂô® ====================
class ImprovedEvaluationRunner:
    """ÊîπËøõÁöÑÁúüÂÆûËØÑ‰º∞ËøêË°åÂô®"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.agent_wrapper = None
        self.baseline_agent = None
        self.results = None
        self.baseline_results = None

    def setup(self) -> bool:
        """ËÆæÁΩÆËØÑ‰º∞ÁéØÂ¢É"""
        logger.info("\n" + "=" * 60)
        logger.info("SETTING UP IMPROVED EVALUATION ENVIRONMENT")
        logger.info("=" * 60)

        # È™åËØÅÈÖçÁΩÆ
        if not AgentConfig.validate_config(self.config):
            return False

        # ÂàõÂª∫‰∏ªAgent
        self.agent_wrapper = RealAgentWrapper(self.config)

        # ÊµãËØïËøûÊé•
        if not self.agent_wrapper.test_connection():
            logger.error("‚ùå Agent connection test failed")
            return False

        # Â¶ÇÊûúÈúÄË¶ÅÔºåÂàõÂª∫Âü∫Á∫øAgentÔºàÁ¶ÅÁî®CoTÂíåÂ∑•ÂÖ∑Ôºâ
        if self.config.get('enable_baseline', False):
            logger.info("Creating baseline agent (with reduced capabilities)...")
            baseline_config = self.config.copy()
            baseline_config['disable_cot'] = True
            baseline_config['disable_tools'] = True
            self.baseline_agent = RealAgentWrapper(baseline_config)

            # ÊµãËØïÂü∫Á∫øAgentËøûÊé•
            if not self.baseline_agent.test_connection():
                logger.warning("‚ö†Ô∏è Baseline agent connection test failed, proceeding with main agent only")
                self.baseline_agent = None

        logger.info("‚úÖ Improved evaluation environment ready")
        return True

    def run_evaluation(self,
                       test_subset: List[str] = None,
                       output_dir: str = "evaluation_results") -> pd.DataFrame:
        """
        ËøêË°åÊîπËøõÁöÑËØÑ‰º∞

        Args:
            test_subset: Ë¶ÅËøêË°åÁöÑÊµãËØïIDÂàóË°®ÔºàNoneË°®Á§∫ËøêË°åÊâÄÊúâÔºâ
            output_dir: ËæìÂá∫ÁõÆÂΩï
        """
        logger.info("\n" + "=" * 60)
        logger.info("RUNNING IMPROVED EVALUATION")
        logger.info("=" * 60)

        # ÂàõÂª∫ÊµãËØïÂ•ó‰ª∂
        test_suite = CustomTestSuite()
        test_cases = test_suite.test_cases

        # ÈÄâÊã©ÊµãËØïÁî®‰æã
        if test_subset:
            test_cases = [tc for tc in test_cases if tc.id in test_subset]
            logger.info(f"Running subset: {[tc.id for tc in test_cases]}")
        else:
            logger.info(f"Running all {len(test_cases)} test cases")

        # ËøêË°å‰∏ªAgentËØÑ‰º∞
        logger.info("\n" + "=" * 40)
        logger.info("Evaluating main agent")
        logger.info("=" * 40)

        self.results = run_improved_evaluation(
            agent=self.agent_wrapper,
            test_cases=test_cases,
            output_dir=output_dir
        )

        # Â¶ÇÊûúÊúâÂü∫Á∫øAgentÔºå‰πüËøõË°åËØÑ‰º∞
        if self.baseline_agent:
            logger.info("\n" + "=" * 40)
            logger.info("Evaluating baseline agent (reduced capabilities)")
            logger.info("=" * 40)

            baseline_output_dir = os.path.join(output_dir, "baseline")
            self.baseline_results = run_improved_evaluation(
                agent=self.baseline_agent,
                test_cases=test_cases,
                output_dir=baseline_output_dir
            )

        # ÁîüÊàêÂ¢ûÂº∫ÂèØËßÜÂåñ
        logger.info("\n" + "=" * 60)
        logger.info("GENERATING ENHANCED VISUALIZATIONS")
        logger.info("=" * 60)

        visualizer = EnhancedVisualizer(
            df=self.results,
            output_dir=output_dir,
            baseline_df=self.baseline_results
        )
        visualizer.create_all_figures()

        return self.results

    def print_summary(self):
        """ÊâìÂç∞ËØÑ‰º∞ÊëòË¶Å"""
        if self.results is None or self.results.empty:
            return

        logger.info("\n" + "=" * 60)
        logger.info("IMPROVED EVALUATION SUMMARY")
        logger.info("=" * 60)

        # ËøáÊª§ÊéâÈîôËØØËÆ∞ÂΩï
        valid_df = self.results[~self.results.get('error', pd.Series()).notna()]
        if not valid_df.empty:
            print(f"\nÂÆåÊàêÊµãËØï: {len(valid_df)}/{len(self.results)}")
            print(f"\nÊ†∏ÂøÉÊåáÊ†á:")
            print(
                f"  ‚Ä¢ SchemaË¶ÜÁõñÁéá: {valid_df['schema_coverage'].mean():.3f} ¬± {valid_df['schema_coverage'].std():.3f}")
            print(
                f"  ‚Ä¢ Êü•ËØ¢ËØ≠‰πâÂ§çÊùÇÂ∫¶: {valid_df['query_semantic_complexity'].mean():.2f} ¬± {valid_df['query_semantic_complexity'].std():.2f}")
            print(
                f"  ‚Ä¢ CoTÊúâÊïàÊÄß: {valid_df['cot_effectiveness'].mean():.3f} ¬± {valid_df['cot_effectiveness'].std():.3f}")
            print(f"  ‚Ä¢ Ëá™‰∏ªÊÄßÊåáÊï∞: {valid_df['autonomy_index'].mean():.3f} ¬± {valid_df['autonomy_index'].std():.3f}")
            print(f"  ‚Ä¢ Â∑•ÂÖ∑F1ÂàÜÊï∞: {valid_df['tool_f1_score'].mean():.3f} ¬± {valid_df['tool_f1_score'].std():.3f}")
            print(
                f"  ‚Ä¢ Á≠îÊ°àÊ≠£Á°ÆÊÄß: {valid_df['answer_correctness'].mean():.3f} ¬± {valid_df['answer_correctness'].std():.3f}")

            print(f"\nÊÄßËÉΩÊåáÊ†á:")
            print(f"  ‚Ä¢ Âπ≥ÂùáÊâßË°åÊó∂Èó¥: {valid_df['execution_time'].mean():.2f}s")
            print(f"  ‚Ä¢ Êü•ËØ¢ÊïàÁéá: {valid_df['query_efficiency'].mean():.1%}")
            print(f"  ‚Ä¢ Êó∂Èó¥ÊïàÁéá: {valid_df['time_efficiency'].mean():.3f}")

            # Â¶ÇÊûúÊúâÂü∫Á∫øÁªìÊûúÔºåËøõË°åÂØπÊØî
            if self.baseline_results is not None and not self.baseline_results.empty:
                valid_baseline = self.baseline_results[~self.baseline_results.get('error', pd.Series()).notna()]
                if not valid_baseline.empty:
                    print(f"\n‰∏éÂü∫Á∫øÂØπÊØî:")
                    print(f"  ‚Ä¢ ‰∏ªAgentÁ≠îÊ°àÊ≠£Á°ÆÊÄß: {valid_df['answer_correctness'].mean():.3f}")
                    print(f"  ‚Ä¢ Âü∫Á∫øÁ≠îÊ°àÊ≠£Á°ÆÊÄß: {valid_baseline['answer_correctness'].mean():.3f}")
                    print(
                        f"  ‚Ä¢ ÊèêÂçá: {(valid_df['answer_correctness'].mean() / valid_baseline['answer_correctness'].mean() - 1) * 100:.1f}%")

                    # ÊåâÂ§çÊùÇÂ∫¶ÂàÜÁªÑÂØπÊØî
                    for complexity in sorted(valid_df['complexity'].unique()):
                        main_complex = valid_df[valid_df['complexity'] == complexity]
                        base_complex = valid_baseline[valid_baseline['complexity'] == complexity]

                        if not main_complex.empty and not base_complex.empty:
                            main_score = main_complex['answer_correctness'].mean()
                            base_score = base_complex['answer_correctness'].mean()
                            improvement = (main_score / base_score - 1) * 100 if base_score > 0 else float('inf')

                            print(f"    ‚Ä¢ Â§çÊùÇÂ∫¶ {complexity}: ÊèêÂçá {improvement:.1f}%")

        # ÊâìÂç∞ÁªüËÆ°ÊëòË¶Å
        print("\nÂàÜÁ±ªÁªüËÆ°:")
        category_stats = valid_df.groupby('category')['answer_correctness'].agg(['mean', 'std', 'count'])
        for cat, row in category_stats.iterrows():
            print(f"  ‚Ä¢ {cat.replace('_', ' ').title()}: {row['mean']:.3f} ¬± {row['std']:.3f} (n={int(row['count'])})")

        # ÊâìÂç∞Â§çÊùÇÂ∫¶ÁªüËÆ°
        print("\nÂ§çÊùÇÂ∫¶ÁªüËÆ°:")
        complexity_stats = valid_df.groupby('complexity')['answer_correctness'].agg(['mean', 'std', 'count'])
        for comp, row in complexity_stats.iterrows():
            print(f"  ‚Ä¢ Level {comp}: {row['mean']:.3f} ¬± {row['std']:.3f} (n={int(row['count'])})")


# ==================== STEP 8: ‰∏ªÂáΩÊï∞ ====================
def main():
    """‰∏ªÂáΩÊï∞ÔºöËøêË°åÊîπËøõÁöÑAgentËØÑ‰º∞"""
    import argparse

    parser = argparse.ArgumentParser(description='Run KGAgent V7 Improved Evaluation')
    parser.add_argument('--subset', nargs='+', help='Test IDs to run (default: all)')
    parser.add_argument('--output', default='evaluation_results', help='Output directory')
    parser.add_argument('--with-baseline', action='store_true', help='Enable baseline comparison')
    parser.add_argument('--config-file', help='JSON config file path')

    args = parser.parse_args()

    # Âä†ËΩΩÈÖçÁΩÆ
    if args.config_file:
        with open(args.config_file, 'r') as f:
            config = json.load(f)
        logger.info(f"Loaded config from: {args.config_file}")
    else:
        config = AgentConfig.get_config()

    # Â¶ÇÊûúÊåáÂÆö‰∫ÜÂü∫Á∫øÊØîËæÉÔºåÊ∑ªÂä†Âà∞ÈÖçÁΩÆ
    if args.with_baseline:
        config['enable_baseline'] = True

    # ÂàõÂª∫ËØÑ‰º∞ËøêË°åÂô®
    runner = ImprovedEvaluationRunner(config)

    # ËÆæÁΩÆÁéØÂ¢É
    if not runner.setup():
        logger.error("‚ùå Setup failed. Exiting.")
        return 1

    # ËøêË°åËØÑ‰º∞
    results = runner.run_evaluation(
        test_subset=args.subset,
        output_dir=args.output
    )

    if results is not None and not results.empty:
        # ÊâìÂç∞ÊëòË¶Å
        runner.print_summary()

        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ IMPROVED EVALUATION COMPLETE")
        logger.info("=" * 60)
        return 0
    else:
        logger.error("‚ùå Evaluation failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())