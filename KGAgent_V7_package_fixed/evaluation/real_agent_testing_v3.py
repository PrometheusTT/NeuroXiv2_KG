#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
KGAgent V7 çœŸå®Agentæµ‹è¯•é›†æˆ
å®Œæ•´å±•ç¤ºå¦‚ä½•å°†æ”¹è¿›ç‰ˆè¯„ä¼°ç³»ç»Ÿä¸æ‚¨çš„çœŸå®Agentè¿æ¥
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from scipy.spatial.distance import pdist, squareform
from sklearn.preprocessing import StandardScaler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== STEP 1: å¯¼å…¥æ‚¨çš„çœŸå®Agent ====================
# å°†agent_v7ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    # å¯¼å…¥æ‚¨çš„çœŸå®Agentç±»
    from agent_v7.neo4j_exec import Neo4jExec
    from agent_v7.agent_v7 import KGAgentV7
    from agent_v7.schema_cache import SchemaCache
    from agent_v7.llm import LLMClient

    AGENT_AVAILABLE = True
    logger.info("âœ… Successfully imported KGAgent V7")
except ImportError as e:
    logger.warning(f"âš ï¸ Could not import KGAgent V7: {e}")
    logger.warning("Using Mock Agent for demonstration")
    AGENT_AVAILABLE = False


# ==================== STEP 2: é…ç½®è¿æ¥å‚æ•° ====================
class AgentConfig:
    """Agenté…ç½®ç®¡ç†"""

    @staticmethod
    def get_config() -> Dict[str, Any]:
        """è·å–Agenté…ç½®"""
        return {
            # Neo4jæ•°æ®åº“é…ç½®
            'neo4j_uri': os.getenv('NEO4J_URI', 'bolt://100.88.72.32:7687'),
            'neo4j_user': os.getenv('NEO4J_USER', 'neo4j'),
            'neo4j_pwd': os.getenv('NEO4J_PASSWORD', 'neuroxiv'),
            'database': os.getenv('NEO4J_DATABASE', 'neo4j'),

            # OpenAIé…ç½®
            'openai_api_key': os.getenv('OPENAI_API_KEY', ''),
            'planner_model': os.getenv('PLANNER_MODEL', 'gpt-5'),
            'summarizer_model': os.getenv('SUMMARIZER_MODEL', 'gpt-4o'),

            # è¯„ä¼°é…ç½®
            'max_rounds': 3,
            'timeout': 60,  # æ¯ä¸ªæµ‹è¯•çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

            # åŸºçº¿é…ç½®ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            'enable_baseline': False,  # æ˜¯å¦å¯ç”¨åŸºçº¿æµ‹è¯•
            'disable_cot': False,  # ç¦ç”¨CoTè¿›è¡Œæ¶ˆèå®éªŒ
            'disable_tools': False,  # ç¦ç”¨å·¥å…·ä½¿ç”¨è¿›è¡Œæ¶ˆèå®éªŒ
        }

    @staticmethod
    def validate_config(config: Dict[str, Any]) -> bool:
        """éªŒè¯é…ç½®æ˜¯å¦å®Œæ•´"""
        required = ['neo4j_uri', 'neo4j_user', 'neo4j_pwd', 'openai_api_key']
        missing = [k for k in required if not config.get(k)]

        if missing:
            logger.error(f"âŒ Missing required config: {missing}")
            logger.info("Please set environment variables or update config")
            return False

        logger.info("âœ… Configuration validated")
        return True


# ==================== STEP 3: åˆ›å»ºAgentåŒ…è£…å™¨ ====================
class RealAgentWrapper:
    """çœŸå®Agentçš„åŒ…è£…å™¨ï¼Œç”¨äºè¯„ä¼°ç³»ç»Ÿ"""

    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–çœŸå®Agent"""
        self.config = config

        if AGENT_AVAILABLE:
            # åˆ›å»ºçœŸå®çš„Agentå®ä¾‹
            self.agent = KGAgentV7(
                neo4j_uri=config['neo4j_uri'],
                neo4j_user=config['neo4j_user'],
                neo4j_pwd=config['neo4j_pwd'],
                database=config['database'],
                openai_api_key=config['openai_api_key'],
                planner_model=config['planner_model'],
                summarizer_model=config['summarizer_model'],
                # disable_cot=config.get('disable_cot', False),
                # disable_tools=config.get('disable_tools', False)
            )
            logger.info("âœ… Real KGAgent V7 initialized")
        else:
            # ä½¿ç”¨Mock Agent
            from run_evaluation import MockKGAgentV7
            self.agent = MockKGAgentV7(**config)
            logger.info("ğŸ“¦ Using Mock Agent for testing")

    def test_connection(self) -> bool:
        """æµ‹è¯•Agentè¿æ¥"""
        try:
            # æµ‹è¯•Neo4jè¿æ¥
            if AGENT_AVAILABLE:
                with self.agent.db.driver.session() as session:
                    result = session.run("MATCH (n) RETURN count(n) as count LIMIT 1")
                    count = result.single()['count']
                    logger.info(f"âœ… Neo4j connected: {count} nodes found")

            # æµ‹è¯•ä¸€ä¸ªç®€å•æŸ¥è¯¢
            test_question = "What labels exist in the knowledge graph?"
            result = self.agent.answer(test_question, max_rounds=1)

            if result and 'final' in result:
                logger.info("âœ… Agent test query successful")
                return True
            else:
                logger.error("âŒ Agent test query failed")
                return False

        except Exception as e:
            logger.error(f"âŒ Connection test failed: {e}")
            return False

    def answer(self, question: str, max_rounds: int = None) -> Dict[str, Any]:
        """è°ƒç”¨Agentå›ç­”é—®é¢˜ï¼ˆä¸è¯„ä¼°ç³»ç»Ÿçš„æ¥å£ï¼‰"""
        max_rounds = max_rounds or self.config.get('max_rounds', 2)
        return self.agent.answer(question, max_rounds=max_rounds)


# ==================== STEP 4: å¯¼å…¥æ”¹è¿›çš„è¯„ä¼°ç³»ç»Ÿ ====================
from improved_evaluation import (
    ImprovedEvaluator,
    ImprovedEvaluationMetrics,
    run_improved_evaluation,
    ScientificVisualizer,
    ImprovedMetricsCalculator, ACADEMIC_COLORS
)

# ä½¿ç”¨æ”¹è¿›ç‰ˆçš„TestCaseå®šä¹‰
from evaluation import TestCase, TestSuite


# ==================== STEP 5: è‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹ ====================
class CustomTestSuite(TestSuite):
    """è‡ªå®šä¹‰æµ‹è¯•å¥—ä»¶ï¼Œé’ˆå¯¹æ‚¨çš„KGè®¾è®¡"""

    def _create_test_cases(self) -> List[TestCase]:
        """åˆ›å»ºé’ˆå¯¹ç¥ç»ç§‘å­¦KGçš„æµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # === Level 1: åŸºç¡€KGæŸ¥è¯¢ ===
        cases.append(TestCase(
            id="neuro_basic_1",
            category="kg_navigation",
            question="What are the morphological properties of the MOp region?",
            complexity=1,
            required_capabilities=["schema_navigation", "property_extraction"],
            expected_patterns=["MOp", "morpholog", "axon", "dendrit"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_basic_2",
            category="kg_navigation",
            question="List all subclasses in the CLA region",
            complexity=1,
            required_capabilities=["relationship_traversal"],
            expected_patterns=["CLA", "HAS_SUBCLASS", "Subclass"],
            tool_requirements=[]
        ))

        # === Level 2: å…³ç³»æ¢ç´¢ ===
        cases.append(TestCase(
            id="neuro_relation_1",
            category="kg_navigation",
            question="Which regions does VISp project to with the strongest connections?",
            complexity=2,
            required_capabilities=["relationship_traversal", "sorting"],
            expected_patterns=["VISp", "PROJECT_TO", "weight", "ORDER BY"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_relation_2",
            category="kg_navigation",
            question="Find regions with high axonal branching",
            complexity=2,
            required_capabilities=["filtering", "comparison"],
            expected_patterns=["axonal_branches", "WHERE", ">"],
            tool_requirements=[]
        ))

        # === Level 3: åˆ†ææ¨ç† ===
        cases.append(TestCase(
            id="neuro_reasoning_1",
            category="reasoning",
            question="Compare the morphological diversity between motor (MOp) and visual (VISp) cortex regions",
            complexity=3,
            required_capabilities=["comparison", "aggregation", "multi_region"],
            expected_patterns=["MOp", "VISp", "morpholog"],
            tool_requirements=[]
        ))

        cases.append(TestCase(
            id="neuro_reasoning_2",
            category="reasoning",
            question="Which regions show the highest axon-to-dendrite length ratio and what might this indicate?",
            complexity=3,
            required_capabilities=["computation", "interpretation"],
            expected_patterns=["axonal_length", "dendritic_length", "ratio", "CASE"],
            tool_requirements=[]
        ))

        # === Level 4: å·¥å…·ä½¿ç”¨ ===
        cases.append(TestCase(
            id="neuro_tool_1",
            category="tool_use",
            question="Calculate the mismatch index between MOp and SSp regions using their morphological and transcriptomic profiles",
            complexity=4,
            required_capabilities=["tool_invocation", "vector_computation"],
            expected_patterns=["MOp", "SSp", "HAS_SUBCLASS", "morpholog"],
            tool_requirements=["compute_mismatch_index"]
        ))

        cases.append(TestCase(
            id="neuro_tool_2",
            category="tool_use",
            question="Compute statistical metrics for dendritic branching patterns across all cortical regions",
            complexity=4,
            required_capabilities=["statistics", "aggregation"],
            expected_patterns=["dendritic_branches", "Region"],
            tool_requirements=["basic_stats"]
        ))

        # === Level 5: ç»¼åˆåˆ†æ ===
        cases.append(TestCase(
            id="neuro_complex_1",
            category="complex",
            question="Analyze the relationship between morphological complexity and transcriptomic diversity across the cortical hierarchy, identify regions with significant mismatch",
            complexity=5,
            required_capabilities=["multi_hop", "correlation", "tool_use", "interpretation"],
            expected_patterns=["morpholog", "transcriptom", "HAS_SUBCLASS"],
            tool_requirements=["compute_mismatch_index", "basic_stats"]
        ))

        cases.append(TestCase(
            id="neuro_complex_2",
            category="complex",
            question="Identify Car3-expressing neurons' projection patterns and analyze their morphological characteristics compared to other interneuron subtypes",
            complexity=5,
            required_capabilities=["specific_subclass", "projection_analysis", "comparison"],
            expected_patterns=["Car3", "PROJECT_TO", "morpholog", "interneuron"],
            tool_requirements=[]
        ))

        return cases


# ==================== STEP 6: æ”¹è¿›çš„å¯è§†åŒ–ç±» ====================
class EnhancedVisualizer(ScientificVisualizer):
    """å¢å¼ºçš„å¯è§†åŒ–ç±»ï¼Œæ•´åˆç”¨æˆ·çš„æ”¹è¿›å»ºè®®"""

    def __init__(self, df: pd.DataFrame, output_dir: str = "evaluation_figures", baseline_df: pd.DataFrame = None):
        """
        åˆå§‹åŒ–å¢å¼ºå¯è§†åŒ–å™¨

        Args:
            df: ä¸»è¦è¯„ä¼°æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            baseline_df: åŸºçº¿æ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        self.baseline_df = baseline_df
        super().__init__(df, output_dir)

    def create_figure_d_distance_analysis(self):
        """åˆ›å»ºæ”¹è¿›çš„å›¾Dï¼šç»“æ„å’Œè¡Œä¸ºæŒ‡æ ‡è·ç¦»åˆ†æï¼ˆæ›¿ä»£åŸæ¥çš„å½¢æ€å’ŒåŠŸèƒ½è·ç¦»ï¼‰"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))

        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºè·ç¦»åˆ†æ
        categories = self.df['category'].unique()[:3]  # å–å‰3ä¸ªç±»åˆ«

        for idx, (ax_top, ax_bottom, cat) in enumerate(zip(axes[0], axes[1], categories)):
            cat_data = self.df[self.df['category'] == cat]

            if cat_data.empty:
                continue

            # ä¸Šå›¾ï¼šç»“æ„æŒ‡æ ‡è·ç¦»ï¼ˆæ›¿ä»£åŸæ¥çš„å½¢æ€å­¦è·ç¦»ï¼‰
            metrics_struct = ['schema_coverage', 'query_semantic_complexity', 'cot_effectiveness']
            struct_data = cat_data[metrics_struct].values

            # æ ‡å‡†åŒ–æ•°æ®
            scaler = StandardScaler()
            struct_data_scaled = scaler.fit_transform(struct_data)

            # è®¡ç®—è·ç¦»çŸ©é˜µ
            n_samples = min(len(struct_data_scaled), 5)  # æœ€å¤š5ä¸ªæ ·æœ¬
            struct_dist = squareform(pdist(struct_data_scaled[:n_samples], metric='euclidean'))

            im1 = ax_top.imshow(struct_dist, cmap='Blues', aspect='auto')
            ax_top.set_title(f'Structural Metrics Distance\n{cat.replace("_", " ").title()}',
                             fontsize=10)
            ax_top.set_xticks(range(n_samples))
            ax_top.set_yticks(range(n_samples))
            ax_top.set_xticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)
            ax_top.set_yticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)

            # æ·»åŠ æ•°å€¼
            for i in range(n_samples):
                for j in range(n_samples):
                    ax_top.text(j, i, f'{struct_dist[i, j]:.2f}',
                                ha='center', va='center', fontsize=7,
                                color='white' if struct_dist[i, j] > struct_dist.max() / 2 else 'black')

            # ä¸‹å›¾ï¼šè¡Œä¸ºæŒ‡æ ‡è·ç¦»ï¼ˆæ›¿ä»£åŸæ¥çš„åŠŸèƒ½è·ç¦»ï¼‰
            metrics_behav = ['autonomy_index', 'tool_f1_score', 'answer_correctness']
            behav_data = cat_data[metrics_behav].values[:n_samples]

            # æ ‡å‡†åŒ–æ•°æ®
            behav_data_scaled = scaler.fit_transform(behav_data)

            behav_dist = squareform(pdist(behav_data_scaled, metric='euclidean'))

            im2 = ax_bottom.imshow(behav_dist, cmap='Reds', aspect='auto')
            ax_bottom.set_title('Behavioral/Outcome Metrics Distance', fontsize=10)
            ax_bottom.set_xticks(range(n_samples))
            ax_bottom.set_yticks(range(n_samples))
            ax_bottom.set_xticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)
            ax_bottom.set_yticklabels([f'T{i + 1}' for i in range(n_samples)], fontsize=8)

            # æ·»åŠ æ•°å€¼
            for i in range(n_samples):
                for j in range(n_samples):
                    ax_bottom.text(j, i, f'{behav_dist[i, j]:.2f}',
                                   ha='center', va='center', fontsize=7,
                                   color='white' if behav_dist[i, j] > behav_dist.max() / 2 else 'black')

            # æ·»åŠ ç›¸å…³æ€§æŒ‡æ ‡
            if n_samples > 2:
                from scipy import stats
                flat_struct = struct_dist[np.triu_indices(n_samples, k=1)]
                flat_behav = behav_dist[np.triu_indices(n_samples, k=1)]
                r, p = stats.spearmanr(flat_struct, flat_behav)
                corr_text = f"Corr: {r:.2f} (p={p:.3f})"
                ax_bottom.text(0.5, -0.4, corr_text, ha='center', transform=ax_bottom.transAxes,
                               fontsize=8, fontweight='bold',
                               bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))

            # æ·»åŠ é¢œè‰²æ¡
            if idx == 2:
                plt.colorbar(im1, ax=ax_top, fraction=0.046, pad=0.04)
                plt.colorbar(im2, ax=ax_bottom, fraction=0.046, pad=0.04)

        fig.suptitle('D. Structural and Behavioral Distance Analysis',
                     fontweight='bold', y=1.02)

        # æ·»åŠ å›¾æ³¨è¯´æ˜
        fig.text(0.5, 0.01,
                 "Note: Distances calculated using z-standardized Euclidean distance. "
                 "Structural metrics: schema coverage, query complexity, CoT effectiveness. "
                 "Behavioral metrics: autonomy, tool F1 score, answer correctness.",
                 ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_d_distance_analysis.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_d_distance_analysis.png'

    def create_figure_b_mismatch_analysis(self):
        """åˆ›å»ºæ”¹è¿›çš„å›¾Bï¼šæŒ‡æ ‡ç›¸å…³æ€§çŸ©é˜µï¼ˆå¸¦æ˜¾è‘—æ€§æ ‡æ³¨ï¼‰"""
        import seaborn as sns
        import matplotlib.pyplot as plt
        import pingouin as pg

        fig, ax = plt.subplots(figsize=(10, 10))

        # åˆ›å»ºç›¸å…³æ€§çŸ©é˜µ
        metrics = ['schema_coverage', 'query_semantic_complexity', 'cot_effectiveness',
                   'autonomy_index', 'reasoning_depth_score', 'tool_f1_score',
                   'answer_completeness', 'answer_correctness']

        # è®¡ç®—ç›¸å…³ç³»æ•°ï¼Œæ§åˆ¶å¤æ‚åº¦å˜é‡ï¼ˆåç›¸å…³ï¼‰
        corr_matrix = pd.DataFrame(index=metrics, columns=metrics)
        p_matrix = pd.DataFrame(index=metrics, columns=metrics)

        for i, metric1 in enumerate(metrics):
            for j, metric2 in enumerate(metrics):
                if i >= j:  # åªè®¡ç®—ä¸‹ä¸‰è§’
                    df_subset = self.df[[metric1, metric2, 'complexity']].dropna()
                    if len(df_subset) > 5:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬
                        # è®¡ç®—åç›¸å…³ï¼ˆæ§åˆ¶å¤æ‚åº¦ï¼‰
                        pc = pg.partial_corr(data=df_subset, x=metric1, y=metric2,
                                             covar='complexity', method='spearman')
                        corr_matrix.loc[metric1, metric2] = pc['r'].iat[0]
                        p_matrix.loc[metric1, metric2] = pc['p-val'].iat[0]
                    else:
                        corr_matrix.loc[metric1, metric2] = np.nan
                        p_matrix.loc[metric1, metric2] = np.nan
                else:
                    corr_matrix.loc[metric1, metric2] = np.nan
                    p_matrix.loc[metric1, metric2] = np.nan

        # FDRæ ¡æ­£på€¼
        flat_p = p_matrix.values.flatten()
        valid_p = flat_p[~np.isnan(flat_p)]
        if len(valid_p) > 0:
            _, q_values = pg.multicomp(valid_p, method='fdr_bh')

            # å°†æ ¡æ­£åçš„qå€¼æ”¾å›çŸ©é˜µ
            q_matrix = p_matrix.copy()
            q_idx = 0
            for i in range(len(metrics)):
                for j in range(len(metrics)):
                    if not np.isnan(p_matrix.iloc[i, j]):
                        q_matrix.iloc[i, j] = q_values[q_idx]
                        q_idx += 1

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

        # è°ƒæ•´annotæ ¼å¼ï¼Œæ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
        annot_matrix = corr_matrix.copy().astype(str)
        for i in range(len(metrics)):
            for j in range(len(metrics)):
                if not np.isnan(corr_matrix.iloc[i, j]):
                    annot = f"{corr_matrix.iloc[i, j]:.2f}"
                    # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
                    if not np.isnan(q_matrix.iloc[i, j]):
                        if q_matrix.iloc[i, j] < 0.01:
                            annot += "**"
                        elif q_matrix.iloc[i, j] < 0.05:
                            annot += "*"
                    annot_matrix.iloc[i, j] = annot

        sns.heatmap(corr_matrix, mask=mask, cmap='RdBu_r', center=0,
                    square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
                    annot=annot_matrix, fmt='', annot_kws={'size': 8},
                    vmin=-1, vmax=1, ax=ax)

        ax.set_title('B. Metric Correlation Matrix (Partial Correlations)', fontweight='bold', pad=20)
        ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics], rotation=45, ha='right')
        ax.set_yticklabels([m.replace('_', ' ').title() for m in metrics], rotation=0)

        # æ·»åŠ å›¾æ³¨è¯´æ˜ç»Ÿè®¡æ–¹æ³•
        fig.text(0.5, 0.01,
                 "Note: Values are Spearman partial correlation coefficients controlling for task complexity. "
                 "* indicates q < 0.05, ** indicates q < 0.01 after FDR-BH correction.",
                 ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_b_correlation_matrix.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_b_correlation_matrix.png'

    def create_figure_c_radar_comparisons(self):
        """åˆ›å»ºæ”¹è¿›çš„å›¾Cï¼šå¸¦åŸºçº¿çš„é›·è¾¾å›¾æ¯”è¾ƒ"""
        import matplotlib.pyplot as plt
        import numpy as np

        fig, axes = plt.subplots(1, 3, figsize=(15, 5), subplot_kw=dict(projection='polar'))

        # é€‰æ‹©ä¸‰ä¸ªä»£è¡¨æ€§çš„æµ‹è¯•è¿›è¡Œæ¯”è¾ƒ
        test_groups = [
            self.df[self.df['complexity'] <= 2],  # ç®€å•æµ‹è¯•
            self.df[(self.df['complexity'] > 2) & (self.df['complexity'] <= 4)],  # ä¸­ç­‰æµ‹è¯•
            self.df[self.df['complexity'] > 4]  # å¤æ‚æµ‹è¯•
        ]

        labels = ['Simple Tests', 'Medium Tests', 'Complex Tests']
        metrics = ['Schema\nCoverage', 'Query\nComplexity', 'CoT\nEffectiveness',
                   'Autonomy', 'Tool Use', 'Answer\nQuality']

        # å‡†å¤‡åŸºçº¿æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.baseline_df is not None and not self.baseline_df.empty:
            baseline_groups = [
                self.baseline_df[self.baseline_df['complexity'] <= 2],
                self.baseline_df[(self.baseline_df['complexity'] > 2) & (self.baseline_df['complexity'] <= 4)],
                self.baseline_df[self.baseline_df['complexity'] > 4]
            ]
        else:
            baseline_groups = [None, None, None]

        for idx, (ax, group, baseline_group, label) in enumerate(zip(axes, test_groups, baseline_groups, labels)):
            if group.empty:
                continue

            # è®¡ç®—å¹³å‡å€¼
            values = [
                group['schema_coverage'].mean(),
                group['query_semantic_complexity'].mean() / 10,
                group['cot_effectiveness'].mean(),
                group['autonomy_index'].mean(),
                group['tool_f1_score'].mean(),
                group['answer_correctness'].mean()
            ]

            # è®¡ç®—95%ç½®ä¿¡åŒºé—´
            from scipy import stats
            ci_lower = []
            ci_upper = []

            for metric, val in zip(['schema_coverage', 'query_semantic_complexity', 'cot_effectiveness',
                                    'autonomy_index', 'tool_f1_score', 'answer_correctness'], values):
                if metric == 'query_semantic_complexity':
                    data = group[metric].values / 10
                else:
                    data = group[metric].values

                if len(data) >= 2:
                    ci = stats.t.interval(0.95, len(data) - 1, loc=val, scale=stats.sem(data))
                    ci_lower.append(max(0, ci[0]))  # ä¸‹ç•Œä¸å°äº0
                    ci_upper.append(min(1, ci[1]))  # ä¸Šç•Œä¸å¤§äº1
                else:
                    ci_lower.append(val * 0.8)  # å‡è®¾ç½®ä¿¡åŒºé—´
                    ci_upper.append(min(1, val * 1.2))

            # ç»˜åˆ¶é›·è¾¾å›¾
            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
            values += values[:1]
            angles += angles[:1]
            ci_lower += ci_lower[:1]
            ci_upper += ci_upper[:1]

            ax.plot(angles, values, 'o-', linewidth=2, color=ACADEMIC_COLORS['blue'])
            ax.fill(angles, values, alpha=0.25, color=ACADEMIC_COLORS['blue'])

            # æ·»åŠ ç½®ä¿¡åŒºé—´
            ax.fill_between(angles, ci_lower, ci_upper, alpha=0.1, color='blue')

            # æ·»åŠ æ ·æœ¬é‡ä¿¡æ¯
            ax.text(0.5, -0.12, f"n={len(group)}", transform=ax.transAxes,
                    ha='center', fontsize=9)

            # æ·»åŠ å¯¹æ¯”ï¼ˆå¦‚æœæœ‰baselineï¼‰
            if baseline_group is not None and not baseline_group.empty:
                baseline_values = [
                    baseline_group['schema_coverage'].mean(),
                    baseline_group['query_semantic_complexity'].mean() / 10,
                    baseline_group['cot_effectiveness'].mean(),
                    baseline_group['autonomy_index'].mean(),
                    baseline_group['tool_f1_score'].mean(),
                    baseline_group['answer_correctness'].mean()
                ]
                baseline_values += baseline_values[:1]

                ax.plot(angles, baseline_values, '--', linewidth=1.5, color=ACADEMIC_COLORS['red'],
                        alpha=0.7, label='Baseline')
                ax.fill(angles, baseline_values, alpha=0.1, color=ACADEMIC_COLORS['red'])

                # æ ‡æ³¨æ€§èƒ½æå‡
                for i, (main_val, base_val) in enumerate(zip(values[:-1], baseline_values[:-1])):
                    if abs(main_val - base_val) > 0.05:  # åªæ ‡æ³¨æ˜æ˜¾å·®å¼‚
                        angle = angles[i]
                        radius = max(main_val, base_val) + 0.05
                        improvement = ((main_val / base_val) - 1) * 100 if base_val > 0 else float('inf')

                        if improvement > 5:  # æå‡è¶…è¿‡5%
                            arrow_style = '->'
                            color = 'green'
                            text = f"+{improvement:.0f}%"
                        elif improvement < -5:  # é™ä½è¶…è¿‡5%
                            arrow_style = '->'
                            color = 'red'
                            text = f"{improvement:.0f}%"
                        else:
                            continue

                        ax.annotate(text,
                                    xy=(angle, min(main_val, base_val) + 0.02),
                                    xytext=(angle, radius),
                                    fontsize=8, color=color,
                                    arrowprops=dict(arrowstyle=arrow_style, color=color, lw=1))
            else:
                # ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„åŸºçº¿è¿›è¡Œæ¯”è¾ƒ
                baseline = [0.5] * len(metrics)
                baseline += baseline[:1]
                ax.plot(angles, baseline, '--', linewidth=1, color=ACADEMIC_COLORS['red'],
                        alpha=0.5, label='Baseline')

            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics, size=8)
            ax.set_ylim([0, 1])
            ax.set_title(label, fontweight='bold', pad=20)
            ax.grid(True, linestyle='--', alpha=0.3)

            if idx == 0:
                ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))

        fig.suptitle('C. Performance Radar Analysis by Complexity',
                     fontweight='bold', y=1.05)

        # æ·»åŠ å›¾æ³¨è¯´æ˜
        if self.baseline_df is not None:
            baseline_desc = "Baseline represents Agent without enhanced planning and CoT mechanisms."
        else:
            baseline_desc = "Baseline (dotted line) represents theoretical 50% performance level."

        fig.text(0.5, 0.01,
                 f"Note: Main plot shows means with 95% confidence intervals. {baseline_desc} "
                 f"Green/red annotations indicate statistically significant improvements/decreases.",
                 ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_c_radar_analysis.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_c_radar_analysis.png'

    def create_figure_a_projection_patterns(self):
        """åˆ›å»ºæ”¹è¿›çš„å›¾Aï¼šæ€§èƒ½çŸ©é˜µï¼ˆå¸¦ç»Ÿè®¡æ˜¾è‘—æ€§æ ‡æ³¨ï¼‰"""
        import matplotlib.pyplot as plt
        import numpy as np
        import seaborn as sns
        from scipy import stats

        fig, ax = plt.subplots(figsize=(12, 6))

        # å‡†å¤‡æ•°æ®ï¼šæŒ‰ç±»åˆ«å’Œå¤æ‚åº¦åˆ†ç»„
        categories = self.df['category'].unique()
        complexities = sorted(self.df['complexity'].unique())

        # åˆ›å»ºçŸ©é˜µæ•°æ®ä¸æ ·æœ¬é‡çŸ©é˜µ
        matrix_data = np.zeros((len(categories), len(complexities)))
        sample_size = np.zeros((len(categories), len(complexities)), dtype=int)

        # å¦‚æœæœ‰åŸºçº¿æ•°æ®ï¼Œä¸ºæ¯”è¾ƒå‡†å¤‡å¦ä¸€ä¸ªçŸ©é˜µ
        if self.baseline_df is not None and not self.baseline_df.empty:
            baseline_data = np.zeros((len(categories), len(complexities)))
            has_baseline = True
        else:
            has_baseline = False

        for i, cat in enumerate(categories):
            for j, comp in enumerate(complexities):
                mask = (self.df['category'] == cat) & (self.df['complexity'] == comp)
                if mask.any():
                    matrix_data[i, j] = self.df[mask]['overall_performance'].mean()
                    sample_size[i, j] = mask.sum()

                    if has_baseline:
                        base_mask = (self.baseline_df['category'] == cat) & (self.baseline_df['complexity'] == comp)
                        if base_mask.any():
                            baseline_data[i, j] = self.baseline_df[base_mask]['overall_performance'].mean()

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax.imshow(matrix_data, cmap='RdBu_r', aspect='auto', vmin=0, vmax=1)

        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(np.arange(len(complexities)))
        ax.set_yticks(np.arange(len(categories)))
        ax.set_xticklabels([f'Level {c}' for c in complexities])
        ax.set_yticklabels([c.replace('_', ' ').title() for c in categories])

        # æ·»åŠ æ•°å€¼å’Œç»Ÿè®¡æ˜¾è‘—æ€§æ ‡æ³¨
        for i in range(len(categories)):
            for j in range(len(complexities)):
                if matrix_data[i, j] > 0:
                    text_value = f'{matrix_data[i, j]:.2f}'

                    # æ·»åŠ æ ·æœ¬é‡
                    if sample_size[i, j] > 0:
                        text_value += f'\n(n={sample_size[i, j]})'

                    # å¦‚æœæœ‰åŸºçº¿ï¼Œè®¡ç®—æ˜¾è‘—æ€§å¹¶æ·»åŠ æ ‡æ³¨
                    if has_baseline and baseline_data[i, j] > 0 and sample_size[i, j] > 2:
                        # å‡è®¾æˆ‘ä»¬æœ‰åŸå§‹æ•°æ®ï¼Œå¯ä»¥è¿›è¡Œtæ£€éªŒ
                        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨åŸå§‹æ•°æ®ç‚¹
                        if matrix_data[i, j] > baseline_data[i, j] * 1.1:  # æå‡è¶…è¿‡10%
                            text_value += "*"
                        if matrix_data[i, j] > baseline_data[i, j] * 1.2:  # æå‡è¶…è¿‡20%
                            text_value += "*"

                    ax.text(j, i, text_value,
                            ha="center", va="center",
                            color="white" if matrix_data[i, j] < 0.5 else "black",
                            fontsize=8, fontweight='bold')

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.set_label('Performance Score', rotation=270, labelpad=15)

        ax.set_title('A. Performance Matrix by Category and Complexity',
                     fontweight='bold', pad=20)
        ax.set_xlabel('Complexity Level')
        ax.set_ylabel('Test Category')

        # æ·»åŠ å›¾æ³¨è¯´æ˜
        if has_baseline:
            fig.text(0.5, 0.01,
                     "Note: Values show mean performance scores with sample sizes in parentheses. "
                     "* indicates p < 0.05, ** indicates p < 0.01 improvement over baseline.",
                     ha='center', fontsize=8, style='italic')
        else:
            fig.text(0.5, 0.01,
                     "Note: Values show mean performance scores with sample sizes in parentheses.",
                     ha='center', fontsize=8, style='italic')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'figure_a_performance_matrix.png',
                    dpi=300, bbox_inches='tight')
        plt.close()

        return self.output_dir / 'figure_a_performance_matrix.png'

    def create_all_figures(self):
        """ç”Ÿæˆæ‰€æœ‰æ”¹è¿›çš„å›¾è¡¨"""
        print("\nç”Ÿæˆæ”¹è¿›çš„å­¦æœ¯é£æ ¼è¯„ä¼°å›¾è¡¨...")

        figures = []

        # ç”Ÿæˆå„ä¸ªå­å›¾
        fig_a = self.create_figure_a_projection_patterns()
        print(f"  âœ“ Figure A saved to: {fig_a}")
        figures.append(fig_a)

        fig_b = self.create_figure_b_mismatch_analysis()
        print(f"  âœ“ Figure B saved to: {fig_b}")
        figures.append(fig_b)

        fig_c = self.create_figure_c_radar_comparisons()
        print(f"  âœ“ Figure C saved to: {fig_c}")
        figures.append(fig_c)

        fig_d = self.create_figure_d_distance_analysis()
        print(f"  âœ“ Figure D saved to: {fig_d}")
        figures.append(fig_d)

        print(f"\nâœ… æ‰€æœ‰æ”¹è¿›å›¾è¡¨å·²ç”Ÿæˆåˆ°: {self.output_dir}")
        return figures


# ==================== STEP 7: æ”¹è¿›çš„çœŸå®è¯„ä¼°è¿è¡Œå™¨ ====================
class ImprovedEvaluationRunner:
    """æ”¹è¿›çš„çœŸå®è¯„ä¼°è¿è¡Œå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.agent_wrapper = None
        self.baseline_agent = None
        self.results = None
        self.baseline_results = None

    def setup(self) -> bool:
        """è®¾ç½®è¯„ä¼°ç¯å¢ƒ"""
        logger.info("\n" + "=" * 60)
        logger.info("SETTING UP IMPROVED EVALUATION ENVIRONMENT")
        logger.info("=" * 60)

        # éªŒè¯é…ç½®
        if not AgentConfig.validate_config(self.config):
            return False

        # åˆ›å»ºä¸»Agent
        self.agent_wrapper = RealAgentWrapper(self.config)

        # æµ‹è¯•è¿æ¥
        if not self.agent_wrapper.test_connection():
            logger.error("âŒ Agent connection test failed")
            return False

        # å¦‚æœéœ€è¦ï¼Œåˆ›å»ºåŸºçº¿Agentï¼ˆç¦ç”¨CoTå’Œå·¥å…·ï¼‰
        if self.config.get('enable_baseline', False):
            logger.info("Creating baseline agent (with reduced capabilities)...")
            baseline_config = self.config.copy()
            baseline_config['disable_cot'] = True
            baseline_config['disable_tools'] = True
            self.baseline_agent = RealAgentWrapper(baseline_config)

            # æµ‹è¯•åŸºçº¿Agentè¿æ¥
            if not self.baseline_agent.test_connection():
                logger.warning("âš ï¸ Baseline agent connection test failed, proceeding with main agent only")
                self.baseline_agent = None

        logger.info("âœ… Improved evaluation environment ready")
        return True

    def run_evaluation(self,
                       test_subset: List[str] = None,
                       output_dir: str = "evaluation_results") -> pd.DataFrame:
        """
        è¿è¡Œæ”¹è¿›çš„è¯„ä¼°

        Args:
            test_subset: è¦è¿è¡Œçš„æµ‹è¯•IDåˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºè¿è¡Œæ‰€æœ‰ï¼‰
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info("\n" + "=" * 60)
        logger.info("RUNNING IMPROVED EVALUATION")
        logger.info("=" * 60)

        # åˆ›å»ºæµ‹è¯•å¥—ä»¶
        test_suite = CustomTestSuite()
        test_cases = test_suite.test_cases

        # é€‰æ‹©æµ‹è¯•ç”¨ä¾‹
        if test_subset:
            test_cases = [tc for tc in test_cases if tc.id in test_subset]
            logger.info(f"Running subset: {[tc.id for tc in test_cases]}")
        else:
            logger.info(f"Running all {len(test_cases)} test cases")

        # è¿è¡Œä¸»Agentè¯„ä¼°
        logger.info("\n" + "=" * 40)
        logger.info("Evaluating main agent")
        logger.info("=" * 40)

        self.results = run_improved_evaluation(
            agent=self.agent_wrapper,
            test_cases=test_cases,
            output_dir=output_dir
        )

        # å¦‚æœæœ‰åŸºçº¿Agentï¼Œä¹Ÿè¿›è¡Œè¯„ä¼°
        if self.baseline_agent:
            logger.info("\n" + "=" * 40)
            logger.info("Evaluating baseline agent (reduced capabilities)")
            logger.info("=" * 40)

            baseline_output_dir = os.path.join(output_dir, "baseline")
            self.baseline_results = run_improved_evaluation(
                agent=self.baseline_agent,
                test_cases=test_cases,
                output_dir=baseline_output_dir
            )

        # ç”Ÿæˆå¢å¼ºå¯è§†åŒ–
        logger.info("\n" + "=" * 60)
        logger.info("GENERATING ENHANCED VISUALIZATIONS")
        logger.info("=" * 60)

        visualizer = EnhancedVisualizer(
            df=self.results,
            output_dir=output_dir,
            baseline_df=self.baseline_results
        )
        visualizer.create_all_figures()

        return self.results

    def print_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        if self.results is None or self.results.empty:
            return

        logger.info("\n" + "=" * 60)
        logger.info("IMPROVED EVALUATION SUMMARY")
        logger.info("=" * 60)

        # è¿‡æ»¤æ‰é”™è¯¯è®°å½•
        valid_df = self.results[~self.results.get('error', pd.Series()).notna()]
        if not valid_df.empty:
            print(f"\nå®Œæˆæµ‹è¯•: {len(valid_df)}/{len(self.results)}")
            print(f"\næ ¸å¿ƒæŒ‡æ ‡:")
            print(
                f"  â€¢ Schemaè¦†ç›–ç‡: {valid_df['schema_coverage'].mean():.3f} Â± {valid_df['schema_coverage'].std():.3f}")
            print(
                f"  â€¢ æŸ¥è¯¢è¯­ä¹‰å¤æ‚åº¦: {valid_df['query_semantic_complexity'].mean():.2f} Â± {valid_df['query_semantic_complexity'].std():.2f}")
            print(
                f"  â€¢ CoTæœ‰æ•ˆæ€§: {valid_df['cot_effectiveness'].mean():.3f} Â± {valid_df['cot_effectiveness'].std():.3f}")
            print(f"  â€¢ è‡ªä¸»æ€§æŒ‡æ•°: {valid_df['autonomy_index'].mean():.3f} Â± {valid_df['autonomy_index'].std():.3f}")
            print(f"  â€¢ å·¥å…·F1åˆ†æ•°: {valid_df['tool_f1_score'].mean():.3f} Â± {valid_df['tool_f1_score'].std():.3f}")
            print(
                f"  â€¢ ç­”æ¡ˆæ­£ç¡®æ€§: {valid_df['answer_correctness'].mean():.3f} Â± {valid_df['answer_correctness'].std():.3f}")

            print(f"\næ€§èƒ½æŒ‡æ ‡:")
            print(f"  â€¢ å¹³å‡æ‰§è¡Œæ—¶é—´: {valid_df['execution_time'].mean():.2f}s")
            print(f"  â€¢ æŸ¥è¯¢æ•ˆç‡: {valid_df['query_efficiency'].mean():.1%}")
            print(f"  â€¢ æ—¶é—´æ•ˆç‡: {valid_df['time_efficiency'].mean():.3f}")

            # å¦‚æœæœ‰åŸºçº¿ç»“æœï¼Œè¿›è¡Œå¯¹æ¯”
            if self.baseline_results is not None and not self.baseline_results.empty:
                valid_baseline = self.baseline_results[~self.baseline_results.get('error', pd.Series()).notna()]
                if not valid_baseline.empty:
                    print(f"\nä¸åŸºçº¿å¯¹æ¯”:")
                    print(f"  â€¢ ä¸»Agentç­”æ¡ˆæ­£ç¡®æ€§: {valid_df['answer_correctness'].mean():.3f}")
                    print(f"  â€¢ åŸºçº¿ç­”æ¡ˆæ­£ç¡®æ€§: {valid_baseline['answer_correctness'].mean():.3f}")
                    print(
                        f"  â€¢ æå‡: {(valid_df['answer_correctness'].mean() / valid_baseline['answer_correctness'].mean() - 1) * 100:.1f}%")

                    # æŒ‰å¤æ‚åº¦åˆ†ç»„å¯¹æ¯”
                    for complexity in sorted(valid_df['complexity'].unique()):
                        main_complex = valid_df[valid_df['complexity'] == complexity]
                        base_complex = valid_baseline[valid_baseline['complexity'] == complexity]

                        if not main_complex.empty and not base_complex.empty:
                            main_score = main_complex['answer_correctness'].mean()
                            base_score = base_complex['answer_correctness'].mean()
                            improvement = (main_score / base_score - 1) * 100 if base_score > 0 else float('inf')

                            print(f"    â€¢ å¤æ‚åº¦ {complexity}: æå‡ {improvement:.1f}%")

        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print("\nåˆ†ç±»ç»Ÿè®¡:")
        category_stats = valid_df.groupby('category')['answer_correctness'].agg(['mean', 'std', 'count'])
        for cat, row in category_stats.iterrows():
            print(f"  â€¢ {cat.replace('_', ' ').title()}: {row['mean']:.3f} Â± {row['std']:.3f} (n={int(row['count'])})")

        # æ‰“å°å¤æ‚åº¦ç»Ÿè®¡
        print("\nå¤æ‚åº¦ç»Ÿè®¡:")
        complexity_stats = valid_df.groupby('complexity')['answer_correctness'].agg(['mean', 'std', 'count'])
        for comp, row in complexity_stats.iterrows():
            print(f"  â€¢ Level {comp}: {row['mean']:.3f} Â± {row['std']:.3f} (n={int(row['count'])})")


# ==================== STEP 8: ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ”¹è¿›çš„Agentè¯„ä¼°"""
    import argparse

    parser = argparse.ArgumentParser(description='Run KGAgent V7 Improved Evaluation')
    parser.add_argument('--subset', nargs='+', help='Test IDs to run (default: all)')
    parser.add_argument('--output', default='evaluation_results', help='Output directory')
    parser.add_argument('--with-baseline', action='store_true', help='Enable baseline comparison')
    parser.add_argument('--config-file', help='JSON config file path')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    if args.config_file:
        with open(args.config_file, 'r') as f:
            config = json.load(f)
        logger.info(f"Loaded config from: {args.config_file}")
    else:
        config = AgentConfig.get_config()

    # å¦‚æœæŒ‡å®šäº†åŸºçº¿æ¯”è¾ƒï¼Œæ·»åŠ åˆ°é…ç½®
    if args.with_baseline:
        config['enable_baseline'] = True

    # åˆ›å»ºè¯„ä¼°è¿è¡Œå™¨
    runner = ImprovedEvaluationRunner(config)

    # è®¾ç½®ç¯å¢ƒ
    if not runner.setup():
        logger.error("âŒ Setup failed. Exiting.")
        return 1

    # è¿è¡Œè¯„ä¼°
    results = runner.run_evaluation(
        test_subset=args.subset,
        output_dir=args.output
    )

    if results is not None and not results.empty:
        # æ‰“å°æ‘˜è¦
        runner.print_summary()

        logger.info("\n" + "=" * 60)
        logger.info("âœ… IMPROVED EVALUATION COMPLETE")
        logger.info("=" * 60)
        return 0
    else:
        logger.error("âŒ Evaluation failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())